{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ee5bcd-8ee6-45f1-b35b-469dda21acf3",
   "metadata": {},
   "source": [
    "## 머신러닝과 모델링\n",
    "### 1. 머신러닝\n",
    "- 데이터에서 패턴을 찾아 이를 바탕으로 새로운 데이터에 대한 예측하기\n",
    "  - 비지도 학습 : 정답이 없는 데이터로 학습\n",
    "  - 지도 학습 : 정답이 있는 데이터로 학습\n",
    "### 2. 모델링\n",
    "1. 전체 데이터를 훈련 데이터/테스트 데이터로 분리\n",
    "2. 훈련 데이터로 모델을 만들고 테스트 데이터로 모델의 성능 평가\n",
    "3. 지속적인 피드백을 통해 최종 모델 생성\n",
    "- 훈련 데이터로부터 패턴을 찾아내요, 새로운 데이터를 예측하고자 함\n",
    "- 모델의 성능을 테스트 데이터를 통해 측정함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a9742-ad05-4cd8-b61b-198460b5c4d2",
   "metadata": {},
   "source": [
    "## 회귀와 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8bc1a-11d2-4951-9bf4-41436517cd7b",
   "metadata": {},
   "source": [
    "### 1. 회귀(Regression)란?\n",
    "- 데이터 -> 패턴 찾기 -> 미래 값 예측\n",
    "- **연속적인 숫자**를 다룬다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18983715-26c4-4d59-8433-0655449c00f0",
   "metadata": {},
   "source": [
    "### 2. 회귀의 종류\n",
    "- 선형회귀\n",
    "- 비선형 회귀\n",
    "- 릿지회귀\n",
    "- 라쏘회귀\n",
    "- 다항회귀 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e036b-eb0b-4c47-b915-705af8765178",
   "metadata": {},
   "source": [
    "### 3. 선형 회귀\n",
    "- 데이터를 가장 잘 설명하는 회귀선 (ex. y=ax+b)를 찾아 예측하는 과정\n",
    "  - 현재 데이터 요약 + 미래 값 예측\n",
    "  - 독립변수(x)를 통해 종속변수(y)를 예측\n",
    "- '선형'이란?\n",
    "  - 반드시 직선을 의미하는 것은 아님\n",
    "  - 모델이 파라미터에 대해 선형이라는 뜻; 계수(베타)들이 선형적으로 결합되면 선형회귀\n",
    "  - ex. B0 + B1sin(x) -> 비직선이지만 B에 대해 선형\n",
    "- 선형 회귀 종류\n",
    "  - 단순 선형 회귀: 독립변수 1개, 직선 형태\n",
    "  - 다중 선형 회귀: 독립변수 2개 이상으로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528859b4-e3ed-4d46-9e96-74d519d81a8a",
   "metadata": {},
   "source": [
    "### (1). 단순선형회귀분석\n",
    "- 독립변수 1개  \n",
    "- 데이터를 가장 잘 설명하는 직선 y=ax+b 찾기  \n",
    "- 적절한 회귀계수(a=기울기)와 절편(b)를 찾는 과정이다  \n",
    "- 잔차를 정의해나가며 잘 찾기\n",
    "  - 오차: 모델이 설명 못한 실제 - 실제 평균값의 차이\n",
    "  - 잔차: 모델이 설명 못한 실제 - 예측값의 차이  \n",
    "      - **예측값을 조절하며 잔차 줄여나가는 과정**  \n",
    "        \n",
    "- 최소제곱법: 잔차 제곱의 합(RSS)이 최소가 되는 지점으로 최적의 회귀선을 구하는 방식\n",
    "  - 잔차의 부호을 없애고, 추후 경사하강법 계산의 미분에 유리하도록 제곱을 취함\n",
    "  - 회귀선과 실제 데이터와의 차이 최소화 하기 위한 시도\n",
    "  - 최소제곱법을 통해 구한 함수가 복잡하여 최솟값을 구하기 힘든 경우 사용한다  \n",
    "  \n",
    "- 경사하강법\n",
    "      - 1. 함수가 닫힌 상태인 경우  \n",
    "      - 2. 함수가 너무 복잡해 미분계수를 구하기 어려운 경우  \n",
    "      - 3. Gradient Descent를 구현하는게 미분계수를 구하는 것보다 더 쉬운 경우  \n",
    "      - 4. 데이터의 양이 너무 많은 경우 효율성을 위해 사용한다  \n",
    "  - 마찬가지로 a와b를 잘 찾고자 함. 잔차 제곱을 줄이려는 목적은 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac4717-b158-4233-b81c-b06b0fc17bde",
   "metadata": {},
   "source": [
    "- 목적함수, 비용함수, 손실함수\n",
    "  - 목적함수: 우리가 달성하고 싶은 목표, 모델의 예측이 얼마나 잘 맞는지 평가하는 기준\n",
    "    - 최적화하려는 대상을 수학적으로 표현한 함수\n",
    "  - 손실함수: 개별 데이터 샘플에 대한 오차를 측정하는 함수\n",
    "    - 한 개의 데이터 포인트에 대해 모델이 얼마나 틀렸는지 평가\n",
    "    - ex. 회귀분석 -> 평균제곱오차(MSE), 평균절대오차(MAE)\n",
    "    - ex. 분류문제 -> 교차 엔트로피\n",
    "  - 비용함수: 전체 데이터셋에서 평균적인 손실을 측정하는 함수\n",
    "    - 손실함수를 모든 데이터셋에 대해 계산한 후 평균 or 합산한 값\n",
    "    - 머신러닝에서는 비용함수를 최적화(최소화)하는 것이 목표\n",
    "  - 모델 학습시 대부분 비용함수를 최소화하는 것이 목표\n",
    "    - 비용함수=목적함수로 봐도 무방 (비용함수 최소화 = 목적함수 최적화 방향)\n",
    "    - 예외도 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8778b-84bc-43f6-9c80-b7384ada2f10",
   "metadata": {},
   "source": [
    "- 손실함수\n",
    "  - E값을 최소화하기 위한 w와 b값 찾기\n",
    "  - x,y를 x축으로, E를 y축으로 놓고 경사하강법 사용. (Newton's Method와 비슷함)\n",
    "    - 최솟값에 도달하면 미분계수가 0이 되어  '현재 값 - 미분계수값'에 변화가 없어짐 -> 최솟값에 도달했다고 생각"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fba146-7336-4444-834c-d3f394b2bab6",
   "metadata": {},
   "source": [
    "- 학습률(Learning Rate)\n",
    "  - 최솟값에 도달하기 위해 움직이는 보폭\n",
    "  - 하이퍼파라미터에 의해 결정\n",
    "  - 너무 작음: 시간 오래걸림\n",
    "  - 너무 큼: 모델이 발산하여 손실값이 계속 커짐\n",
    "- 학습률 설정\n",
    "  1. 작은 값부터 경험적으로 시도\n",
    "  2. 실험을 통해 조정\n",
    "  - Learning Rate Scheduler, Grid search, Random Serch등을 통해 자동으로 학습률을 찾을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147a96c-2950-49fc-a2c7-942b1bb57227",
   "metadata": {},
   "source": [
    "- Local Minima\n",
    "  - 이동량이 기울기에 비레해서 움직임\n",
    "  - MSE, RMSE와 같은 비용함수는 볼록함수여서 local minima를 걱정 안해도 되지만, 아닌 경우에는 주의해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3b807-38e4-446a-8c0e-094cfa30d772",
   "metadata": {},
   "source": [
    "- 모멘텀 - Local Minima 해결법\n",
    "  - 기존의 경사 하강법은 이전의 기울기를 고려하지 않은 채 현재의 기울기만 고려함\n",
    "  - but, 이전의 기울기(관성)을 고려하여 local minima 문제 해결\n",
    "    - 현재의 이동량 반영 + 이전 단계의 이동 반영 = 실제 이동\n",
    "      - 이동량을 기울기 대신 사용, 이동량=속도\n",
    "  - 작은 기울기는 쉽게 넘어갈 수 있고, 지역 최솟값을 탈출할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46509288-99fd-441f-9191-c4db5152844e",
   "metadata": {},
   "source": [
    "### (2). 다중선형회귀분석\n",
    "- 독립변수가 2개 이상\n",
    "- 독립변수가 2개인 경우 3차원 공간에 표현되고, 회귀선->평면\n",
    "- 최소제곱법 사용 가능, 관측치와 평면(예측치)간의 차이 = 잔차\n",
    "- 경사하강법 사용하는 것도 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc9b58-8228-448f-9eff-a8b0d389adad",
   "metadata": {},
   "source": [
    "- 다중공선성\n",
    "  - 독립변수끼리 겹치는 상황\n",
    "  - 다중공선성이 높음 => 특정 독립변수가 종속변수에 미치는 영향의 정도를 정확하게 구분하기 어려움 => 회귀분석 정확도 낮아짐\n",
    "\n",
    "- 확인 방법\n",
    "  1. 상관계수\n",
    "  2. VIF지수 (분산 팽창 인수)\n",
    "     - 높으면(VIF>5) 다중공선성에 문제가 있다고 판단\n",
    "\n",
    "- 대처 방법\n",
    "  1. 변수 제거\n",
    "  2. 변수 변환\n",
    "     - 변수들을 합치거나 빼기 \n",
    "  4. 규제 선형 모델 활용\n",
    "     - 모델의 복잡도 줄이기\n",
    "  6. PCA(주성분분석)\n",
    "     - 중요한 정보 보존하면서 데이터의 복자보 줄이기\n",
    "     - 상관있는 변수끼리 묶어서 더 적은 수의 대표 축으로 바꾸기\n",
    "     1. 전처리 (정규화)\n",
    "     2. 주성분 찾기\n",
    "     3. 차원 축소\n",
    "     4. 복원 가능(선택적)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee7a0a-3e86-4f7c-90cd-09a50ccf58eb",
   "metadata": {},
   "source": [
    "### 4. 규제선형모델\n",
    "- 오버피팅이 되지 않도록 규제\n",
    "  - RSS를 최소화하는 방법은 회귀계수가 쉽게 커져 과적합 발생\n",
    "  - 특정 변수에 과하게 의존하게 됨\n",
    "- 특성에 곱해지는 계수(기울기)의 크기 조절 -> alpha값으로 제어. alpha는 규제 강도에 비례\n",
    "\n",
    "- 종류\n",
    "1. L2규제: 릿지회귀, W(회귀계수)의 제곱에 대해 페널티 부여\n",
    "   - W의 크기에 따라 페널티를 부여. 강강약약\n",
    "   - 계수가 0이 되지는 않음\n",
    "   - 모든 변수 유지 + 크기를 적절히 줄이기\n",
    "   - 예측 변수가 많거나 다중공선성 존재할 때 사용\n",
    "2. L1규제: 라쏘회귀, W의 절댓값에 대해 페널티 부여\n",
    "   - 모든 W에 대해 같은 페널티 부여.\n",
    "   - 일부 계수는 0이 되기도 함\n",
    "   - 비중이 낮은 변수는 사라지고, 중요한 변수만 남음\n",
    "   - 예측변수 수 많고 그 중 일부만 중요하거나, 모델의 해석을 간단하게 유지하고자할 때 사용\n",
    "4. 엘라스틱넷 회귀: L2 + L1\n",
    "   - L2,L1 둘 다 사용\n",
    "   - 안정적인 예측(모든 변수가 균형있게 줄어듦) + 희소성(변수 제거)을 동시에 달성 가능\n",
    "   - 예측 변수 수가 많고, 그 중 중요한 변수를 선택하면서도 다중공선성을 관리할 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c672a-2902-4aa5-a088-46b6abf9e8ea",
   "metadata": {},
   "source": [
    "### 5. 모델 평가 방법\n",
    "1. 성능 평가 지표\n",
    "- 평균 제곱 오차 (Mean Squared Error)\n",
    "  - mean((실제 관측값 - 모델 예측값)^2)\n",
    "- 평균 절대 오차 (Mean Absolute Error)\n",
    "  - mean(|실제 예측값 - 모델 예측값|)\n",
    "\n",
    "- 둘의 차이\n",
    "  - MSE는 큰 오차(이상치)에 민간하게 반응, MAE는 모든 오차에 동일하게 반응\n",
    " \n",
    "2. 변수 유의성 평가\n",
    "- t 검정: 독립변수의 회귀계수가 유의미한지 검정\n",
    "  - 귀무가설: 회귀계수(a)가 0이다 -> 독립변수의 영향력 x\n",
    "  - 대립가설: 회귀계수(a)가 0이 아니다 -> 독립변수의 영향력 o\n",
    "- 검정 과정\n",
    "  - t값 계산\n",
    "  - p_value확인 및 판단\n",
    "- 해석\n",
    "  - 회귀계수가 0 -> 해당 독립변수는 의미 없음\n",
    "  - p값 작을수록 더 큰 영향을 준다고 볼 수 있음\n",
    "  - t값 크면 귀무가설 기각할 가능성 높아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e1e2fae-75bf-4725-9653-0d22fd0635ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e393d4-81fc-4c4c-9490-49446688d02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
