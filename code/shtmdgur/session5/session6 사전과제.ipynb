{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900b5316-51cf-4e8e-b0db-ff1d2eebd213",
   "metadata": {},
   "source": [
    "### 1. 지도 학습과 비지도 학습의 차이에 대해 설명해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc6b49-9a28-435b-9cbb-20726b4c6e94",
   "metadata": {},
   "source": [
    "- 지도학습은 문제와 답 데이터를 통해 학습을 시켜 예측값이 정답에 최대한 가까워지도록 학습시키는 것이고, 비지도 학습은 문제만 가지고 학습을 시켜 데이터들의 특징에서 패턴을 찾아내는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32377440-f094-4a66-bf2e-8e6afe0bfcef",
   "metadata": {},
   "source": [
    "### 2. 회귀와 분류의 차이에 대해 설명해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b877136-2a6a-432c-81ea-3927abc3fe1b",
   "metadata": {},
   "source": [
    "- 회귀는 연속형 변수를 회귀선을 통해 예측한다(종속변수가 연속형). 그에 비해 분류는 범주형 변수를 여러 방법들로 구분한다(종속변수가 범주형)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a3367-181d-4597-a363-4926f58cf1fd",
   "metadata": {},
   "source": [
    "### 3. 분류 모델의 네 가지 종류와, 각 모델이 무엇인지 간단하게 정리해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28b3f6-627f-49b8-ad9d-38dca2c94574",
   "metadata": {},
   "source": [
    "- 분류 모델에는 로지스틱 회귀, 결정 트리, 서포트 벡터 머신, KNN등이 있다\n",
    "- 로지스틱 회귀는 시그모이드함수를 통해 각 데이터를 0과1로 분류한다.\n",
    "- 결정 트리는 임계값을 기준으로 각 child로 나누는 과정을 순수한 label만 남을때까지 반복한다. 대표적으로 cart 알고리즘을 사용하는데, 불순도(지니계수)가 낮아지게끔 분류한다.\n",
    "- 서포트 벡터 머신은 label에 따라 분류된 데이터들을 거리를 기준으로 결정 경계(최적의 경계선)를 찾는 것이다\n",
    "- knn은 사전 학습을 하지 않고, k개의 이웃 데이터를 찾아내 데이터들이 가장 많이 갖고있는 label을 통해 분류한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319566b5-fc2f-4e44-9e7a-e455da9f520c",
   "metadata": {},
   "source": [
    "### 4. 분류 평가 지표에는 무엇이 있는지 작성해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f2142-e5f2-47f1-9ebb-8605fb3e931c",
   "metadata": {},
   "source": [
    "- 혼동행렬, f1-score, roc/auc curve등이 있다\n",
    "- 혼동행렬은 tp/tn/fp/fn으로 데이터들을 분류하여 정확도, 정밀도, 재현율을 계산하고 이를 이용한다\n",
    "- f1-score은 정밀도와 재현율의 조화평균값이다. 이를 통해 모델의 성능을 평가할 수 있다\n",
    "- roc curve를 통해 tpr과 fpr을 알아내고, 그것의 적분값은 auc이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a959ac8-fcc9-496d-8d56-72fa029381b5",
   "metadata": {},
   "source": [
    "### 5. 하이퍼파라미터 최적화가 무엇인지 쓰세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e81cc6f-b695-4757-ac10-ea6e37d05f19",
   "metadata": {},
   "source": [
    "- 하이퍼파라미터란, 사용자가 직접 학습 전에 세팅하는 변수이다. 이를 최적화하여 모델 성능을 높이기 위해 grid search, random search, bayesian  optimization을 사용한다. bayesian optimization이 가장 효율적인듯?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b442cebc-1afc-423f-ade2-30f6ffe7b0a6",
   "metadata": {},
   "source": [
    "### 내용 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef03443-cb5c-41f3-b5ba-55ebaa51350c",
   "metadata": {},
   "source": [
    "- 머신러닝: 데이터를 통해 컴퓨터가 스스로 판단 혹은 예측할 수 있도록 학습시키는 것\n",
    "  - 지도 학습: 문제+답을 알려주고 학습시키기 (회귀, 분류)\n",
    "    - 예측값과 정답이 최대한 같아지도록 학습\n",
    "    - 회귀\n",
    "      - 연속형 변수 예측\n",
    "    - 분류: 기존 데이터가 속하는 레이블의 패턴을 인지하여 새로운 데이터의 레이블 판별\n",
    "      - 범주형 변수 예측\n",
    "      - 이진 분류: 예측하고자하는 변수의 기준이 2가지 (T/F)\n",
    "      - 다중 분류: 예측하고자하는 변수의 기준이 3가지 이상\n",
    "  - 비지도 학습: 문제만 알려주고 학습시키기 (군집화)\n",
    "    - 데이터의 특성을 학습하여 스스로 패턴을 파악함\n",
    "  - 강화 학습: 보상을 통해 상을 최대화, 벌을 최소화 하는 방향으로 학습시키기 (알파고)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82721ff1-683c-4a65-81fb-4d0a8630b786",
   "metadata": {},
   "source": [
    "- 분류 모델\n",
    "  - 로지스틱 회귀\n",
    "    - 이진 분류 문제, 샘플이 특정 클래스에 속할 확률 추정\n",
    "    - 시그모이드 함수를 통해 이진 분류 (lim z->-inf=0, lim z->inf=1, H(0)=1/2)\n",
    "    - H(x)=1/(1+e^(-(wx+b)))에서 적절한 가중치 w와 b구하기\n",
    "      - w 커질수록 기울기 급해짐, b 커질수록 그래프가 위로 올라감\n",
    "  - 결정 나무.\n",
    "     - 조건에 따라 데이터를 분류, 데이터가 순수한 label의 집합으로 구성될 때까지 분류를 반복함\n",
    "     - 용어:\n",
    "       - root node: decision tree의 시작이 되는 노드\n",
    "       - edge: 노드와 노드를 연결하는 길목\n",
    "       - leag nodes: tree의 가장 만지막 노드, 모델에서 label에 해당\n",
    "       - height(depth): tree의 깊이로, 클수록 tree의 깊이가 복잡해짐\n",
    "       - level: 노드의 절대적 위계. root node의 level-=0, leag node의 level은 height-1\n",
    "       - parent: 상대적으로 높은 위계의 노드\n",
    "       - child: 상대적으로 낮은 위계의 노드\n",
    "       - binary tree: 이진트리. children이 최대 2개인 트리\n",
    "    - Cart 알고리즘\n",
    "      - 데이터셋을 임계값을 기준으로 두 child로 나누는 알고리즘\n",
    "      - 불손도(지니 계수)가 낮아지는 방향으로 임계값을 나눔\n",
    "        - 데이터 집합에서 서로 다른 클래스가 섞여있는 정도. 0에 가까워질수록 순수도가 높음\n",
    "      - 임계값 설정 -> 그룹 나누기 -> 불순도 감소 알고리즘\n",
    "      - 한계: Greedy Algorithm\n",
    "        - 당장의 지니계수를 낮추는 판단만 함\n",
    "    - 고려해야할 것들\n",
    "      - parameter 설정\n",
    "        - min_samples_split: 분할되기 위해 노드가 가져야 하는 최소 샘플 수\n",
    "        - min_samples_leaf: 리프 노드가 가지고 있어야 하는 최소 샘플 수\n",
    "        - min_weight_funtion_leaf: 가중치가 부여된 전체 샘플 수에서의 비율\n",
    "        - max_leaf_nodes: 리프 노드의 최대 개수\n",
    "        - max_features: 각 노드에서 분할에 사용할 특성의 최대 수\n",
    "      - 시각화\n",
    "        - 분류가 잘 일어났는지, 과적합은 없는지 등 판단\n",
    "      - prunning(가지치기)\n",
    "        - 불필요한 노드 지움으로써 일반화 성능 높이기\n",
    "        - 깊이가 줄어들고 결과의 개수도 줄어들음\n",
    "    - 서포트 벡터 머신 (SVM)\n",
    "      - 클래스를 분류할 수 있는 최적의 경계선을 찾아내는 알고리즘\n",
    "      - 명확하게 분류 가능한 데이터셋, 고차원 공간등 에서 효과적\n",
    "      - 구성\n",
    "        - support vector: 구분선과 가장 가까운 포인트\n",
    "        - decision boundary(결정 경계): 집단을 구분하는 선\n",
    "        - margin: 선과 각 점의 거리\n",
    "      - 결정 경계는 데이터로부터 가장 멀리떨어진(margin이 가장 큰) 경우를 선택함\n",
    "    - KNN\n",
    "      - 비슷한 특성을 가진 데이터끼리 서로 가까이 있음을 이용해 분류\n",
    "      - 거리가 가까운 k개의 다른 데이터 레이블을 참조하여 분류\n",
    "      - 데이터 준비(학습 불필요, 각 데이터는 특징 벡터와 레이블로 이루어짐) -> k값 설정(가장 가까운 이웃의 개수 설정, 홀수개로 설정) -> 거리 계산(새로운(예측하려는)데이터와 기존의 모든 데이터 간의 거리 계산) -> 가까운 k개의 이웃 선택 -> 분류(예측결과 = 이웃 클래스 중 가장 많이 등장하는 것)\n",
    "      - 훈련 필요 x, 정보 손실 x\n",
    "      - but, 쿼리 처리에 많은 시간 소요, 로버스트하지 않음\n",
    "      - test 시점 연산이 적은 것이 유리함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0642221-12dd-431d-8463-de052e2f226b",
   "metadata": {},
   "source": [
    "- 앙상블\n",
    "  - 여러 개의 개별 분류 모델을 결합하여 더 좋은 성능 내기\n",
    "  - 종류\n",
    "    - 보팅\n",
    "      - 다른 모델을 병렬로 사용\n",
    "    - 배깅\n",
    "      - 같은 모델을 병렬로 사용\n",
    "    - 부스팅\n",
    "      - 같은 모델을 직렬(순차적)으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c4871-bd08-4591-b875-eac448dd99c3",
   "metadata": {},
   "source": [
    "- 분류 평가 지표\n",
    "  - 혼동 행렬\n",
    "    - 실제 정답과 예측 결과를 2*2 테이블로 표현\n",
    "    - TP, TN, FP, FN\n",
    "      - T_/F_: 모델이 정답/틀림 (TRUE/FALSE)\n",
    "      - _P/_N: 양성/음성 예측 (POSITIVE, NEGATIVE)\n",
    "    - 이를 이용한 분류 모델 평가 지표\n",
    "      - 정확도 (accuracy)\n",
    "        - TP+TN/전체\n",
    "        - 얼마나 정확한가?\n",
    "        - 맞춘 것/전체\n",
    "      - 정밀도 (precision)\n",
    "        - TP/(TP+FP)\n",
    "        - 참이라고 예측한 것 중, 실제로 얼마나 참인가?\n",
    "          - 참으로 예측했는데 진짜 참인 비율\n",
    "        - 실제 참/예측 참\n",
    "        - ex. 암 환자 판단\n",
    "      - 재현도 (recall)\n",
    "        - TP/(TP+FN)\n",
    "        - 실제로 참인것 중, 얼마나 참이라고 예측했는가?\n",
    "          - 진짜 참 중에 참이라고 맞춘(예측한) 비율\n",
    "        - 예측 참/실제 참\n",
    "        - ex. 스팸 메일 분류\n",
    "      - Threshold\n",
    "        - 분류시 참/거짓을 나누는 경계값\n",
    "        - threshold 높임 -> positive 예측 늘어남 -> recall증가, precision감소\n",
    "        - threshold 낮춤 -> positive 예측 줄어듬 -> recall감소, precision증가\n",
    "        - pricision&recall 그래프에서 두 그래프가 만나는 점 -> threshold로 하면 예측 오류 최소화 가능\n",
    "  - F1-score\n",
    "     - precision과 recall의 조화평균\n",
    "       - 더 낮은 값에 가까운 평균\n",
    "  - ROC/AUC cURVE\n",
    "    - 분류 모델 평가할 때 사용\n",
    "    - ROC\n",
    "      - TPR: 참 중 참이라고 예측한 비율 (=recall)\n",
    "      - FPR: 거짓 중 참이라고 잘못 예측한 비율\n",
    "    - AUC\n",
    "      - ROC와 x축 사이의 면적 (적분값)\n",
    "      - 1에 가까울수록 성능 굳\n",
    "  - 다중 분류 평가 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523093d7-5c4b-4fe9-948f-24498daae734",
   "metadata": {},
   "source": [
    "- 하이퍼파라미터 최적화\n",
    "  - 하이퍼파라미터: 학습 시작 전 사용자가 직접 설정하는 변수\n",
    "  - 하이퍼 파라미터 최적화: tuning을 통해 적절한 하이퍼파라미터를 찾아 모델 성능 향상\n",
    "    - 하이퍼파라미터 탐색 범위 설정 -> 평가 지표 계산 함수 정의 -> 1단계에서 샘플링한 하이퍼파라미터값을 사용해 검증 데이터로 정확도 평가 -> 윗 단계들 반복하여 정확도 결과를 보고 하이퍼파라미터의 범위 좁히기\n",
    "    - 최적화 방법\n",
    "      - grid search\n",
    "        - 정해진 범위에서 하이퍼파라미터를 모두 순회\n",
    "        - 정확도는 높지만 시간 오래걸림\n",
    "      - random search\n",
    "        - 무작위로 탐색\n",
    "        - 속도는 비교적 빠르지만 정확도가 떨어짐\n",
    "      - bayesian optimization\n",
    "        - 여러개의 하이퍼파라미터들에 대해 auisition function을 적용했을 때, 가장 큰 값이 나올 확률이 높은 지점을 찾아냄\n",
    "        - '사전 정보를 최적값 탐색에 반영'\n",
    "        - 보다 효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214847f-2dfb-49cd-87ba-ff985c7bdddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
