{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6차 교육세션_분류_사전과제\n",
    "\n",
    "목표\n",
    "* 분류의 개념, 종류, 평가 지표에 대해 이해한다.\n",
    "* 하이퍼파라미터 최적화를 이해한다.\n",
    "* 코드를 통해 분류 모델을 구현하고 평가한다.\n",
    "\n",
    "다음 질문에 대해 정리가 끝난 후 답하여 봅시다.\n",
    "1. 지도 학습과 비지도 학습의 차이\n",
    "2. 회귀와 분류의 차이\n",
    "3. 분류 모델의 네 가지 종류와 각 모델이 무엇인지 간단하게 정리\n",
    "4. 분류 평가 지표에는 무엇이 있는가\n",
    "5. 하이퍼파라미터 최적화가 무엇인가\n",
    "\n",
    "## 1. 분류란\n",
    "\n",
    "### 1.1 머신러닝\n",
    "\n",
    "머신러닝: 인공지능의 한 분야, 컴퓨터가 스스로 학습할 수 있도록 도와주는 알고리즘 혹은 기술을 개발하는 분야\n",
    "* 머신러닝에서 컴퓨터는\n",
    "1. 알고리즘을 이용하여 데이터를 분석하고\n",
    "2. 분석 결과를 스스로 학습한 후\n",
    "3. 이를 기반으로 판단이나 예측을 하게 된다.\n",
    "\n",
    "머신러닝의 종류\n",
    "머신러닝 = 컴퓨터가 스스로 어떠한 판단 또는 예측을 할 수 있게 학습시키는 것 \n",
    "- 지도 학습: 문제와 정답을 모두 알려주고 공부시키는 방법\n",
    "- 비지도 학습: 답을 가르쳐주지 않고 공부시키는 방법\n",
    "- 강화 학습 : 보상을 통해 상을 최대화 / 벌을 최소화하는 방향으로 공부시키는 방법\n",
    "\n",
    "### 1.2 지도 학습: 회귀와 분류\n",
    "\n",
    "지도 학습: 입력 값과 함께 결과 값(정답 레이블)을 같이 주고 학습을 시키는 방법\n",
    "\n",
    "학습의 방향 :  예측값(prediction)과  정답(label)이 최대한 같아지도록 학습\n",
    "\n",
    "분류(Classification) : 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것\n",
    "\n",
    "e.g. 대출상환여부 (0/1) (타겟) 예측\n",
    "\n",
    "**종속변수** = 대출상환여부(0/1)\\\n",
    "**독립변수** = 고객의 특성, 고객이 받은 대출의 특성\\\n",
    "+ 이메일이 스팸인지 여부, 종양이 악성인지 여부도 분류에 해당\n",
    "\n",
    "그렇다면 회귀와 분류의 차이는 무엇인가 \n",
    "- 회귀: 데이터가 **연속형** 변수를 예측하기 위해 사용될 때\n",
    "- 분류: 데이터가 **범주형** 변수를 예측하기 위해 사용될 때\n",
    "\n",
    "**[분류에 쓰이는 대표적인 머신러닝 알고리즘]**\n",
    "\n",
    "1. **로지스틱 회귀(Logistic Regression)** : 독립변수와 종속변수의 **선형 관계성**에 기반해 분류\n",
    "2. **결정 트리 (Decision Tree)** : 데이터 **균일도**에 따른 규칙 기반의 분류\n",
    "3. **서포트 벡터 머신 (SVM)** : 개별 클래스 간의 **최대 마진**을 효과적으로 찾아 분류\n",
    "4. **최소 근접 (Nearest Neighbor)** 알고리즘 : **근접 거리** 기준으로 분류\n",
    "\n",
    "### 1.3 이진 분류와 다중 분류\n",
    "\n",
    "- 이진 분류(Binary Classification): 예측하고자 하는 변수 어떤 기준에 대하여 참(True) 또는 거짓(False)의 값만을 가짐\n",
    "- 다중 분류(Multiclass Classification): 예측하고자 하는 변수가 가질 수 있는 값이 3개 이상임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 분류 모델\n",
    "\n",
    "### 2.1 로지스틱 회귀\n",
    "\n",
    "로지스틱 회귀는 이진 분류 문제를 푸는 대표적인 알고리즘으로, 샘플이 특정 클래스에 속할 확률을 추정하는 것을 목표로 한다.\n",
    "\n",
    "출력이 0과 1사이의 값을 가지면서 S자 형태로 그려지는 함수를 이용해야 한다\n",
    "\n",
    "그리하여 필요한 것이 시그모이드 함수\n",
    "\n",
    "시그모이드 함수: 로지스틱 함수: 출력이 0과 1사이의 값을 가지면서 S자 형태로 그려지는 함수\n",
    "\n",
    "- 입력값이 커지면 1에 수렴하고, 입력값이 작아지면 0에 수렴\n",
    "- 출력값이 0부터의 1까지의 값을 가짐\n",
    "\n",
    "→ **출력값이 특정값 이상이면 1(True), 특정값 이하면 0(False)로 정하면 이진 분류 문제를 풀기 위해서 사용할 수 있다!**\n",
    "\n",
    "---\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x)) # 시그모이드 함수 식을 그대로 표현한 것입니다\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y, 'g')\n",
    "plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\n",
    "plt.title('Sigmoid Function')\n",
    "plt.show()\n",
    "```\n",
    "---\n",
    "\n",
    "수식은 다음과 같이 정리가 되는데, σ(wx+b), 이는 결국 그래프의 모양을 바꾸는 건 w와 b라는 것\n",
    "→ w가 높아지면 기울기가 가팔라지고, b 값에 따라 평행이동하게 된다.\n",
    "\n",
    "그런 다음에 승산(odds) 이야기가 나오는데,,,사실 이해가 안되는 부분이 좀 있네. 세션 때 해설을 다시 들어봐야겠다. 왜 쓰는지는 알겠는데 어디서 쓰는건지 모르겠는 느낌\n",
    "\n",
    "### 2.2 결정 나무\n",
    "\n",
    "결정나무는 조건에 따라 데이터를 분류하며, 최종적으로 데이터가 순수한 label의 집합으로 구성될 때까지 분류를 반복하는 모델\n",
    "\n",
    "---\n",
    "[결정 나무 용어 정리]\n",
    "- **Root Node**: Decision Tree의 시작이 되는 노드\n",
    "- **Edge**: 노드와 노드를 연결하는 길목\n",
    "- **Leaf Nodes**: Tree의 가장 마지막 노드로, 모델에서 label에 해당\n",
    "- **Height(depth)**: Tree의 깊이로, 클수록 tree의 구조는 복잡해짐\n",
    "- **Level**: 노드의 절대적 위계, Root node의 level = 0, leaf node의 level = height - 1\n",
    "- **Parent**: 상대적으로 높은 위계의 노드\n",
    "- **Child**: 상대적으로 낮은 위계의 노드\n",
    "    - **Binary Tree**(이진 트리): Tree 중에서 children이 최대 2개인 tree\n",
    "---\n",
    "\n",
    "CART: 가장 대표적인 결정 나무 알고리즘으로, 데이터셋을 임계값을 기준으로 두 child로 나누는 알고리즘\n",
    "\n",
    "그러면 임계값의 기준은 무엇인가: 불순도(지니 계수)가 낮아지는 방향이다.\n",
    "\n",
    "그렇다면 불순도는 무엇인가: 분류하려는 데이터 집합에서 서로 다른 클래스(범주)가 섞여 있는 정도를 의미. CART 알고리즘에서는 불순도를 확인하기 위해 “지니 계수”를 사용.\n",
    "\n",
    "주요 절차:\n",
    "1. 임계값 설정\n",
    "2. 불순도 감소 알고리즘\n",
    "\n",
    "[실제 학습 시 고려해야 할 것들: 모수 설정, 차이점 시각화, Prunning]\n",
    "\n",
    "1. Parameter 설정\n",
    "\n",
    "    `min_samples_split` : 분할되기 위해 노드가 가져야 하는 최소 샘플(데이터) 수\\\n",
    "    `min_samples_leaf` : 리프 노드가 가지고 있어야 하는 최소 샘플 수\\\n",
    "    `min_weight_fraction_leaf` : `min_samples_leaf`와 같지만 가중치가 부여된 전체 샘플 수에서의 비율\\\n",
    "    `max_leaf_nodes`: 리프 노드의 최대 개수\\\n",
    "    `max_features` : 각 노드에서 분할에 사용할 특성의 최대 수\n",
    "\n",
    "2. 시각화\n",
    "\n",
    "*시각화를 수햄함으로 분류가 잘 이루어졌는지 확인할 수 있다*\n",
    "\n",
    "3. Prunning(가지치기)\\\n",
    ": 불필요한 노드 지우기\n",
    "\n",
    "    노드가 너무 많아지면 2번 왼쪽 그림처럼 과적합이 될 확률이 높습니다.\\\n",
    "    이럴 때 Prunning을 통해 하부 트리를 제거하여 일반화 성능을 높일 수 있습니다.\\\n",
    "    pruning을 하게 되면 깊이가 줄어들고 결과의 개수가 줄어듭니다.\n",
    "\n",
    "### 2.3 서포트 벡터 머신 (SVM)\n",
    "\n",
    ": 클래스를 분류할 수 있는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘\n",
    "\n",
    "*명확하게 분류할 수 있는 데이터 집단에서 뛰어난 성능을 보이며, 고차원 공간(다수의 feature)에서도 효과적으로 사용 가능하다.*\n",
    "\n",
    "SVM의 구성\n",
    "- Support vector: 구분하는 선과 가장 가까운 포인트\n",
    "- Decision Boundary(결정 경계): 집단을 구분하는 선\n",
    "- Margin: 선과 각 점의 거리\n",
    "\n",
    "최적의 선은 어떻게 정해지는가:\\\n",
    "결정 경계는 데이터로부터 가장 멀리 떨어져 있는게 좋다. 따라서 SVM은 Margin (거리)가 가장 큰 경우를 선택함으로써 최적의 선을 찾는다.\n",
    "\n",
    "### 2.4 KNN (K-Nearest Neighbor)\n",
    "\n",
    ": 데이터로부터 거리가 가까운 k개의 다른 데이터 레이블을 참조하여 분류하는 알고리즘\n",
    "\n",
    "계산 순서:\\\n",
    "1. 데이터 준비\\\n",
    "2. K 값 설정\n",
    "    * 원하는 K 값을 설정한다. K는 가장 가까운 이웃의 개수를 나타내며, 보통 홀수개로 설정한다.\\\n",
    "    *k를 짝수로 설정하였을 때, 동점이 발생하는 문제를 방지하기 위해 보통 홀수개로 설정한다*\\\n",
    "3. 거리 계산\n",
    "    * 새로운 데이터(즉, 예측하려는 데이터)가 주어지면, 이 값과 기존 모든 데이터 간의 거리를 계산합니다.\n",
    "    * 보통 이때 유클리드 거리, 맨해튼 거리 등을 사용합니다.\n",
    "4. 가장 가까운 K개의 이웃 선택\n",
    "    * 계산된 거리 중에서 가장 작은 거리 값을 가진 K개의 데이터를 선택합니다. 이 데이터 포인트들이 “가장 가까운 이웃”입니다.\n",
    "5. 분류하기\n",
    "    * K개의 이웃 중 가장 많이 등장하는 클래스가 예측 결과가 됩니다.\n",
    "\n",
    "[ 장단점 ]\n",
    "\n",
    "- 장점\n",
    "    - 훈련이 필요하지 않음\n",
    "    - 정보 손실 없음\n",
    "- 단점\n",
    "    - 쿼리를 처리하는 데에 시간이 오래 소요\n",
    "    - Not Robust : 이상치에 큰 영향을 받음 (각 데이터와의 거리에 기반해서 분류하는 모델이기 때문)\n",
    "\n",
    "*cf. 앙상블*\\\n",
    ": 여러 개의 개별 분류 모델들을 “결합”해 하나의 분류 모델보다 더 좋은 성능을 내는 머신러닝 기법\n",
    "\n",
    "**앙상블(Ensemble)의 종류**\n",
    "\n",
    "1. **보팅(Voting) : 다른 알고리즘**의 모델을 **병렬**로 사용\n",
    "2. **배깅(Bagging): 동일 알고리즘**의 모델을 **병렬**로 사용\n",
    "3. **부스팅(Boosting): 동일 알고리즘**의 모델을 **직렬**(순차적/Sequential)로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 분류 평가 지표\n",
    "\n",
    "### 3.1 혼동 행렬\n",
    ": 분류 모델의 예측 결과를 정확한 예측과 잘못된 예측으로 구분하여 나타낸 표(행렬)\n",
    "\n",
    "[ 혼동 행렬을 이용한 분류 모델 평가 지표 ]\n",
    "\n",
    "- **정확도 (Accuracy)**: **모든 가능한 예측 중** 참인 비율\n",
    "- **정밀도 (precision)**: **참이라고 예측한 경우** 실제 참의 비율\n",
    "- **재현도 (recall)**: **실제로 참인 경우 중** 참으로 예측하는 비율\n",
    "\n",
    "[ 정밀도(recall) & 재현도(precision) 그래프 ]\n",
    "\n",
    "정밀도&재현도 그래프에서 **두 value 값이 만나는 지점을 Threshold로 정하면, 예측 오류를 최소화**할 수 있다.\n",
    "\n",
    "- 각 점은 분류 모델이 해당 임계값에서 precision과 recall value를 어떻게 예측했는지 표시\n",
    "- 모든 임계값에 대해 점을 그려서 모델의 예측 성능을 시각적으로 파악할 수 있음\n",
    "- **두 value 값이 만나는 지점을 임계값으로 정할 때, 예측 오류를 최소화**할 수 있음\n",
    "\n",
    "### 3.2 F1-Score\n",
    ": 정밀도(precision)와 재현율(recall)의 조화 평균\n",
    "\n",
    "*조화평균을 사용하는 이유는 Precision(정밀도)과 Recall(재현율) 간의 균형을 효과적으로 평가하기 위해서이다.*\n",
    "\n",
    "### 3.3 ROC / AUC Curve\n",
    "\n",
    "* ROC Curve: 얼마나 분류가 잘 되었는가를 보여주는 그래프\n",
    "* AUC Curve: ROC와 x축 사이의 면적(적분값)\n",
    "    * 모델의 성능을 숫자로 나타낼 수 있다. 0에서 1사이의 값을 가지며, 1에 가까울수록 분류 성능이 좋은 것에 해당합니다.\n",
    "\n",
    "### 3.4 다중 분류 평가 지표 (참고)\n",
    "\n",
    "지금까지 살펴보았던 분류 평가 지표는 모두 이진 분류 평가 지표였다. 다중 분류 평가 지표를 간단하게 정리하자면, 위 이진 분류 평가 지표를 사용해서 클래스별로 점수를 구한 뒤, **이를 적절히 평균**을 내리는 것이다.  \n",
    "\n",
    "- 참고) 이진 분류와 다중 분류\n",
    "    - 이진 분류(Binary Classification): 예측하고자 하는 변수 어떤 기준에 대하여 참(True) 또는 거짓(False)의 값만을 가짐\n",
    "    - 다중 분류(Multiclass Classification): 예측하고자 하는 변수가 가질 수 있는 값이 3개 이상\n",
    "    ex) 손글씨로 쓴 숫자를 보고 0~9 중 하나로 분류하는 문제\n",
    "\n",
    "다중 분류 평가 지표\n",
    "- Macro average: 클래스별로 구한 평가 지표 평균\n",
    "- Weighted average: 클래스별로 구한 평가 지표 가중 평균\n",
    "- Micro average: **모든 클래스의 예측 결과를 더하여**  전체적인 성능을 평가하는 지표\n",
    "\n",
    "[ Macro average ]\n",
    "- **클래스별로 구한 평가 지표를 평균**하여 전체적인 모델 성능을 평가하는 지표\n",
    "- 모든 클래스에 **동등한 가중치**를 부여\n",
    "\n",
    "[ Weighted average ]\n",
    "- **각 클래스별로 구한 평가 지표를 클래스의 샘플 수로 가중 평균**하여, 전체적인 모델 성능을 평가하는 지표\n",
    "- 클래스에 따라 샘플 수가 다르기 때문에, **빈도가 높은 클래스에 더 큰 가중치**를 부여\n",
    "\n",
    "[ Micro average ]\n",
    "- **모든 클래스의 예측 결과를 더하여**  전체적인 성능을 평가하는 지표\n",
    "- 모든 클래스에 대해 **동등한 가중치**를 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 하이퍼파라미터 최적화\n",
    "\n",
    "### 4.1 하이퍼파라미터 최적화\n",
    "\n",
    "- 하이퍼파라미터: 모델 학습 과정에 반영되는 값으로, 학습 시작 전에 사용자가 직접 설정하는 변수\n",
    "- 하이퍼파라미터 최적화: 하이퍼파라미터 최적화란, Tuning을 거쳐 적절한 하이퍼파라미터를 찾음으로써 Model 성능을 향상시키는 것\n",
    "\n",
    "\n",
    "[ 하이퍼파라미터 최적화 과정 ]\n",
    "\n",
    "1. Hyperparameter 탐색 범위 설정 (최적 값을 찾고 싶은 하이퍼파라미터의 범위를 설정)\n",
    "2. 평가 지표 계산 함수(성능 평가 함수) 정의(탐색하려는 Hyperparameter를 인수로 받아 평가지표 값을 계산해주는 함수를 정의)\n",
    "3. 1단계에서 샘플링한 Hyperparameter 값을 사용하여 검증 데이터로 정확도 평가\n",
    "4. 위 단계를 특정 횟수 반복하며, 정확도 결과를 보고 하이퍼파라미터의 범위 좁힘\n",
    "\n",
    "\n",
    "### 4.2 하이퍼파라미터 최적화 방법\n",
    "\n",
    "- Grid Search: 정해진 범위에서 Hyperparameter를 모두 순회\n",
    "    - **장점**: 범위가 넓고 step이 작을수록 꼼꼼하게 전 범위를 탐색하니 최적해를 **정확히 찾을 수 있다**.\n",
    "    - **단점**: 시간이 너무 오래 걸린다.\n",
    "    - **적용**: 넓은 범위, 큰 step을 활용해 범위를 좁힌다.\n",
    "- Random Search: 정해진 범위에서 Hyperparameter를 무작위로 탐색\n",
    "    - **장점**: 속도가 Grid Search보다 빠르다.\n",
    "    - **단점**: 무작위라는 한계 때문에 **정확도가 떨어진다**. 따라서 Grid Search나 Bayesian Optimization에 비해 사용 빈도가 적다.\n",
    "- Bayesian Optimization: 사전 정보를 바탕으로 Hyperparameter 값을 확률적으로 추정하며 탐색\n",
    "    * 특징 : \"Gausain Process\"라는 통계학을 기반으로 만들어진 모델로, 여러 개의 하이퍼 파라미터들에 대해서, \"Aquisition Fucntion\"을 적용했을 때, \"가장 큰 값\"이 나올 확률이 높은 지점을 찾아냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "이제 질문에 대해 답하여 봅시다.\n",
    "\n",
    "1. 지도 학습과 비지도 학습의 차이\n",
    "    * 지도 학습: 문제와 정답을 모두 알려주고 공부시키는 방법\n",
    "    * 비지도 학습: 답을 가르쳐주지 않고 공부시키는 방법\n",
    "2. 회귀와 분류의 차이\n",
    "    - 회귀: 데이터가 **연속형** 변수를 예측하기 위해 사용될 때\n",
    "    - 분류: 데이터가 **범주형** 변수를 예측하기 위해 사용될 때\n",
    "3. 분류 모델의 네 가지 종류와 각 모델이 무엇인지 간단하게 정리\n",
    "    1. 로지스틱 회귀\n",
    "        * 로지스틱 회귀는 이진 분류 문제를 푸는 대표적인 알고리즘으로, 샘플이 특정 클래스에 속할 확률을 추정하는 것을 목표로 한다.\n",
    "    2. 결정 나무\n",
    "        * 결정나무는 조건에 따라 데이터를 분류하며, 최종적으로 데이터가 순수한 label의 집합으로 구성될 때까지 분류를 반복하는 모델\n",
    "    3. 서포트 벡터 머신\n",
    "        * 클래스를 분류할 수 있는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘\n",
    "    4. KNN\n",
    "        * 데이터로부터 거리가 가까운 k개의 다른 데이터 레이블을 참조하여 분류하는 알고리즘\n",
    "4. 분류 평가 지표에는 무엇이 있는가\n",
    "    * 혼동 행렬\n",
    "    * F1 - Score\n",
    "    * ROC / AUC Curve\n",
    "5. 하이퍼파라미터 최적화가 무엇인가\n",
    "    * Tuning을 거쳐 적절한 하이퍼파라미터를 찾음으로써 Model 성능을 향상시키는 것\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
