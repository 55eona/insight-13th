{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전과제\n",
    "* EDA & 전처리 교육자료 예습 후 Markdown으로 정리하기\n",
    "\n",
    "목표\n",
    "* 데이터 분석의 전반적인 과정 이해\n",
    "* EDA의 개념과 대상, 종류, 유형을 이해\n",
    "* 전처리의 개념과 과정을 이해\n",
    "* 파이썬을 이용해 전처리와 시각화를 수행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 데이터 분석의 프로세스\n",
    "데이터 분석은 다음과 같은 4 단계를 거친다.\n",
    "\n",
    "1. 데이터 수집\n",
    "* 다양한 형태의 **데이터를 수집**\n",
    "2. 데이터 탐색 *(=EDA)*\n",
    "* 수집한 **데이터를 여러가지 방식으로 파악**\\\n",
    "*사실 이 단계는 딱 하나의 단계라고 말하기 어렵다.*\\\n",
    "*데이터 전처리를 수행하기 전후에, 또 데이터 분석을 수행하며 꾸준히 데이터에 대한 탐색이 이루어지기 때문*\n",
    "3. 데이터 전처리\n",
    "* 데이터 분석(모델링)을 위해 **데이터를 적절한 방식으로 손질**하는 과정\n",
    "4. 데이터 모델링\n",
    "* 데이터로부터 **유용한 정보를 추출하기 위해 모델을 구축**하는 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA의 개념\n",
    "데이터 분석을 위해 데이터를 여러 가지 방식으로 파악하는 모든 과정\n",
    "* 시각화같은 도구를 통해서 패턴을 발견하거나\n",
    "* 데이터의 특이성을 확인하거나\n",
    "* 통계와 그래프(혹은 시각적 표현)으로 가설을 검정하는 과정 등을 통해\n",
    "\n",
    "**주어진 데이터에 대해 알아보는 것!**\n",
    "\n",
    "EDA는 *Exploratory Data Analysis*의 약자로, 탐색적 데이터 분석 과정을 일컫는다.\\\n",
    "이는 데이터를 분석하고 결과는 내는 과정에 있어서 지속적으로 해당 데이터에 대한 '탐색과 이해'를 기본으로 가져야 한다는 것을 의미한다.\\\n",
    "그렇기에 EDA 과정에서 데이터를 잘못 해석하고, 문제를 올바르게 정의하지 못한다면 향후 분석 과정에서 큰 어려움을 겪기에, EDA는 매우 중요한 단계라고 볼 수 있다.\n",
    "\n",
    "\n",
    "## (1) EDA 대상 (일변량/다변량)\n",
    "\n",
    "**- Univariate(일변량)**\\\n",
    "EDA를 통해 한 번에 파악하려는 변수가 한 개.\\\n",
    "데이터를 설명하고 그 안에 존재하는 패턴을 찾는 것이 주요 목적.\n",
    "\n",
    "**- Multi-variate(다변량)**\\\n",
    "EDA를 통해 한 번에 파악하려는 변수가 여러개.\\\n",
    "여러 변수들 간의 관계를 보는 것이 주요 목적.\\\n",
    "변수를 동시에 확인하기 전에 개별 데이터를 먼저 파악하는 것이 오류에 대처하기 용이하다.\n",
    "\n",
    "\n",
    "## (2) EDA 종류 (시각화/비시각화)\n",
    "\n",
    "**- Graphic(시각화)**\\\n",
    "차트 혹은 그림 등을 이용하여 데이터를 확인 하는 방법\n",
    "\n",
    "**- Non-Graphic(비시각화)**\\\n",
    "그래픽적인 요소를 사용하지 않고 주로 Summary Statistics를 통해 데이터를 확인하는 방법.\n",
    "\n",
    "→데이터를 그래프로 표현하게 되면 한눈에 데이터를 파악할 수 있으므로 graphic한 EDA를 통해 대략적인 형태를 파악할 수 있다.\\\n",
    "반면, 정확한 값이 필요하다면 non-graphic한 EDA를 사용해야 할 것\n",
    "\n",
    "\n",
    "## (3) EDA 유형\n",
    "\n",
    "대상과 종류에 따라 2*2로 총 4가지 유형이 구분됨.\n",
    "\n",
    "**① Uni-Non Graphic(일변량 비시각화)**: 주어진 데이터의 Distribution을 확인하는 것이 주 목적\\\n",
    "**② Uni-Graphic(일변량 시각화)**: 주어진 데이터를 전체적으로 살펴보는 것이 주 목적.\\\n",
    "**③ Multi-Non Graphic(다변량 비시각화)**: 주어진 둘 이상의 변수 간 관계를 확인하는 것이 주 목적.\\\n",
    "**④ Multi-Graphic(다변량 시각화)**: 주어진 둘 이상의 변수 간 관계를 전체적으로 살펴보는 것이 주 목적."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 확인\n",
    "\n",
    "## 데이터 읽기/쓰기\n",
    "데이터를 분석하려면 데이터가 필요하다.\n",
    "\n",
    "읽기: 작업 공간에 데이터를 불러오는 것\\\n",
    "쓰기: 새로운 데이터를 만드는 것\n",
    "\n",
    "### (1) 절대경로와 상대경로\n",
    "데이터를 읽어오는 형태는 다음과 같다.\n",
    "\n",
    "---\n",
    "```python\n",
    "# import pandas as pd\n",
    "pd.read_csv(\"파일경로/파일이름.csv\")\n",
    "```\n",
    "---\n",
    "\n",
    "여기서 파일경로를 나타내는 방법에는\\\n",
    "① 절대 경로와 ② 상대 경로 두 가지가 있다.\n",
    "\n",
    "① 절대 경로\\\n",
    "처음(root파일)부터 시작하여 목적지까지 전체적인 경로 (URL), 해당 파일까지의 모든 경로.\n",
    "\n",
    "*ex) C:\\Users\\Allenkim\\insight\\2차시 실습\\titanic.csv*\n",
    "\n",
    "② 상대 경로\\\n",
    "현재 작업하고 있는 디렉터리를 기준으로 상하위 디렉터리와 같이 상대적으로 표현되는 경로.\n",
    "\n",
    "* ./: 현재 디렉터리\\\n",
    "* ../: 상위 디렉터리\\\n",
    "* /: 최상위(루트) 디렉터리\n",
    "\n",
    "---\n",
    "```python\n",
    "pd.read_csv('./titanic.csv')\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "### (2) 데이터 입출력\n",
    "\n",
    "데이터 파일의 형식에 따라 데이터를 읽고 쓰는 함수가 다르다.\n",
    "|File Format|Reader|Writer|\n",
    "|------|---|---|\n",
    "|CSV|read_csv|to_csv|\n",
    "|JSON|read_json|to_json|\n",
    "|HTML|read_html|to_html|\n",
    "|MS EXCEL|read_excel|to_excel|\n",
    "|SQL|read_sql|to_sql|\n",
    "\n",
    "→이중에서 CSV파일과 EXCEL 파일을 앞으로 많이 다루게 된다.\n",
    "\n",
    "\n",
    "### (3) CSV와 EXCEL\n",
    "\n",
    "* CSV 파일: comma separted value의 약자\\\n",
    "데이터를 쉼표(,)로 구분하고 있는 덱스트 파일.\n",
    "\n",
    "**데이터의 크기가 작고 압축이 용이하여 가장 널리 사용되는 데이터 형식**\n",
    "\n",
    "* EXCEL 파일: 행과 열이 데이터프레임의 행, 열로 일대일 대응\n",
    "\n",
    "여러 개의 시트로 구성된 데이터를 읽을 때 불러올 특정 시트를 설정할 수 있다.\n",
    "\n",
    "*ex) sheet_name = 'Sheet 2'*\n",
    "\n",
    "여러 sheet를 불러올 때는 list로 받으면 된다.\n",
    "\n",
    "*ex) sheet_name = ['Sheet 1', 'Sheet 2']*\n",
    "\n",
    "---\n",
    "```python\n",
    "pd.read_excel('파일경로/파일이름.xlsx',sheet_name='불러올 시트')\n",
    "```\n",
    "---\n",
    "\n",
    "→ 두 형식 모두 pandas를 통해 DataFrame 형식으로 읽어올 수 있다.\n",
    "\n",
    "## 데이터셋 파악하기\n",
    "\n",
    "### (1) 데이터 프레임 보기\n",
    "\n",
    "`head(n)`: 데이터 프레임 상위 n개 데이터 확인 가능\n",
    "\n",
    "### (2) 데이터 변수 확인\n",
    "\n",
    "* 변수 = 데이터 프레임의 column = feature\n",
    "\n",
    "변수 정의 확인: 어떤 정보를 가지는 변수인지 확인\n",
    "\n",
    "변수 유형 확인: 질적/번주형(Categorical Data)과 양적/수치형(Numerical Data)으로 구분\n",
    "|범주형(Categorical): 몇 개의 범주로 나누어진 데이터|수치형(Numerical): 숫자로 표현되는 데이터|\n",
    "|---|---|\n",
    "|명목형(Nominal): 성별, 성공여부, 혈액형 등 순서 없이 단순히 분류된 자료|이상형(Discrete): 이산적인 값으로, 정수 단위로 떨어져 셀 수 있는 데이터|\n",
    "|순서형(Ordinal): 범주형 데이터 중 그들 사이에 순서 관계가 존재하는 자료|연속형(Continuous): 연속적인 값을 갖는 데이터로 신장, 체중 등|\n",
    "\n",
    "\n",
    "변수 데이터 형식(자료형) 확인: 날짜, 수치, 텍스트, 이미지 등의 구분\n",
    "|판다스 자료형|파이썬 자료형|비고|\n",
    "|---|---|---|\n",
    "|int64|int|정수형 데이터|\n",
    "|float64|float|실수형 데이터(소수점이 있는 수)|\n",
    "|object|string|문자열 데이터|\n",
    "|datetime64, timedelta64|없음(datetime 라이브러리 활용)|시간 데이터|\n",
    "\n",
    "판다스는 Numpy를 기반으로 만들어졌기 때문에 Numpy에서 사용하는 자료형을 기본적으로 사용할 수 있다. 파이썬의 기본 자료형과 비슷하지만 시간을 나타내는 datetime64와 같은 자료형이 있다는 점에서 일부 차이가 있다.\n",
    "\n",
    "### (3) 데이터 분포 확인\n",
    "\n",
    "단변수 분석: 원시 데이터(raw data)의 평균값, 최빈값, 중간값 등 변수들의 분포를 산포도, 박스 플롯, 히스토그램 등의 그래프를 통해 단변수, 즉 하나의 데이터 분포를 확인해 분석할 수 있는 방법\\\n",
    "원시 데이터 분포의 확인을 통해 **전처리 아이디어**를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 전처리\n",
    "\n",
    "데이터 전처리: 데이터 분석(모델링)을 위해 데이터를 적절한 방식으로 손질하는 과정\n",
    "\n",
    "머신러닝 등 데이터 분석의 정확도는 데이터의 품질에 의해 좌우된다. 따라서 데이터 품질을 높이기 위해서는 누락 데이터, 중복 데이터 등 오류를 수정하고 분석 목적에 맞게 변형하는 과정이 중요하다.\n",
    "\n",
    "*cf 전처리의 여러 기술*\n",
    "\n",
    "*데이터 정제(cleansing): 데이터에서 누락된 결측값을 보완하고 튀는 값을 이상값으로 제거해 데이터를 깨끗하게 만드는 기술*\\\n",
    "*데이터 변환(Transformation): 데이터 분석을 보다 쉽게 하기 위해 데이터를 변환해 일관성을 확보하고 데이터의 중복을 최소화해 데이터 분석시간을 절약하는 기술*\n",
    "*데이터 필터링(Filtering): 데이터의 오류를 발견하고 삭제 및 보정을 통해 데이터의 품질 향상시키는 기술*\\\n",
    "*데이터 통합(Integration): 데이터 분석을 수월하게 하기 위해 유사한 성질의 데이터를 연계하는 증 데이터를 통합하는 기술*\n",
    "\n",
    "## 결측값 처리\n",
    "\n",
    "결측값이란?\\\n",
    "데이터 수집 과정에서 측정되지 않거나 누락된 데이터\n",
    "\n",
    "변수의 결측값이 있는 상태로 데이터 분석 모델을 만들면 변수 간의 관계가 왜곡될 수 있어 모델의 정확성이 떨어지게 되고, 이는 데이터의 시각적 표현에도 문제를 일으킬 수 있다.\n",
    "\n",
    "### (1) 결측값 확인\n",
    "\n",
    "데이터 프레임 상에서 `Nan`, `?`. `0` 등의 값으로 나타나 있는 결측값을 확인합니다.\n",
    "이때 0의 경우, **진짜 0의 값으로 측정된 것인지, 결측되어 0으로 표기된 것**뿐인지 확인해볼 필요 있다.\n",
    "\n",
    "* `info()`: 데이터 프레임의 요약 정보를 출력 → 각 열에 속하는 유효한 값(NaN 값이 아닌 non-null)의 개수를 보여줌\n",
    "* `value_counts(dropna=False)`: 각 열의 결측값을 포함한 전체 데이터 확인 가능\n",
    "* `isnull()`: 누락 데이터면 True, 유효한 데이터면 False 반환\\\n",
    "→ `df.isnull().sum`의 형식으로 자주 사용!\n",
    ">나중에 찾아볼 필요 有\n",
    "* `notnull()`: 유효 데이터면 True, 누락 데이터면 False 반환\n",
    "*`replace()`: 결측값이 NaN이 아니라 '0', '?'등으로 입력되기도 하는데, 이때 replace를 활용하면, NaN으로 변환할 수 있음\n",
    "\n",
    "### (2) 결측값 처리\n",
    "\n",
    "#### a. 삭제\n",
    "    \n",
    "데이터가 있는 행 또는 열 삭제\n",
    "\n",
    "`dropna()`: DataFrame내의 결측값이 포함된 레이블을 제거하는 Method\n",
    "\n",
    "* <삭제 기준>\n",
    "* `axis = 0`: 결측값이 있는 행 삭제 - 분석 대상의 관측값(레코드)을 제거, 기본값\n",
    "* `axis = 1`: 결측값이 있는 열 삭제 - 분석 대상이 갖는 특성(변수)를 삭제\n",
    "\n",
    "* <삭제 조건>\n",
    "* `how = 'any'`: 하나라도 결측값 존재시 삭제, 기본값\n",
    "* `how = 'all'`: 모든 값이 결측값일때만 삭제\n",
    "\n",
    "* <특정 column 기준으로 삭제>\n",
    "* `subset = ['column명']`\n",
    "*cf, `axis = 0`일때만 적용 가능*\n",
    "\n",
    "* <원본 데이터 변경 여부>\n",
    "* `inplace = False`: 반영한 내용을 새로운 DataFrame으로 반환, 기본값\n",
    "* `inplace = True`: 원본 DataFrame 자체를 변경하여 반영\n",
    "\n",
    "**삭제 시 주의할 점**\\\n",
    "삭제는 결측값이 무작위로 발생한 경우에 사용한다.\\\n",
    "결측값이 무작위로 발생한 것이 아닌데 관측치를 삭제한 데이터를 사용할 경우, 왜곡된 모델이 생설될 수 있습니다.\n",
    "\n",
    "\n",
    "#### b. 대체\n",
    "\n",
    "결측값을 다른 값으로 대체\\\n",
    "→ 평균값, 최빈값 등을 활용\n",
    "\n",
    "`fillna()`: 결측값 대체\n",
    "\n",
    "* 일괄 대체: 모든 변수들 일괄적으로 같은 값으로 대체\\\n",
    "*ex. 모든 변수들의 평균값을 구해 일괄적으로 대체*\\\n",
    "\n",
    "---\n",
    "```python\n",
    "# 평균값으로 일괄 대체\n",
    "a = df[\"열1\"].mean(axis=0)\n",
    "df[\"열1\"].fillna(a, inplace=True) # 원본객체 변경됨\n",
    "```\n",
    "---\n",
    "\n",
    "* 유사 유형 대체: 범주형 변수들을 활용해 유사한 범주에 따라 다른 값으로 대체\\\n",
    "*ex. 범주형 변수들을 활용해 유사한 유형의 평균값으로 대체*\n",
    "\n",
    "---\n",
    "```python\n",
    "# 범주별로 그룹화하여 평균값 계산\n",
    "category_means = df.groupby('Category')[\"Value\"].mean()\n",
    "\n",
    "# 결측값을 범주의 평균값으로 대체\n",
    "df[\"Value\"] = df.groupby('Category')['Value'].apply(lambda x: x.fillna(x.mean()))\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "## 이상치 처리\n",
    "\n",
    "이상치 = 관측된 데이터의 범위에서 많이 벗어난 값\n",
    "\n",
    "이상치를 처리해주지 않으면 데이터 분석에 큰 영향을 끼치게 되기 때문에 알맞은 처리를 진행해주어야 합니다.\n",
    "\n",
    "### (1) 이상치 확인\n",
    "\n",
    "a. 통계를 통해 확인: describe()\\\n",
    "b. 시각화를 통해 확인: Boxplot\n",
    "\n",
    "---\n",
    "```python\n",
    "for i in df.descibe().columns:\n",
    "    df[[i]].boxplot()\n",
    "    plt.show()\n",
    "```\n",
    "---\n",
    "\n",
    "c. Z-score을 통해 확인\n",
    "* 데이터를 평균(0)과 표준편차(1)로 정규화하여, 평균으로부터 얼마나 떨어져있는지 나타냄\\\n",
    "* 평균에 가까울수록 0에 가깝고 멀어질수록 Z-score가 커짐\\\n",
    "* Z-score가 특정 기준값(*보통 2~3*)을 넘어가는 데이터를 이상치로 간주\n",
    "\n",
    "d. Tukey Fences을 통해 확인: 사분위 범위(IQR, Interquartile Range)를 기반으로, 두 가지 경우에 이상치라고 판단\n",
    "* Q1 - (1.5 * IQR) 미만\n",
    "* Q3 + (1.5 * IQR) 초과\n",
    "Tukey Fences(튜키 펜스)는 이상치를 식별하는 통계적인 방법 중 하나로, 데이터의 사분위 범위를 기반으로 한다.\\\n",
    "IQR은 데이터의 중간 50% 범위로 측정하며, 이는 데이터의 상위 25%와 하위 25%를 제외한 중간 범위를 의미한다.\\\n",
    "→ 이상치 판단 기준이 될 수 있다.\n",
    "\n",
    "### (2) 이상치 제거\n",
    "\n",
    "a. 전체 삭제\n",
    "* 이상값이 Human error에 의해서 발생한 경우에는 해당 관측치를 삭제하면 된다.\n",
    "* 단순 오타나, 주관식 설문 등의 비현실적인 응답, 데이터 처리 과정에서의 오류 등의 경우에 사용한다.\n",
    "\n",
    "b. 다른 값으로 대체\n",
    "* 절대적인 관측치의 숫자가 작은 경우, 단순 삭제를 통해 이상치를 제거하면 관측치의 절대량이 작아져 신뢰성 문제가 발생한다.\n",
    "* 이런 경우 이상값이 Human error에 의해 발생했더라도 관측치를 삭제하는 대신 다른 값(평균 등)으로 대체하거나, 결측값과 유사하게 다른 변수들을 사용해서 예측 모델을 만들고, 이상값을 예측한 후 해당 값으로 대체하는 방법도 사용할 수 있다.\n",
    "\n",
    "c. 변수화\n",
    "* 이상값이 자연 발생한 경우, 단순 삭제나 대체의 방법을 통해 수립된 모델은 설명/예측하고자 하는 현상을 잘 설명하지 못할 수 있다.\n",
    "\n",
    "d. 리샘플링\n",
    "* 해당 이상값을 분리해서 모델을 만드는 방법\n",
    "→ 이상값을 포함한 모델과 제외한 모델을 모두 만들고 각각의 모델에 대한 설명을 다는 것\n",
    "→ 즉, 자연발생한 이상값에 별다른 특이점이 발견되지 않는다면, 단순 제외 보다는 케이스를 분리하여 분석하는 것을 추천\n",
    "\n",
    "## 피처 엔지니어링 (변수 가공)\n",
    "\n",
    "피처 엔지니어링: 도메인 지식과 기존의 변수를 사용해 기존의 데이터에 정보를 추가하는 일련의 과정\n",
    "\n",
    "피처 엔지니어링의 방식\n",
    "\n",
    "1) 레이블인코딩, 원핫인코딩: 텍스트로 주어지는 값을 숫자로 바꾸는 작업\n",
    "* 레이블인코딩: 범주형 변수를 0부터 N-1까지의 숫자로 변환\n",
    "* 원핫인코딩: 범주형 변수를 이진 벡터(0과 1)로 변환\n",
    "\n",
    "2) 구간화\n",
    "연속 데이터를 그대로 사용하기 보다는 일정한 구간으로 나눠서 분석하는 것이 효율적인 경우가 있다.\n",
    "\n",
    "3) 변환\n",
    "기존의 피처를 다른 피처로 변환하여 변수를 추가한다.\\\n",
    "기존 데이터의 특성 또는 다른 정보를 이용해 다른 데이터로 변환 및 분석하기 위한 것으로, 분석 대상 데이터의 구간화 방법과 마찬가지로 정해진 원칙이 있는 방법이 아니기에 **분석가의 데이터 특성에 대한 이해도**에 따라 다양한 데이터가 생성될 수 있다.\n",
    "\n",
    "4) 스케일링\n",
    "서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업\\\n",
    "숫자의 상대적인 크기 차이로 인한 결과의 왜곡을 방지하기 위해, 각 변수에 속하는 데이터 값을 동일한 크기 기준으로 나눈 비율로 나타내는 정규화 작업을 수행한다.\n",
    "\n",
    "a. StandardScaler (표준화)\\\n",
    "각 feature의 평균을 0, 분산을 1로 변경하여, 모든 피처들이 같은 스케일을 갖게 된다.\\\n",
    "→ 정규분포를 따른다고 가정하는 기술에 적합!\n",
    "\n",
    "b. MinMaxScaler\n",
    "모든 feature가 0과 1사이에 위치하게 만든다.\\\n",
    "→ 데이터가 서로 다른 비율의 속성으로 구성되어 있을 때, 같은 비율로 속성을 맞춤.\n",
    "→ 연산 속도를 높이고 알고리즘 최적화하는 데에 효과적!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 시각화\n",
    "\n",
    "시각화: 데이터나 정보를 시각적인 형태로 표현하는 과정 또는 결과물\\\n",
    "시각화를 통해 숫자와 텍스트로 표현되는 정보를 그래프, 차트, 그림, 도표 등의 시각적 요소로 변환하고, 이를 통해 데이터 패턴, 관계, 추세 등을 쉽게 파악할 수 있습니다.\n",
    "\n",
    "시각적 분석기법을 활용할 때 일반적으로 Matplotlib과 Seaborn을 사용한다.\\\n",
    "* Matplotlib은 Python 프로그래밍 언어 및 수학적 확장 NumPy 라이브러리를 활용한 플로팅(그래프를 그리기 위한) 라이브러리\\\n",
    "**Seaborn은 matplotlib을 기반으로 만들어져 통계 데이터 시각화에 최적화된 인기 라이브러리**\n",
    "\n",
    "## 파라미터\n",
    "컴퓨터 프로그래밍에서 매개변수랑 프로그래밍된 함수의 입력값을 의미한다.\\\n",
    "함수를 내가 원하는 조건에 맞춰서 이용하기 위해 `파라미터 = 인자` 형식으로 전달한다.\n",
    "\n",
    "## 다양한 그래프 톺아보기\n",
    "\n",
    "(1) boxplot(상자 수염 그림) : 사분위수와 이상치를 보여주는 그래프\\\n",
    "(2-1) countplot: 범주형 변수의 빈도수를 확인하는 그래프\\\n",
    "(2-2) histplot: 도수분포표를 그래프로 나타낸 것, 수치형 변수의 구간별 빈도수를 보여준다.\\\n",
    "(3) displot, kdeplot (커널밀도추정 그래프): 히스토그램을 연속적으로 곡선으로 연결한 그래프\\\n",
    "(4) barplot, pointplot: 범주형 데이터 값 x에 따른 수치형 데이터 값 y의 평균 값을 제공한다. pointplot은 막대 그래프와 모양만 다르고 동일한 정보를 제공한다.\\\n",
    "(5) scatterplot(산점도 그래프), regplot(회귀선이 추가된 산점도 그래프): 두 변수 간의 관계를 시각화하기 위해 사용되는 그래프 유형. regplot은 두 개의 연속 변수 사이의 산점도를 그리고 회귀선을 함께 나타내는 그래프\\\n",
    "(6) catplot: categoryplot의 준말로, 수치형 데이터와 범주형 데이터의 관계를 볼 때 주로 사용한다.\\\n",
    "(7) pieplot: 데이터의 부분과 전체 간의 비율을 표현하는 그래프. 주료 비율을 강조하기 위해 사용됨. 모든 데이터가 합쳐져 전체를 이루는 경우에 효과적으로 활용됨.\\\n",
    "(8) heatmap: 변수간 상관계수를 직관적으로 볼 수 있는 그래프. `corr()` 메서드로 변수 간의 상관계수를 구하고 이를 히트맵에 표현할 수 있다\\\n",
    "\n",
    "*cf.상관계수란?*\\\n",
    "*두 수치형 변수 사이의 상관 관계의 정도를 수치적으로 나타낸 계수, 즉 두 변수간 서로 영향을 주는 정도. -1과 1 사이의 값을 가지며 절댓값 1에 가까울 수록 큰 상관계수를 가진다고 판단함*\n",
    "\n",
    "(9) violinplot: 박스 플롯과 커널밀도추정 함수 그래프를 합쳐 놓은 그래프"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
