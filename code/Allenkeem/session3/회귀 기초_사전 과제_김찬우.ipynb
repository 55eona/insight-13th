{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa1ee5c-6e16-471a-b6ac-14f4cb40f8fb",
   "metadata": {},
   "source": [
    "# [4차 세션] 회귀 기초 사전 과제\n",
    "\n",
    "들어가야할 내용:\n",
    "* 선형회귀 (단순, 다중선형회귀)\n",
    "* 경사하강법\n",
    "* 다중공선성\n",
    "* 규제선형모델\n",
    "* 모델평가방법\n",
    "\n",
    "## 머신러닝과 모델링\n",
    "\n",
    "### 1. 머신러닝\n",
    "\n",
    "* 머신러닝: 새로운 데이터를 예측하거나 결정을 내릴 수 잇도록 하는 기술\\\n",
    "    *머신러닝을 통해 데이터에서 패턴을 찾아내고, 이 패턴을 새로운 데이터에 대한 예측을 가능하게 한다.*\n",
    "  * 비지도학습: 정답이 없는 데이터로 학습하는 방식\n",
    "  * 지도학습: 정답이 있는 데이터로 학습하는 방식\n",
    "\n",
    "\n",
    "### 2. 모델링\n",
    "\n",
    "전체 데이터를 훈련 데이터와 테스트 데이터로 나눈다.\n",
    "\n",
    "* 훈련 데이터: 모델을 학습(훈련)시킬 때 쓰는 데이터\n",
    "* 테스트 데이터: 학습된 모델을 검증하기 위해 사용하는 데이터 = unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674e428-7e0e-4da9-bc42-108e67f123f9",
   "metadata": {},
   "source": [
    "## 회귀와 모델링\n",
    "\n",
    "### 1. 회귀\n",
    "\n",
    "* 회귀: 데이터에서 패턴을 찾아내어 미래 값을 예측하는 것\n",
    "* 집값, 주식 가격 등과 같은 연속적인 숫자를 다룬다.\n",
    "\n",
    "    *만약 알고 싶은게 불연속적이라면?*\\\n",
    "    ***분류**를 이용한다. 이는 분류세션에서 다시...*\n",
    "\n",
    "### 2. 회귀의 종류\n",
    "\n",
    "* **선형회귀**\n",
    "* 비선형 회귀\n",
    "* 릿지회귀\n",
    "* 라쏘회귀\n",
    "* 다항회귀\n",
    "* etc...\n",
    "\n",
    "**이번 세션의 주안점은 선형회귀**\n",
    "\n",
    "### 3. 선형회귀\n",
    "\n",
    "선형회귀: 데이터를 가장 잘 설명하는 **회귀선**을 찾아 예측하는 과정\n",
    "\n",
    "예측을 위해 사용하는 변수 = 독립 변수\\\n",
    "예측하고자 하는 변수 = 종속 변수\n",
    "\n",
    "예컨대 마케팅 비용에 따른 매출액이라면 마케팅 비용은 독립 변수! 매출은 종속 변수!\n",
    "\n",
    "*cf. '선형'이 꼭 직선을 의미하는 것은 아님. 정확히 말하면 모델이 파라미터(계수)에 대해 선형이라는 것. 예를 들어 함수가 이차방정식이거나 하는 식이어도 여전히 선형이라고 한다. 함수의 모양이 곡선이더라도, 계수들이 선형적으로 결합되어 있기에. 그러나 지수함수처럼 계수가 지수에 들어가면, 이는 비선형회귀이다. 계수에 대해 비선형이기에.*\n",
    "\n",
    "*선형 회귀 기본가정*\n",
    "1. 선형성: 종속 변수와 독립 변수들 간의 관계가 선형적이어야 한다.\n",
    "2. 독립성: 각각의 관측치는 서로 독립적이어야 한다.\n",
    "3. 등분산성: 모든 독립 변수들의 수준에서 오차 항의 분산이 일정해야 한다.\n",
    "4. 정규성: 오차 항이 정규 분포를 따른다.\n",
    "\n",
    "선형회귀의 종류\n",
    "* 단순 선형 회귀: 데이터를 직선 형태로 표현하는 경우\n",
    "* 다중 선형 회귀: 데이터를 두 개 이상의 독립 변수를 사용하여 표현하는 경우\n",
    "\n",
    "#### (1) 단순선형회귀\n",
    "\n",
    "* 하나의 독립변수만 존재하는 선형 회귀\n",
    "* 데이터를 가장 잘 설명하는 직선 y = aX + b를 찾는 과정에 해당.\n",
    "* a는 회귀계수(기울기), b는 y절편을 의미\n",
    "* 적절한 a와 b를 찾는 과정   *그렇다면 how?*\n",
    "\n",
    "**By 최소제곱법**\\\n",
    ": 잔차의 제곱의 합이 최소가 되는 지점으로 최적의 회귀선을 구하는 방식\n",
    "\n",
    "최소제곱법을 통해 구한 함수가 복잡하여 최소값을 구하기 어려울 때 보통 **경사하강법**을 사용한다.\\\n",
    "잔차제곱을 줄이려는 목적은 동일하다.\\\n",
    "계산 과정이 복잡하여 최소로 가는 과정에서 경사하강법을 사용한다.\n",
    "\n",
    "*cf.*\\\n",
    "*목적함수: 최적화를 위해 오차를 최소화하는 것이 목적인 함수. 즉, 최적화하려는 대상을 수학적으로 표현한 함수*\\\n",
    "*ex. 비용의 최소화? 매출의 최대화?*\n",
    "\n",
    "*손실함수: 개별 데이터 샘플에 대한 오차를 측정하는 함수*\\\n",
    "*비용함수: 전체 데이터셋에서 평균적인 손실을 측정하는 함수*\n",
    "\n",
    "**모델을 학습할 때 대부분은 비용함수를 최소화하는 것이 목표! : 비용함수 = 목적함수**\\\n",
    "*cf. 강화학습 또는 생성모델에서는 꼭 비용함수가 목적함수가 아닐 수 있음*\n",
    "\n",
    "(1)\n",
    "\n",
    "오차라는 게 결국 예측한 y 데이터에서 실제 y 값을 빼서 제곱한 것을 평균 낸 것. \n",
    "\n",
    "예측한 y 데이터에서 실제 y 값을 뺀 건 일차함수인데 제곱했으니까 이차함수가 되고 크게 보면 이차방정식의 모양을 보임.\\\n",
    "**기울기가 0 => loss가 0**인 지점.\n",
    "\n",
    "(2)\n",
    "\n",
    "*얼마나 빨리 하강할 것인가: **학습률***\n",
    "\n",
    "학습률이 너무 작다, 모델이 수렴할 때까지 너무 오랜 시간이 걸릴 것이고, 너무 크다면 수렴하지 못하고 계속 발산하게 될 것.\n",
    "\n",
    "(3)\n",
    "\n",
    "모든 비용함수가 하나의 최솟값을 가지지 않을 수 있다.\\\n",
    "    → Local minima에 갇힐 수 있음.\n",
    "\n",
    "(4)\n",
    "\n",
    "해결법 - 모멘텀\\\n",
    "**이전의 기울기, 이동해오던 방향을 고려하도록 관성 부여**\\\n",
    "기울기에 관성을 부여하여 지역 최소값을 탈출할 수 있게 함.\n",
    "\n",
    "이를 위해서는 이동량(속도)를 업데이트하여 매개변수 w를 꾸준히 업데이트한다.\n",
    "\n",
    "#### (2) 다중선형회귀분석\n",
    "\n",
    "독립 변수 x가 2개인 경우 3차원 공간에 표현되고, **회귀선은 평면**이 된다.\n",
    "\n",
    "* 단순과 마찬가지로 최소제곱법을 사용할 수 있다. 관측치와 평면간의 차이가 잔차가 된다.\n",
    "\n",
    "**다중공선성**\n",
    "\n",
    "다중공선성은 회귀 분석에서 독립 변수들 간에 상관관계가 큰 경우 발생한다.\\\n",
    "**→ 다중공선성이 높은 경우 어떤 독립 변수가 종속 변수에 얼마나 영향을 미치는지를 정확하게 구분하기 어렵기에 회귀 모델은 어떤 변수의 영향을 반영해야 할지 불확실해지고, 그 결과 회귀분석의 정확도가 낮아진다.**\n",
    "\n",
    "다중공선성을 파악하는 방법에는 두 가지가 있다.\n",
    "1. 상관계수: 기존에 배운 히트맵이나 산점도를 통해 시각적으로 파악 가능\n",
    "2. VIF지수 (분산 팽창 인수): 회귀 모델의 결정 계수 R square를 사용하여 계산된다.  VIF가 높다면 다중공선성이 존재한다고 판단 내린다.\n",
    "\n",
    "다중공선성에는 다음과 같은 방법으로 대처할 수 있다.\n",
    "1. 변수 제거 (변수 선택법)\n",
    "   * 독립변수로서 사용할 변수를 선택하는 방법\n",
    "2. 변수 변환\n",
    "   * 변수들을 더하거나 빼서 새로운 변수 생성\n",
    "   * 독립변수를 더하거나 빼더라도 문제가 없는 경우\n",
    "   * ex) 남편의 수입과 아내의 수입이 서로 상관성이 높다면, 두 개를 더해 가족 수입이라는 하나의 변수로 투입\n",
    "3. 규제 선형 모델 활용\n",
    "   * 릿지, 라쏘, 엘라스틱넷 등의 방법을 통해 모델의 복잡도를 줄이는 방법 사용\n",
    "4. PCA (주성분분석)\n",
    "   * 데이터의 차원을 축소하는 데 사용되는 통계적 기법\n",
    "    1. 데이터 전처리 (정규화)\n",
    "        * 평균을 0으로 조정\n",
    "        * 표준화 (스케일링): 분산을 1로 맞춰 단위 영향 제거\n",
    "    2. 주성분 찾기\n",
    "        * 공분산 행렬 계산 & 고유벡터, 고유값 추출\n",
    "    3. 차원 축소\n",
    "        * 주성분 축에 데이터 투영\n",
    "    4. 복원 가능 (optional)\n",
    "        * 원래 좌표계로 변환 가능 (표준화 해제)\n",
    "      \n",
    "### 4. 규제선형모델\n",
    "\n",
    "모델이 학습 데이터에 과적합되지 않도록 규제를 가하고자 등장한 모델\\\n",
    "→ 선형 회귀 모델에서는 특성에 곱해지는 계수(기울기)의 크기를 조정하는 것\n",
    "\n",
    "특히 여러 독립 변수를 사용하여 예측하기에 모델이 복잡해지는 다중회귀에서 과적합될 가능성 증가함\n",
    "\n",
    "기존 선형 모델에서는 최소제곱법을 통해 비용 함수를 설정하였는데, 이로 인해 회귀 계수가 쉽게 커지게 되어 과적합이 발생한다.\\\n",
    "→**특정 피처에 과하게 의존**한다는 뜻\n",
    "\n",
    "그렇기에 오류는 최대한 줄이면서도, 특정 변수에 집착하지 않도록 균형을 맞춰 과적합을 방지해야함.\n",
    "\n",
    "정리하면 다음과 같다.\n",
    "1. 과적합 방지를 위해서는 회귀계수에 대한 조치가 필요하고\n",
    "2. 그렇기에 회귀계수에 페널티를 부여하는 규제선형모델을 사용한다.\n",
    "3. 그 방식에 따라\n",
    "   * 릿지 회귀\n",
    "   * 라쏘 회귀\n",
    "   * 엘라스틱넷 회귀가 존재한다.\n",
    "\n",
    "#### 릿지 회귀 (L2 규제)\n",
    "* L2규제는 회귀계수 크기에 따라 다른 힘으로 누르는 장치\n",
    "* 회귀계수의 제곱을 페널티로 주기 때문에, 큰 회귀계수일 수록 훨씬 강한 페널티를 받는다.\n",
    "* 모든 계수의 크기는 작아지지만, 완전히 0이 되지는 않음\n",
    "\n",
    "**모든 변수를 유지하며, 크기는 전체적으로 줄여 과적합을 방지!**\n",
    "\n",
    "#### 라쏘 회귀 (L1 규제)\n",
    "* L1 규제는 회귀계수 크기에 상관없이 같은 힘으로 누르는 장치\n",
    "* 회귀계수의 절댓값을 페널티로 주기에, 작은 계수는 힘을 이기지 못해 0이 되는 것이 가능함.\n",
    "\n",
    "**비중이 낮은 변수(작은 계수)는 아예 사라지고, 중요한 변수만 남는다!**\n",
    "\n",
    "#### 엘라스틱넷 (L1 + L2)\n",
    "* L1의 변수 선택 기능 (일부 계수를 0으로 만듦)과 L2의 규제 기능(큰 계수를 줄임)을 동시에 활용한다.\n",
    "* L1은 상관관계가 높은 변수 중 일부만 선택하는 경향이 있음\n",
    "* L2를 함께 적용하여 상관관계가 높은 변수를 함께 고려하면서도 최적의 규제가 가능하다.!\n",
    "\n",
    "**안정적인 예측과 희소성을 동시에 달성!**\n",
    "\n",
    "### 모델 평가방법\n",
    "\n",
    "#### 성능 평가 지표\n",
    "\n",
    "1. 평균 제곱 오차 (Mean Squared Error, MSE)\n",
    "   * 회귀 분석에서 모델의 예측값과 실제 관측값 사이의 **오차 제곱의 평균**을 의미\n",
    "2. 평균 절대 오차 (Mean Absolute Error, MAE)\n",
    "   * 회귀 분석에서 모델의 예측값과 실제 관측값 사이의 **절대값 오차의 평균**을 의미\n",
    "  \n",
    "*cf. MSE는 큰 오차에 대해 민감하게 반응하는 반면 MAE는 모든 오차를 동일하게 취급함*\\\n",
    "*MSE는 이상치에 민감하게 반응하지만, MAE는 이상치가 존재하는 경우에도 전반적인 예측 성능을 유지할 수 있음*\n",
    "\n",
    "#### 변수 유의성 평가\n",
    "\n",
    "1. t검정\n",
    "\n",
    "(1)개념\n",
    "\n",
    "* 독립변수의 회귀계수가 유의미한지 검정하는 데 사용된다\n",
    "\n",
    "(2) 검정 과정\n",
    "\n",
    "* t-값 계산\n",
    "* p-value 확인 및 판단\n",
    "    *if p-value < 0.05 → 유의 수준 5%에서 귀무가설 기각 → **해당 변수는 유의미하다**.\n",
    "    *if p-value >= 0.05 → 귀무가설 기각 불가 → **해당 변수는 유의미하지 않다**.\n",
    "\n",
    "(3) 해석\n",
    "* 회귀계수가 0이라면 해당 독립변수는 의미 없음.\n",
    "* p-value가 작을수록 해당 변수는 종속 변수에 더 큰 영향을 준다고 볼 수 있다.\n",
    "* t값이 크다면 귀무가설을 기각할 가능성이 높아진다.\n",
    "\n",
    "**EXAMPLE**\n",
    "\n",
    "*가설 설정*\\\n",
    "H0: 창문의 개수는 집값에 영향을 주지 않는다.\\\n",
    "H1: 창문의 개수가 집값에 영향을 준다.\n",
    "\n",
    "*t검정 진행*\\\n",
    "if p-value < 0.05 → 귀무가설을 기각하고 대립가설 채택"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
