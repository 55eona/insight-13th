{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296dbd80-8f18-4e07-9841-612f8c62f4a1",
   "metadata": {},
   "source": [
    "# 회귀 \n",
    "데이터에서 **패턴**을 찾아내 미래를 **예측**하는 것. **연속적인 숫자**를 다룬다.\n",
    "\n",
    "## 선형회귀\n",
    "- 데이터를 가장 잘 설명하는 **회귀선**을 찾아 미래를 예측하는 과정\n",
    "- 예측하고 싶은 변수: 종속 변수 \n",
    "- 예측에 사용하는 변수: 독립 변수\n",
    "\n",
    "### 단순선형회귀분석: 직선 형태(하나의 독립 변수)로 표현\n",
    "최적의 직선 y=ax+b의 a(회귀계수)와 b(절편)을 찾는 과정. **예측값**을 조절하며 **잔차**를 줄여나간다. \n",
    "\n",
    "#### 최소제곱법\n",
    "- **잔차의 제곱의 합이 최소**가 되는 지점으로 **최적의 회귀선**을 구하는 방식\n",
    "- **잔차**: 실제과 예측값의 차이\n",
    "\n",
    "#### 경사하강법\n",
    "1. **목적함수, 비용함수, 손실함수**\n",
    "\n",
    "\n",
    "    **목적함수**: **최적화하려는 대상**을 수학적으로 표현한 함수\n",
    "\n",
    "   **손실함수**: 개별 데이터 샘플에 대해 **오차를 측정**하는 함수\n",
    "   \n",
    "   **비용함수**: 전체 데이터셋에서 **평균적인 손실**을 측정하는 함수. 모든 포인트에 대한 손실함수를 평균 혹은 합산\n",
    "\n",
    "2. **손실함수** <Br>\n",
    "    $\\text{E} = \\frac{1}{N} \\sum_{i=1}^{N} (t_i - \\hat{y}_i)^2$\n",
    "\n",
    "   $t_i$: i번째 실제값, $y_i$: i번째 예측값, $y=Wx+b$\n",
    "\n",
    "   E를 최소화하는 W와 b 값은 다음과 같은 식을 사용해 찾는다.\n",
    "\n",
    "   $w = w - \\alpha \\nabla_w J(w)$\n",
    "\n",
    "   미분값의 반대방향으로 이동함으로써 최솟값에 도달하게 되는 원리\n",
    "\n",
    "\n",
    "3. **학습률 ($\\alpha$: learning rate)**\n",
    "\n",
    "   **얼만큼** 이동할 것인가.\n",
    "\n",
    "\n",
    "\n",
    "   초기에 작은 값부터 시도하고 조정해나가며 **적당한** 학습률을 찾는 것이 좋다.\n",
    "\n",
    "\n",
    "4. **Local Minima 문제**\n",
    "\n",
    "   실제론 **작은 기울기에서 조금씩**, **큰 기울기에서 많이** 이동함.\n",
    "\n",
    "   때문에 Local minima에서 빠져나오지 못하는 경우를 주의 해야한다. \n",
    "\n",
    "5. **해결법 - 모멘텀**\n",
    "\n",
    "   이전의 기울기와 이동 방향을 기억해 **관성**을 부여한다.\n",
    "        \n",
    "     \n",
    "### 다중선형회귀분석: 독립변수가 2개 이상인 경우의 선형회귀\n",
    "독립변수가 2개인 경우 3차원 공간에 표현된다. 회귀선 또한 평면\n",
    "\n",
    "마찬가지로 최소제곱법과 경사하강법을 사용할 수 있다. \n",
    "\n",
    "#### 다중공선성\n",
    "두 독립 변수가 **강한 상관관계**를 가질 때 다중공선성이 발생한다. \n",
    "다중공선선이 높은 경우 **회귀분석의 정확도가 낮아진다**.\n",
    "\n",
    "1. **다중공선성 확인 방법**\n",
    "   - 상관계수: 히트맵 또는 corr()함수, pairplot()\n",
    "   - VIF지수: 회귀 모델의 결정계수 R square를 사용하여 계산. 높을수록 다중공선성 존재\n",
    "     \n",
    "2. **다중공선성 대처 방법**\n",
    "    - 변수 제거: 독립변수로서 사용할 변수 선택\n",
    "    - 변수 변환: 변수들을 더하거나 빼 새로운 변수 생성\n",
    "    - 규제 선형 모델 활용: 모델의 복잡도 줄임\n",
    "    - PCA: 상관 있는 변수들을 묶어 데이터의 차원을 축소\n",
    "      \n",
    "### 규제선형모델\n",
    "모델이 학습 데이터에 **과적합(overfitting)**이 되지 않도록 규제를 가하는 모델\n",
    "\n",
    "비용함수: 최소제곱법과 회귀 계수의 값이 커지지 않도록 하는 방법이 균형을 이루도록 유지\n",
    "\n",
    "**규제선형모델의 종류**\n",
    "- L2규제 : 회귀계수의 **제곱**에 대해 페널티를 부여, **릿지(Ridge)회귀**\n",
    "- L1규제 : 회귀계수의 **절댓값**에 대해 페널티를 부여, **라쏘(Lasso) 회귀**\n",
    "- L2규제 + L1규제 결합: **엘라스틱넷(Elastic Net) 회귀**\n",
    "\n",
    "\n",
    "### 모델평가방법\n",
    "회귀 모델의 성능 및 유의성 평가\n",
    "1. **성능 평가 지표**\n",
    "   - 평균 제곱 오차: 모델의 **예측 성능** 평가\n",
    "  \n",
    "     \n",
    "     $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "  \n",
    "     \n",
    "    - 평균 절대 오차: 모델의 **예측 성능** 평가\n",
    "  \n",
    "       $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "\n",
    "3. **변수 유의성 평가**\n",
    "\n",
    "\n",
    "    - t검정: **독립 변수의 회귀계수**가 유의미한지 검정\n",
    "      \n",
    "      t-값 계산, p-value 확인 및 판단\n",
    "\n",
    "      -> p-value < 0.05, t-값이 크면 독립변수가 종속변수에 영향을 준다고 판단할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354970bb-7aa0-450b-8206-d7d14aa2dae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
