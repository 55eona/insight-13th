{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af246b1e-9e22-415a-8262-4f9726479ba6",
   "metadata": {},
   "source": [
    "# 머신러닝과 모델링 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ab691-b0ed-413b-989e-ef7dd1e5bc67",
   "metadata": {},
   "source": [
    "1. 머신러닝\n",
    "   - 새로운 데이터를 예측하거나 결정을 내릴 수 있도록 하는 기술\n",
    "   - 데이터 패턴 분석 -> 패턴을 바탕으로 새로운 데이터에 대한 예측\n",
    "- 비지도 학습\n",
    "  - 정답이 없는 데이터로 학습하는 방식\n",
    "- 지도학습\n",
    "  - 정답이 있는 데이터로 학습하는 방식 머신러닝\n",
    "   - 새로운 데이터를 예측하거나 결정을 내릴 수 있도록 하는 기술\n",
    "   - 데이터 패턴 분석 -> 패턴을 바탕으로 새로운 데이터에 대한 예측\n",
    "- 비지도 학습\n",
    "  - 정답이 없는 데이터로 학습하는 방식\n",
    "  - 군집화/클러스터링(Clustering)\n",
    "- 지도학습\n",
    "  - 정답이 있는 데이터로 학습하는 방식\n",
    "  - 회귀(Regression), 분류(Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f70ba-dfe0-46fb-b583-aad765fb79f6",
   "metadata": {},
   "source": [
    "2. 모델링\n",
    "   - 모델링 과정\n",
    "     1. 전체 데이터를 훈련 데이터/ 테스트 데이터 분리\n",
    "     2. 훈련 데이터로부터 모델 만들기\n",
    "     3. 만든 모델을 테스트 데이터로 반복 성능 평가\n",
    "     4. 최종 모델 생성\n",
    "    - 훈련 데이터 (train_set): 모델 학습(훈련)에 사용되는 데이터\n",
    "    - 테스트 데이터(test_set): 학습된 모델의 검증에 사용되는 데이터 (unseen data)\n",
    "      - 훈련 데이터를 통해 학습시킨 모델이 새로운 데이터(unseen data)에 대해 얼마나 예측을 잘 수행하는지 공정하게 평가하기 위해 data 분리 진행\n",
    "      - 분리 시키지 않은 데이터를 학습시키면, 단순히 데이터를 암기한 결과값을 낼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90925660-d6b1-4da7-a9b0-619d6bc2409f",
   "metadata": {},
   "source": [
    "# 회귀(Regression)와 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ca46f-f4e7-4daf-82ac-98115757ddb5",
   "metadata": {},
   "source": [
    "1. 회귀란\n",
    "   - 데이터에서 패턴을 찾아내 미래 값을 예측하는 것\n",
    "   - 연속적인 숫자를 다룸 (ex. 집값 예측, 주식 가격 예측)\n",
    "     - 목표 변수가 불연속(범주형)일 때는 분류(Classification) 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea85c32-edc9-4e44-8932-a5a15036f0b0",
   "metadata": {},
   "source": [
    "2. 회귀의 종류\n",
    "   - 선형회귀\n",
    "   - 비선형회귀\n",
    "   - 릿지회귀\n",
    "   - 라쏘회귀\n",
    "   - 다향회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d12f5-ea2c-4edf-b073-75ec56aaa2ff",
   "metadata": {},
   "source": [
    "3. 선형 회귀\n",
    "   - 기존에 관측된 데이터를 활용해 해당 데이터를 가장 잘 설명하는 회귀선을 찾아 예측하는 과정\n",
    "   - 회귀선 ex. 직선 y=ax+b\n",
    "   - 현재의 데이터 요약 + 미래 값 예측\n",
    "   - 예측하고자 하는 변수 = 종속 변수 (반응 변수) = y\n",
    "   - 예측을 위해 사용하는 변수 = 독립 변수 (설명 변수) = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c97fb-4f3c-4fdb-8852-397c79a95170",
   "metadata": {},
   "source": [
    "선형의 의미\n",
    "- '선형'은 꼭 직선을 의미하는 것이 아님\n",
    "- \"모델이 파라미터(계수)에 대해 선형이라는 뜻\"\n",
    "  - EX.\n",
    "    1. y=b0 + b1x -> 직선 (기본선형)\n",
    "    2. y=b0 + b1x + b2x^2 -> 곡선이지만 계수(b)에 대해 선형\n",
    "    3. y=b0 + b1sin(x) -> 비직선이지만 계수(b)에 대해 선형\n",
    "- 함수의 모양이 곡선이더라도, 계수(b)들이 선형적으로 결합되면 선형회귀에 포함\n",
    "- 비선형 회귀\n",
    "  - 모델이 계수에 대해 비선형\n",
    "  - Ex. y=b0 + b1^x -> 계수가 지수에 들어감"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e88aac-82b7-45bf-950a-8a4f2509c1b9",
   "metadata": {},
   "source": [
    "선형 회귀의 종류\n",
    "1. 단순 선형 회귀\n",
    "   - 데이터를 직선 형태(하나의 독립 변수)로 표현하는 경우\n",
    "   - x가 하나\n",
    "   - y=ax+b 에서\n",
    "     - a= 회귀계수(기울기)\n",
    "     - b= y절편\n",
    "     - 적절한 a와 b를 찾는 과정 (데이터를 설명하는 최적의 직선 도출)\n",
    "\n",
    "2. 다중 선형 회귀\n",
    "   - 데이터를 두 개 이상의 독립 변수를 사용하여 표현하는 경우\n",
    "   - x가 둘 이상 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f1724-708c-4395-947a-913a02e99368",
   "metadata": {},
   "source": [
    "적절한 a와 b를 찾는 방법 -> 잔차(residual) 정의\n",
    "- 오차와 잔차의 차이\n",
    "  - 오차: 모델이 설명 못한 \"실제 - 실제 평균값\" 간의 차이\n",
    "  - 잔차: 모델이 설명 못한 \"실제 - 예측값\" 간의 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6d5f2-68fb-49b0-9461-71b3d81fa03b",
   "metadata": {},
   "source": [
    "1. 최소제곱법 (Least Squares Method)\n",
    "   - 최소 자승법(Ordinary Least Square)\n",
    "   - 잔차 제곱의 합(Sum of Squared Residuals, RSS)이 최소가 되는 지점으로 최적의 회귀선 결정\n",
    "     - 제곱의 이유: 양의 잔차와 음의 잔차 값의 상쇄 방지 + 경사하강법 계산의 미분에 유리\n",
    "   - 회귀선과 실제 데이터와의 차이(잔차)를 최소화하기 위한 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562bd74-4d79-4d39-83f3-c07c0862b190",
   "metadata": {},
   "source": [
    "2. 경사하강법 (Gradient Descent)\n",
    "   - 최소제곱법을 통해 구한 함수가 복잡해, 최소값을 구하기 어려울 때 사용\n",
    "   - 잔차 제곱을 줄이려는 목적은 그대로, 계산 과정이 복잡할 때 최소로 가는 과정에서 사용\n",
    "\n",
    "   1. 목적함수(Objective Function)\n",
    "      - 우리가 달성하고 싶은 목표\n",
    "      - 최적화(Optimization)을 위해 오차를 최소화하는 것이 목적인 함수\n",
    "      - 최적화하려는 대상을 수학적으로 표현한 함수\n",
    "      - 머신러닝에서 모델의 예측이 얼마나 잘 맞는지 평가하는 기준\n",
    "    2. 손실함수(Loss Function)\n",
    "       - 개별 데이터 샘플에 대한 오차를 측정하는 함수\n",
    "       - 한 개의 데이터 포인트에 대해 모델이 얼마나 틀렸는지 평가하는 함수\n",
    "       - 예측값과 실제값의 차이를 계산\n",
    "    3. 비용함수(Cost Function)\n",
    "       - 전체 데이터셋에서 평균적인 손실을 측정하는 함수\n",
    "       - 손실함수를 모든 데이터 포인트에 대해 계산한 후 평균/합산한 값\n",
    "       - 머신러닝에서는 대부분 모델 훈련 시, 비용함수 최적화(최소화)가 목표\n",
    "         - 이 때는 목적함수 = 비용함수\n",
    "         - 예외 상황\n",
    "           1. 강화학습\n",
    "              - 보상함수(Reward Function) 최대화가 목적\n",
    "            2. 생성 모델(GAN, VAEs 등)\n",
    "               - 때로는 두 목적이 충돌하는 경우도 있어, 최대화/최소화 혼재\n",
    "   - 머신러닝 과정에서는 \"모델을 평가하고 최적화하는 함수\"를 찾는게 핵심\n",
    "   - 손실함수: 개별 / 비용함수: 합계(평균)으로서 차이가 있지만 혼용하는 경우 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547259d2-e15c-44f2-a675-8f6ee32aacfb",
   "metadata": {},
   "source": [
    "손실함수로 회귀선 추정\n",
    "- y=Wx + b\n",
    "- ti: i번째 데이터포인트 실제값\n",
    "- yi: i번째 데이터포인트 예측값\n",
    "- 손실합수 E(W,b) = 1/n * ∑(ti-yi)^2 = 1/n * ∑(ti-(Wxi+b))^2\n",
    "- W에 대해 E가 변화하는 양상을 보기 위한 2차원 그래프 (2차 함수 형태)\n",
    "  - x축: W / y축: E\n",
    "  - 미분계수가 양수 -> W 감소\n",
    "  - 미분계수가 음수 -> W 증가\n",
    "  - 해당 방법을 반복해서 W값을 없데이트\n",
    "  - 미분계수가 0인 점을 발견하면, 해당 값이 E의 최솟값이자 W의 최적값\n",
    "  - x축을 b로 구성후 동일한 원리로 최적의 b 또한 찾아낼 수 있음\n",
    "- 비유적 표현\n",
    "  - 짙은 안개(많은 데이터) 때문에 산(손실 함수) 속에서 길을 잃었을 때,\n",
    "  - 발밑 지면의 기울기만을 느낄 수 있음\n",
    "  - 빨리 골짜기(최적의 가중치와 계수)로 내려가는 방법은 가장 가파른 길을 따라 아래로 내려가는 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d99c681-85eb-4827-aea5-51f60e039e91",
   "metadata": {},
   "source": [
    "학습률 (α: learning rate)\n",
    "- 등산에서의 보폭 크기\n",
    "- 이동의 정도\n",
    "- 경사하강법의 '학습'의 의미 = '오차의 최솟값 탐색'\n",
    "- 학습률이 작은 경우 -> 모델이 수렴할 때까지 시간이 오래 걸림\n",
    "- 학습률이 큰 경우 -> 모델이 수렴하지 못하고 발산함(=손실 값이 계속해서 커지는 것)\n",
    "\n",
    "- 학습률의 결정은 개발자가 직접 설정 (하이퍼파라미터)\n",
    "- 학습률 설정 방법:\n",
    "  1. 초기에 작은 값부터 시도:\n",
    "     - 0.01,0.001,0.0001 등의 값을 경험적으로 시도\n",
    "  2. 실험을 통해 조정:\n",
    "     - 학습이 느리다 -> 학습률 늘리기\n",
    "     - 학습이 발산한다 -> 학습률 줄이기, 여러 번 실험\n",
    "  3. 학습률 자동 설정\n",
    "     - Learning Rate Scheduler\n",
    "       - 학습 과정에서 학습률을 자동으로 줄이거나 늘리기\n",
    "     - Grid Search, Random Search\n",
    "       - 여러 학습률을 자동으로 실험하며 최적값 찾아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd2a0c-d12b-489e-8da7-c445d6de3cd5",
   "metadata": {},
   "source": [
    "Local Minima 문제\n",
    "- 모든 비용함수가 하나의 최솟값을 가지지는 않음 (여러 골짜기)\n",
    "  - Local Minima: 함수 전체의 최솟값이 아닌 특정 구간 내에서의 최솟값 (함정)\n",
    "  - Global Minima: 함수 전체의 최솟값 (목표로 하는 지점)\n",
    "- 실제 이동량을 보면, 작은 기울기에서 조금씩, 큰 기울기에서 많이 이동함\n",
    "- 또 작은 기울기에서 업데이트가 조금씩 이루어니 Local Minima에서 빠져나오기 어려움\n",
    "- 딥러닝의 경우, 비용함수의 모양을 정확히 예측할 수 없음\n",
    "  - local minima에 빠지는 경우를 주의해야 함\n",
    "  - local minima를 탈출하는 방법에 대한 알고리즘 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49547f6-674e-4113-8238-08ba1cc7b5ee",
   "metadata": {},
   "source": [
    "모멘텀\n",
    "- 기존 경사하강법은 이전 기울기 고려 X, 현재 기울기만 기준으로 움직임\n",
    "- 이전의 기울기, 이동하던 방향을 고려하도록 관성(모멘텀) 부여\n",
    "- 모멘텀의 장점:\n",
    "  - 기울기에 관성을 부여해 작은 기울기는 쉽게 넘어감\n",
    "  - Local Minima 탈출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe394b-7b35-4db0-a2bf-b36e0adb2d10",
   "metadata": {},
   "source": [
    "다중선형회귀분석\n",
    "- 독립 변수가 2개 이상인 경우의 선형회귀\n",
    "- 독립 변수가 2개 이상인 경우 3차원 공간에 표현됨 (회귀선 = 평면)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743adc1e-2542-4d4d-8cc0-027d16b980f0",
   "metadata": {},
   "source": [
    "다중공선성 (Multicollinearity)\n",
    "- 회귀 분석에서 독립 변수들 간에 상관 관계가 큰 경우 발생\n",
    "- 어떤 독립 변수가 종속 변수에 얼마나 영향을 미치는지 정확하게 구분하기 어려움\n",
    "- 회귀 분석의 정확도가 낮아짐\n",
    "- 교락 요인\n",
    "\n",
    "다중공선성 확인 방법\n",
    "1. 상관계수\n",
    "   - 히트맵, corr(), pairplot\n",
    "2. VIF 지수 (분산 팽창 인수)\n",
    "   - 회귀 모델의 결정계수 R square를 사용해 계산\n",
    "   - VIF가 높으면 다중공선성 존재\n",
    "     - VIF=1: 해당 독립 변수는 다른 변수와 상관관계가 전혀 없음\n",
    "     - VIF<5: 일반적으로 다중공선성 문제 없음\n",
    "     - VIF>5: 다중공선성의 징후 있음, 주의 필요\n",
    "     - VIF>10: 다중공선성 심각, 변수 간 상관관계를 줄이기 위한 조치 필요\n",
    "     - 일반적 가이드라인(절대적 기준X)\n",
    "\n",
    "다중공선성 대처 방법\n",
    "1. 변수 제거(변수 선택법)\n",
    "   - 독립변수로서 사용할 변수 선택\n",
    "2. 변수 변환\n",
    "   - 변수를 더하거나 빼서새로운 변수 생성\n",
    "   - 독립변수를 더하거나 빼도 문제 없는 경우 사용\n",
    "3. 규제 선형 모델 활용\n",
    "   - 릿지, 라쏘, 엘라스틱넷 등 방법으로 모델 복잡도 줄이는 방법 사용\n",
    "4. PCA (주성분분석)\n",
    "   - 데이터의 차원을 축소하는 데 사용되는 통계적 기법\n",
    "   - 고차원 데이터에서 중요한 정보를  최대한 보존하며, 데이터의 복잡성을 줄이는 것을 목표\n",
    "   - \"상관 있는 변수들을 묶어서 더 적은 수의 대표축으로 바꾸는 것\"\n",
    "   - 데이터의 방향성을 찾아 변환하고, 중요한 정보만 남겨 차원 축소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c819d-540b-47dd-8b45-9b46f8b38476",
   "metadata": {},
   "source": [
    "4. 규제선형모델 \n",
    "- 과적합 방지를 위한 모델\n",
    "- 선형 회귀 모델에서는 특성에 곱해지는 계수(기울기)의 크기를 조정하는 것\n",
    "- 다중 회귀 분석 모델에서 특정 독립 변수의 회귀 계수(가중치)가 커지면, 회귀 계수가 특정 독립 변수에 과하게 의존한다는 뜻\n",
    "- 비용 함수는 'RSS 최소화 방법(최소제곱법)' + '회귀 계수 값이 커지지 않도록 하는 방법'이 균형을 유지해야 함\n",
    "- 오류는 최대한 줄이고, 특정 변수에 집착하지 않도록 균형 맞추기 필요\n",
    "- 최적 모델을 위한 비용 함수 목표\n",
    "   - Min(학습데이터 잔차 오류 최소화 + 회귀계수 크기 제어)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c0391-c696-4ef8-a331-8a6db83b2ffc",
   "metadata": {},
   "source": [
    "규제선형모델의 종류\n",
    "1. L2규제: W(회귀계수)의 제곱에 대해 페널티를 부여하는 방식\n",
    "   - 릿지(Ridge)회귀\n",
    "   - 기둥 크기에 따라 다른 힘으로 누르는 망치\n",
    "   - 큰 회귀계수에 대해 더 큰 페널티\n",
    "   - 비율이 균등하지 않을 수 있지만 전체적으로 작아짐\n",
    "   - 모든 기둥이 작아지지만, 0이 되지는 않음 (변수 제거 X)\n",
    "   - 모든 변수를 유지 + 크기를 적절히 줄여 과적합 방지\n",
    "   - 장점\n",
    "      - 변수 간 상관관계가 높아도 좋은 성능\n",
    "    - 사용\n",
    "      - 예측 변수가 많을 때\n",
    "      - 다중공선성 존재\n",
    "2. L1규제: W(회귀계수)의 절댓값에 대해 페널티를 부여하는 방식\n",
    "   - 라쏘(Lass) 회귀\n",
    "   - 모든 기둥을 같은 힘으로 누르는 망치\n",
    "   - 회귀계수의 크기와 상관없이 같은 페널티\n",
    "   - 작은 계수는 0이 될 수도 있음 (변수 제거 O)\n",
    "   - 비중이 낮은 변수(작은 계수)는 아예 사라지고, 중요한 변수만 남음\n",
    "   - 장점\n",
    "      - 변수간 상관관계가 높으면 성능 하락\n",
    "    - 사용\n",
    "      - 예측 변수 수가 많고, 그 중 일부만 실제로 중요할 때\n",
    "      - 모델의 해석을 간단하게 유지하고자 할 때\n",
    "3. L2규제 + L1규제 결합\n",
    "   - 엘라스틱넷(Elastic Net) 회귀\n",
    "   - 불필요한 변수 제거(L1) + 남은 변수들 적절한 크기로 유지(L2)\n",
    "   - L1의 변수 선택 기능 + L2의 규제 기능\n",
    "   - 안정적 예측과 희소성 동시에 달성 가능\n",
    "   - 장점\n",
    "      - 변수간 상관관계를 반영한 정규화\n",
    "    - 사용\n",
    "      - 예측변수 수가 많고, 그 중 중요한 변수르 선택하면서도 다중공선성을 관리\n",
    "\n",
    "- 규제는 스케일링이 꼭 필요 (StandardScaler, MinMaxScaler)\n",
    "  - 모든 계수에 대해 균등한 페널티를 부과해 모델의 복잡성 최소화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1bd95-c4a4-4e59-8abc-0ff4b5669790",
   "metadata": {},
   "source": [
    "5. 모델 성능 및 유의성 평가 방법\n",
    "   1. 성능 평가 지표\n",
    "      1. 평균 제곱 오차(Mean Squared Error, MSE):\n",
    "         - 모델의 예측값과 실제 관측값 사이의 오차 제곱의 평균\n",
    "       2. 평균 절대 오차 (Mean Absolute Error, MAE):\n",
    "          - 모델의 예측값과 실제 관측값 사이의 절대값 오차의 평균\n",
    "       - MSE와 MAE의 차이\n",
    "         - MSE: 큰 오차(이상치)에 대해 민감하게 반응함\n",
    "         - MAE: 모든 오차를 동일게 취급해, 이상치가 존재하는 경우에도 전반적 예측 성능 유지 가능\n",
    "    2. 변수 유의성 평가\n",
    "       1. t검정\n",
    "          - 독립 변수의 회귀계수가 유의미한지 검정\n",
    "          - 귀무가설(H0): 회귀계수(a)가 0 -> 독립변수가 종속변수에 영향을 주지 않음\n",
    "          - 대립가설(H1): 회귀계수(a)가 0이 아니다 -> 독립변수가 종속변수에 영향을 줌\n",
    "          - 검정통계량(t): b-0/SE(b)\n",
    "            - b: 추정된 회귀계수 / SE(b): 표준오차\n",
    "           - p-value 확인 및 판단 -> 귀무가설 기각 여부 결정\n",
    "           - 해석\n",
    "             - 회귀계수 = 0 -> 해당 독립변수 의미 없음\n",
    "             - p-value가 작을수록 해당 변수는 종속 변수에 더 큰 영향을 준다고 볼 수 있음\n",
    "             - t-값이 크면 -> 귀무가설을 기각할 가능성 높아짐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
