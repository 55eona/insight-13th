{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd4239a-6021-4b49-aa22-218f2d54490f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 머신러닝과 모델링\n",
    "\n",
    "## 1. 머신러닝\n",
    ": 새로운 데이터에서 **패턴**을 찾아내고, 이를 바탕으로 새로운 데이터에 대한 **예측, 결정**을 가능하게 하는 기술\n",
    "- **비지도 학습**: *정답이 없는* 데이터로 학습하는 방식\n",
    "- **지도학습**: *정답이 있는* 데이터로 학습하는 방식\n",
    "\n",
    "## 2. 모델링\n",
    ": 머신러닝 모델을 생성하는 과정\n",
    "- **훈련 데이터**: 학습(훈련)할 때 사용되는 데이터\n",
    "- **테스트 데이터**: 학습된 모델을 테스트할 때 사용되는 데이터 --> unseen data\n",
    "\n",
    "<BR>\n",
    "\n",
    "#### 오늘 배울 내용은 *지도학습 중 회귀!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362afddd-c72d-4e3d-bed4-cdfbd742eebe",
   "metadata": {},
   "source": [
    "# 회귀(Regression)와 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3a030-293c-4790-b5cb-99646eb69040",
   "metadata": {},
   "source": [
    "## 1. 회귀란?\n",
    "- 데이터에서 **패턴**을 찾아내어 미래 값을 **예측**하는 것\n",
    "- 통계, 머신러닝의 주요 개념\n",
    "- 종속변수가 **연속**적인 숫자를 다룸 <BR>\n",
    "  EX) 집값 예측, 주식 가격 예측 등\n",
    "\n",
    "## 2. 회귀의 종류\n",
    ": **선형회귀**, 비선형 회귀, 릿지회귀, 라쏘회귀, 다항 회귀 등이 존재\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57af51-f119-4611-9e82-d00a43f0849e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. 선형회귀\n",
    ": 데이터를 가장 잘 설명하는 회귀선(EX.$y=ax+b$)을 찾아 예측하는 과정<BR>\n",
    "  종속변수($y$): 예측하고자 하는 값/구하고자 하는 값/EX. 미래 주가<BR>\n",
    "  독립변수($x$): 예측을 위해 사용하는 변수/EX. 시간\n",
    "\n",
    "\n",
    "### **3.1 단순선형회귀분석**\n",
    "\n",
    "- 데이터를 직선형태로 표현하는 경우\n",
    "- 회귀선: $y = aX+b$ 형태\n",
    "  - 독립변수($X$)가 하나\n",
    "  - $a$: 회귀계수\n",
    "  - $b$: y절편\n",
    "- 적절한 회귀계수와 절편을 찾아야 함 ==> 최소제곱법 활용\n",
    "\n",
    "#### **3.1.1 최소제곱법(Least Squares Method)**\n",
    "  - 잔차의 제곱의 합(RSS)이 최소가 되는 지점을 찾아, 최적의 회귀선을 찾는 방식\n",
    "  - 잔차: 실제 값과 예측값의 차이\n",
    "  - 최소값을 구하기 어려울 때 ==> 경사하강법 활용\n",
    "\n",
    "#### **3.1.2 경사하강법**\n",
    "- 최소제곱법과 마찬가지로 잔차제곱을 최소가 되는 지점을 찾는 것이 목적<BR>\n",
    "\n",
    "**(0) 목적, 손실, 비용함수**\n",
    "- **목적함수**: 최적화를 위해 오차를 최소화하는 것이 목적인 함수\n",
    "- **손실함수**: 개별 데이터 샘플에 대한 오차를 측정하는 함수\n",
    "- **비용함수**: 전체 데이터셋에서 평균적인 손실을 측정하는 함수<BR>\n",
    "\n",
    "**(1) 손실함수**\n",
    "- 예측값과 실제값 차이 계산/잔차를 제곱하고 합하는 형태\n",
    "- 손실함수 그래프 위의 점을 점점 이동시켜, 미분값이 0인 지점(최솟값)을 찾는 원리<BR>\n",
    "\n",
    "**(2) 학습률(Learning Rate)**\n",
    "- 점이 이동하는 보폭\n",
    "- 너무 작으면, 모델이 수렴할 때까지 오랜 시간이 필요\n",
    "- 너무 크면, 모델이 수렴하지 못하고 발산<BR>\n",
    "\n",
    "**(3) Local Minima 문제**\n",
    "- 작은 기울기에서는 조금씩 이동\n",
    "- 비용함수가 여러개의 최솟값을 가진 경우, 진짜 최솟값(global minima)이 아닌 local minima에 빠질 수 있음<BR>\n",
    "\n",
    "**(4) 해결법-모멘텀**\n",
    "- 기존: 현재 기울 기준으로 움직이는 것 ==> 모멘텀: 이전의 기울기, 이동해오던 방향 고려!(관성 부여하기)\n",
    "### **3.2 다중선형회귀분석**\n",
    "- 회귀선: $y = W_1 x_1 + W_2 x_2 + ... + W_n x_n + b $ 형태\n",
    "  - 독립변수가 2개 이상\n",
    "  - 독립변수($x_i$)가 2개인 경우, 회귀선은 평면\n",
    "  - 최소제곱법 사용 가능\n",
    " \n",
    "#### **3.2.1 다중공선성**\n",
    "- 독립변수들 간에 상관관계가 큰 경우 발생\n",
    "- 다중공선성이 높은 경우, 어떤 독립변수가 종속변수에 얼마나 영향을 미치는지 정확히 파악 불가<BR>\n",
    "  ==> 회귀분석의 정확도 낮아짐\n",
    "- 확인 방법\n",
    "  상관계수|VIF 지수\n",
    "  --------|--------\n",
    "  상관관계 정도를 수치로 나타낸 것 <BR>-1과 1사이의 값으로 나타나며 절대값이 높을수록,<BR> 상관관계가 높다고 판단<BR> 히트맵, `corr()`, pairplot을 통해 확인 가능| **분산 팽창 인수**, R square를 통해 계산됨<BR>*VIF=1*: 상관관계가 없음을 의미<BR>*VIF가 높을수록*, 다중공선성이 심각하다고 판단\n",
    "- 대처 방법\n",
    "  - **변수제거 (변수 선택법)**\n",
    "    - 독립 변수로 사용할 변수를 선택\n",
    "  - **변수변환**\n",
    "    - 독립변수를 합치거나 빼서 새로운 변수 생성\n",
    "    - EX) 남편의 수입과 아내의 수입이 상관성이 높다면, (남편수입) + (아내수입) ==> 가족 수입\n",
    "  - **규제 선형 모델 활용**\n",
    "    - 릿지, 라쏘, 엘라스틱넷 등\n",
    "  - **PCA(주성분분석)**\n",
    "    - 데이터의 차원을 축소하는 방법\n",
    "    - 상관 있는 변수들을 묶어, 더 적은 수의 대표축으로 변환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef0510-78a2-4567-91ea-32fd311589b2",
   "metadata": {},
   "source": [
    "## 4. 규제선형모델\n",
    ": 과적합이 되지 않도록 **계수(기울기)의 크기를 조정**하는 것\n",
    "- 최소제곱법은 회귀계수가 쉽게 커져 과적합이 발생할 확률 높음\n",
    "- 최소제곱법(잔차 오류값 최소화)과 회귀계수 값 조정(과적합 방지)이 균형을 맞춰야함\n",
    "\n",
    "#### 4.1 릿지 회귀 (L2규제)\n",
    "- 회귀계수의 제곱에 패널티를 부여\n",
    "- 계수의 크기에 따라 다른 패널티 부여\n",
    "- 변수 완전히 사라지지 X ==> 계수 크기 조절 기능\n",
    "- *예측 변수 많을 때 / 다중공선성 존재할 때*\n",
    "#### 4.2 라쏘 회귀 (L1규제)\n",
    "- 회귀계수의 절댓값에 패널티를 부여\n",
    "- 계수의 크기와 상관없이 같은 패널티 부여\n",
    "- 변수가 사라지기도 함 ==> 변수 선택 기능\n",
    "- *많은 예측변수 중에 일부만이 중요할 때 / 모델의 해석을 간단하게 유지하고자 할 때*\n",
    "#### 4.3 엘라스틱넷 (L1+L2)\n",
    "- (계수 크기 조절 기능) + (변수 선택 기능)\n",
    "- (안정적인 예측) + (희소성) 동시에 달성 가능\n",
    "- *많은 예측변수 중 중요한 변수를 선택하면서도 다중공선성을 관리하고자 할 때*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b84ee-725a-43fe-b6f0-c2c748520135",
   "metadata": {},
   "source": [
    "## 5. 모델 평가방법\n",
    "\n",
    "#### 5.1 성능 평가 지표\n",
    "- **평균 제곱 오차(MSE)**\n",
    "  - 예측값과 실제 관측값 사이의 오차 제곱(Squared)의 평균\n",
    "  - 모델의 예측 성능을 평가\n",
    "- **평균 절대 오차(MAE)**\n",
    "  - 예측값과 실제 관측값 사이의 절대값(Absolute) 오차의 평균\n",
    "  - 모델의 예측 성능을 평가\n",
    "#### 5.2 변수 유의성 평가\n",
    "- t 검정\n",
    "  - 독립 변수의 회귀계수가 유의미한지 검정할 때 사용됨\n",
    "  - $H_0$: 회귀계수가 0이다 ==> 독립변수가 종속변수에 영향 주지 X ==> 독립변수 의미X\n",
    "  - $H_1$: 회귀계수가 0이 아니다 ==> 독립변수가 종속변수에 영향 O ==> 독립변수 의미O\n",
    "  - p-value 작을 수록, 종속변수에 더 큰 영향 준다고 볼 수 있음\n",
    "  - t값 클수록, $H_0$ 기각 가능성 up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05db6a0-05a6-438b-8db9-26bc9feb50a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
