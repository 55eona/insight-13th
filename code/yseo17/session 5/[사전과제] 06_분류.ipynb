{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제풀이\n",
    "\n",
    "1. 지도 학습과 비지도 학습의 차이에 대해 설명해주세요.\n",
    "<br> 지도 학습은 문제와 정답 (레이블)을 함께 주고 학습시키는 것, 비지도 학습은 레이블 없이 문제만을 주고 학습시키는 것. <br>\n",
    "\n",
    "2. 회귀와 분류의 차이에 대해 설명해주세요.\n",
    "<br> 회귀는 정답이 연속형으로, y=wx+b 같은 식에 따라 x값에 따른 y값을 찾는 것을 목표로 한다. 분류는 정해진 범주형 레이블 중 특정 값이 어떤 클래스에 속할지 판별하는 것을 목표로 한다. <br>\n",
    "\n",
    "3. 분류 모델의 네 가지 종류와, 각 모델이 무엇인지 간단하게 정리해주세요.\n",
    "<br> 로지스틱 회귀 (0-1의 이진 분류시 사용), 결정 트리 (조건에 따라 데이터를 분류하여 같은 label의 값의 집합으로 남게 함), 서포트 벡터 머신(클래스를 분류하는 최적의 선을 찾는 알고리즘), 최소 근접 (학습 없이 이웃하는 k개의 데이터를 기반으로 분류) <br>\n",
    "\n",
    "4. 분류 평가 지표에는 무엇이 있는지 작성해주세요.\n",
    "<br> 혼동 행렬, F1 score, ROC/AUC Curve\n",
    "\n",
    "5. 하이퍼파라미터 최적화가 무엇인지 쓰세요.\n",
    "<br> 하이퍼파라미터란 사용자가 직접 정할 수 있는 값으로, 최적화 시 모델의 성능을 향상시킬 수 있다. grid/random/bayesian 같은 최적화 방법을 이용할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 분류란?\n",
    "## (1) 머신러닝 - 지도학습과 비지도 학습\n",
    "**[머신러닝]**\n",
    "- 머신러닝 : 인공지능의 한 분야로, 컴퓨터가 스스로 학습할 수 있도록 도와주는 알고리즘이나 기술을 개반하는 분야\n",
    "    1. 알고리즘을 이용하여 데이터를 분석\n",
    "    2. 분석 결과를 스스로 학습\n",
    "    3. 이를 기반으로 판단/예측\n",
    "\n",
    "**[지도 학습과 비지도 학습]**\n",
    "- 지도 학습 : 문제+정답 알려주고 공부시킴 - 회귀, 분류\n",
    "- 비지도 학습 : 답을 가르쳐주지 않고 공부시킴 - 군집화\n",
    "- 강화 학습 : 보상을 통해 상을 최대화/ 벌을 최소화하는 방향으로 공부시킴 - 알파고\n",
    "\n",
    "## (2) 지도 학습 : 회귀와 분류\n",
    "### 지도 학습\n",
    "- 입력값 + 정답 레이블을 주고 학습시키는 방법\n",
    "- 예측값 = 정답을 목표로 함. \n",
    "- 과정: labeled data를 준비 - 데이터를 훈련/검증 세트로 나누기 - 훈련 데이터로 모델 학습시키기 - 경사하강법을 통해 예측=정답이 되도록 지도하기\n",
    "\n",
    "**[회귀와 분류]**\n",
    "- 회귀 : 연속적인 숫자값을 예측. 독립변수가 X일때 정답(Y)을 맞추기 (연속형)\n",
    "- 분류 : 입력된 데이터를 주어진 항목들로 나누기. 기존 데이터가 어떤 레이블에 속할지 판별 (범주형)\n",
    "\n",
    "**분류에 쓰이는 대표적인 머신러닝 알고리즘 :**\n",
    "<br>로지스틱 회귀/ 결정 트리 / 서포트 벡터 머신 / 최소 근접\n",
    "\n",
    "## (3) 이진 분류와 다중 분류\n",
    "- 이진 분류 : binary classification. True/False로 구분\n",
    "- 다중 분류 : multiclass classification. 값이 3개 이상임. ex. 사진의 숫자가 1, 2, 3, ... 중 어떤건지 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 분류 모델\n",
    "## (1) 로지스틱 회귀\n",
    "- **이진 분류 문제**를 푸는 알고리즘. 샘플이 특정 클래스에 속할 확률을 추정\n",
    "- 이진 분류는 직선/다중선형회귀로 풀 수 없음. ($y=wx+b$ 형태의 모델)\n",
    "\n",
    "**[시그모이드 함수]**\n",
    "- 출력이 0~1사이의 값을 가지는 S자 형태의 함수\n",
    "- 0~1사이의 값을 가지브로 이진 분류 작업에 활용할 수 있음\n",
    "\n",
    "**[시그모이드 함수의 가중치]**\n",
    "- 인공지능을 통해 가중치 $w$, $b$를 구해야 함. \n",
    "- 수식 : $H(x)=\\frac{1}{1+e^{-(wx+b)}}=sigmoid(wx+b)=\\sigma(wx+b)$\n",
    "- $w$값이 높을수록 그래프의 기울기 변화가 크다.\n",
    "- 가중치 $b$에 따라 그래프의 위치가 변화한다.\n",
    "\n",
    "## (2) 결정 나무\n",
    "- 조건에 따라 데이터를 분류. 데이터가 순수함 label의 집합으로 구성될 때까지 분류를 반복한다. \n",
    "\n",
    "**[결정 나무 용어 정리]**\n",
    "- root node : 결정나무의 시작이 되는 노드\n",
    "- edge : 노드와 노드를 연결하는 길목\n",
    "- leaf Nodes: Tree의 가장 마지막 노드로, 모델에서 label에 해당\n",
    "- height(depth): Tree의 깊이로, 클수록 tree의 구조는 복잡해짐\n",
    "- Level: 노드의 절대적 위계, Root node의 level = 0, leaf node의 level = height - 1\n",
    "- Parent: 상대적으로 높은 위계의 노드\n",
    "- Child: 상대적으로 낮은 위계의 노드\n",
    "    - Binary Tree(이진 트리): Tree 중에서 children이 최대 2개인 tree\n",
    "\n",
    "**[CART 알고리즘]**\n",
    "- = classification and regression tree 알고리즘\n",
    "- 가장 대표적인 결정 나무 알고리즘. 데이터셋을 <임계값>을 기준으로 두 child로 나눈다.\n",
    "- 임계값은 불순도 (지니 계수)가 낮아지는 방향으로 나눔\n",
    "    - 불순도 : 분류하려는 데이터 집합에서 서로 다른 클래스(범주)가 섞여 있는 정도\n",
    "\n",
    "CART 알고리즘의 주요 단계:\n",
    "1. 임계값 설정 - 임계값을 기준으로 데이터를 그룹으로 나눔\n",
    "2. 불순도 감소 알고리즘 - 어떤 기준으로 분류해야 잘 모을수 있을지 판단 -> 불순도가 낮은 쪽으로 형성\n",
    "\n",
    "**[실제 학습 시 고려해야 할 것들 : 모수 설정, 차이점 시각화, prunning]**\n",
    "1. 파라미터 설정 - 샘플 데이터 수, 리프 노드의 최대 개수 등\n",
    "2. 시각화 - 분류가 잘 이루어졌는지 확인\n",
    "3. prunning (가지치기) : 불필요한 노드 지우기. 노드가 너무 많아지면 과적합 가능성 O. 하부 트리를 제거하여 일반화 성능을 높인다.\n",
    "\n",
    "## (3) 서포트 벡터 머신 (SVM)\n",
    "**[서포트 벡터 머신]**\n",
    "- 클래스를 분류할 수 있는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘\n",
    "- 명확하게 분류되는 데이터 집단에서 성능이 좋음. 고차원에서도 효과적\n",
    "\n",
    "**[SVM의 구성]**\n",
    "- support vector: 구분하는 선과 가장 가까운 포인트\n",
    "- decision boundary: 집단을 구분하는 선\n",
    "- margin: 선과 각 점의 거리\n",
    "\n",
    "**[최적의 선을 찾는 방법]**\n",
    "- 데잍로부터 가장 멀리 떨어져 있는 것 = margin이 가장 큰 경우를 선택\n",
    "\n",
    "## (4) KNN (K-nearest-neighbor)\n",
    "- 데이터로부터 거리가 가까운 k개의 다른 데이터 레이블을 참조하여 분류하는 알고리즘\n",
    "- = KNN은 비슷한 특성을 가짐 = 데이터들이 서로 가까이 있다고 가정함\n",
    "- 계산 순서\n",
    "    1. 데이터 준비\n",
    "    2. K값 설정 (보통 홀수개)\n",
    "    3. 거리 계산\n",
    "    4. 가장 가까운 K개의 이웃 선택\n",
    "    5. 분류하기\n",
    "- KNN은 학습(훈련)이 필요하지 않아 별도의 모델 없이 새로운 데이터만으로도 분류할 수 있음. 정보 손실이 없음\n",
    "- but: 쿼리 처리하는데에 시간이 다소 소요됨 + 이상치에 큰 영향을 받는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 분류 평가 지표\n",
    "## (1) 혼동 행렬\n",
    "**[혼동 행렬]**\n",
    "- 분류 모델의 예측 결과를 정확-잘못된 예측으로 구분하여 나타낸 표\n",
    "- 실제 정답의 T/F - 예측 결과의 T/F로 예측 성공/실패를 구분함\n",
    "- TP/TN = 잘 예측  / FP, FN = 예측 틀림\n",
    "\n",
    "**[혼동 행렬을 이용한 분류 모델 평가 지표]**\n",
    "- 정확도 : 모델이 입력된 데이터에 대해 얼마나 정확하게 예측하는지 나타냄 : (TP+TN) / (TP+TN+FP+FN)\n",
    "- 정밀도 : 거짓을 참으로 판단한 정도 TP / TP + FP (정밀도가 높을수록 정도가 낮음)\n",
    "- 재현도 : 참을 거짓으로 예측한 정도가 낮음. TP / TP + FN\n",
    "\n",
    "**[정밀도와 재현도 예시]**\n",
    "- FN을 줄여야 하는 경우 (ex. 암 환자인데 암이 아니라고 하는 경우 치명적) = 재현율을 높여야 함. \n",
    "- FP를 줄요야 하는 경우 (ex. 스팸이 아닌 메일을 스팸으로 분류하는 경우 치명적) = 정밀도를 높여야 함. \n",
    "\n",
    "**[정밀도 & 재현도 그래프]**\n",
    "- 정밀도&재현도를 그래프로 그렸을 때, 두 선이 만나는 지점 = 임계값으로 하면 예측 오류 최소화 가능. \n",
    "\n",
    "## (2) F1-score\n",
    "- 정밀도와 재현율의 조화 평균\n",
    "- 머신러닝 모델의 성능을 평가하는 주요지표\n",
    "\n",
    "## (3) ROC/AUC curve\n",
    "**[ROC curve]**\n",
    "- 분류 모델의 성능 평가. 얼마나 분류가 잘 되었는가? \n",
    "- TRR; 참인것들 중 참이라고 예측한 비율 (=recall)\n",
    "- FPR: 거짓인것들 중 참이라고 잘못 예측한 비율\\\n",
    "- perfect classifier는 나올 수 없으므로 적절한 비용/시간 등을 고려하여 최선의 ROC 커브를 도출한다. \n",
    "\n",
    "**[AUC curve]**\n",
    "- AUC curve = ROC와 x축 사이의 면적\n",
    "- 모델의 성능을 숫자로 나타낸 것. 0~1 사이로 1에 가까울수록 성능이 좋은 것. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 하이퍼파라미터 최적화\n",
    "## (1) 하이퍼파라미터 최적화\n",
    "**[하이퍼파라미터]**\n",
    "- 학습 시작 전에 **사용자가 직접 설정하는 변수**. 적절한 수치를 찾아 모델 성능을 향상시킬 수 있다. \n",
    "- 하이퍼파라미터: decision tree- max_depth, random forest - n_estimators 같은 값들.\n",
    "- 하이퍼파라미터 최적화 : 튜닝을 거쳐 적절한 하이퍼파라미터 찾기 -> 모델 성능 향상\n",
    "    범위 설정 - 평가 지표 계산 함수 정의 - 검증 데이터로 정확도 평가 - 이전 단계 반복하여 하이퍼파라미터 범위 좁히기\n",
    "\n",
    "## (2) 하이퍼파라미터 최적화 방법\n",
    "- grid search : 정해진 범위에서 하이퍼파라미터를 모두 순회 -> 값 찾기 (GridSearchCV)\n",
    "    - 범위가 넓음. 꼼꼼하게 정확히 찾을 수 있음 // 시간소요. \n",
    "- random search : 정해진 범위에서 무작위 탐색 (RandomSearchCV)\n",
    "    - 그리드 서치보다 빠름\n",
    "    - 정확도 떨어짐=> 사용빈도 적음\n",
    "- baysian optimization : 사전 정보를 바탕으로 최적 하이퍼파라미터값을 확률적으로 추정하여 탐색 (bayes_opt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
