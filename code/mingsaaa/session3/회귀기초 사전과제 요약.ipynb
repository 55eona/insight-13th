{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ebd871-4e93-4598-93cb-6c90b155cff1",
   "metadata": {},
   "source": [
    "# 머신러닝과 모델링\n",
    "\n",
    "## 1. 머신러닝\n",
    "### 머신러닝\n",
    "머신러닝은 새로운 데이터를 예측하거나 결정을 내릴 수 있도록 하는 기술입니다. 머신러닝을 통해 데이터에서 패턴을 찾아내고, 이 패턴을 바탕으로 새로운 데이터에 대한 예측을 가능하게 합니다.\n",
    "\n",
    "- **비지도학습**: 정답이 없는 데이터로 학습하는 방식\n",
    "- **지도학습**: 정답이 있는 데이터로 학습하는 방식\n",
    "\n",
    "## 2. 모델링\n",
    "모델링은 아래와 같은 과정으로 머신러닝 모델을 생성하는 과정입니다.\n",
    "\n",
    "### 데이터 분리\n",
    "전체 데이터를 **train data**와 **test data**로 쪼개는 이유는 다음과 같습니다:\n",
    "- **Train data**를 통해 학습시킨 모델이 새로운 데이터, 즉 **unseen data**에 대해 얼마나 예측을 잘 수행하는지 공정하게 평가하기 위함입니다.\n",
    "\n",
    "### 모델링의 핵심\n",
    "모델링의 핵심은 **데이터로부터 패턴을 찾아내어 새로운 데이터를 예측하는 것**입니다. 만약 데이터를 분리하지 않고 훈련시킨 데이터로 모델 평가를 한다면, 모델이 그 데이터를 단순히 암기한 결과가 될 수 있기 때문에 실제 성능을 제대로 평가할 수 없습니다.\n",
    "\n",
    "## 3. 회귀 (Regression)와 모델링\n",
    "\n",
    "### 회귀란?\n",
    "- **회귀**는 데이터에서 패턴을 찾아내어 미래 값을 예측하는 기법입니다.\n",
    "- 통계나 머신러닝에서 중요한 개념으로, 집값 예측, 주식 가격 예측 등과 같이 연속적인 숫자를 다룰 때 사용됩니다.\n",
    "- 목표 변수가 불연속적일 때(범주형)는 **분류(Classification)**를 사용합니다.\n",
    "\n",
    "### 회귀의 종류\n",
    "- 선형회귀\n",
    "- 비선형 회귀\n",
    "- 릿지회귀\n",
    "- 라쏘회귀\n",
    "- 다항회귀 등 다양한 회귀 기법이 존재합니다.\n",
    "\n",
    "### 선형 회귀\n",
    "선형 회귀는 데이터를 가장 잘 설명하는 **회귀선**(예: 직선 y = ax + b)을 찾아 예측하는 과정입니다. 이 회귀선은 현재 데이터를 요약하고, 아직 관측되지 않은 미래 값을 예측하는 도구가 됩니다.\n",
    "\n",
    "#### 선형 회귀의 종류\n",
    "- **단순 선형 회귀**: 하나의 독립 변수를 사용하여 데이터를 직선 형태로 표현하는 경우\n",
    "- **다중 선형 회귀**: 두 개 이상의 독립 변수를 사용하여 데이터를 표현하는 경우\n",
    "\n",
    "### 단순선형회귀분석\n",
    "- **단순의 의미**: 하나의 독립 변수만 존재하는 선형 회귀입니다.\n",
    "- 데이터를 가장 잘 설명하는 직선 **y = aX + b**을 찾는 과정에 해당합니다. 여기서 **a**는 회귀계수(기울기), **b**는 y절편을 의미합니다.\n",
    "\n",
    "#### 최소제곱법 (Least Squares Method)\n",
    "최적의 회귀선은 잔차의 제곱의 합(Sum of Squared Residuals, RSS)이 최소가 되는 지점으로 구합니다. **잔차**는 실제 값과 예측값의 차이를 의미합니다.\n",
    "\n",
    "#### 경사하강법 (Gradient Descent)\n",
    "경사하강법은 최소제곱법과 마찬가지로 **a**와 **b**를 잘 찾는 방법이며, 계산이 복잡할 때 사용됩니다. 목적은 잔차 제곱을 줄여 가장 적합한 직선 **y = aX + b**을 찾는 것입니다.\n",
    "\n",
    "### 목적함수, 손실함수, 비용함수\n",
    "- **목적함수 (Objective Function)**: 우리가 달성하고 싶은 목표로, 최적화하려는 대상을 수학적으로 표현한 함수입니다. 머신러닝에서는 모델의 예측이 얼마나 잘 맞는지를 평가하는 기준이 됩니다.\n",
    "- **손실함수 (Loss Function)**: 개별 데이터 샘플에 대한 오차를 측정하는 함수입니다. 예를 들어 회귀 분석에서는 **평균제곱오차(MSE)**나 **평균절대오차(MAE)**가 손실함수로 사용됩니다.\n",
    "- **비용함수 (Cost Function)**: 전체 데이터셋에서 평균적인 손실을 측정하는 함수로, 모델을 훈련할 때 비용함수를 최소화하는 것이 목표입니다.\n",
    "\n",
    "#### 손실 함수 그래프\n",
    "손실 함수의 최솟값을 찾는 과정에서 기울기가 음수일 때는 현재 값이 커지게 하고, 기울기가 양수일 때는 현재 값이 작아지게 되어, 최솟값에 도달하게 됩니다.\n",
    "\n",
    "### 학습률 (Learning Rate)\n",
    "- **학습률**은 모델이 수렴할 때, 얼마나 보폭을 두고 이동할지를 결정하는 중요한 요소입니다. \n",
    "- 학습률이 너무 작으면 수렴하는 데 시간이 오래 걸리고, 너무 크면 발산하게 됩니다.\n",
    "\n",
    "#### Local Minima 문제\n",
    "비용 함수가 매끈한 형태가 아니기 때문에 **Local Minima**와 **Global Minima**가 존재할 수 있습니다. 작은 기울기에서는 **Local Minima**에서 빠져나오기 어려운 문제가 발생할 수 있습니다.\n",
    "\n",
    "### 해결법 – 모멘텀\n",
    "- 기존의 경사하강법은 이전 기울기를 고려하지 않습니다. 이를 해결하기 위해 모멘텀을 사용하여 이전 기울기를 반영해 이동 방향을 조정합니다. 모멘텀은 지역 최소값에서 빠져나오게 해줍니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222eeeca-9af6-4bf1-9198-6fcf925064fb",
   "metadata": {},
   "source": [
    "## 4. 다중선형회귀분석\n",
    "\n",
    "### 다중의 의미\n",
    "\n",
    "- **독립 변수가 2개 이상**인 경우의 선형회귀 → N가지의 독립변수\n",
    "- **독립 변수 $x_i$가 2개인 경우** 아래와 같이 3차원 공간에 표현됨. 회귀선은 평면이 된다: 말은 선형회귀지만 차원상으로는 평면\n",
    "- 단순회귀와 마찬가지로 **최소제곱법**을 사용할 수 있다. 관측치와 평면간의 차이가 **잔차**가 됩니다. → 파악이 힘들 땐 경사하강법(gradient descent) 사용해야 하는 경우도 단순회귀와 마찬가지로 적용됩니다.\n",
    "- 변수가 2개보다 많은 경우는 비슷한 느낌으로 확장됨.\n",
    "\n",
    "### 다중공선성 (Multicollinearity)\n",
    "\n",
    "- **변수들끼리 겹치는 상황**을 상상\n",
    "- 다중공선성은 회귀 분석에서 독립 변수들 간에 상관관계가 큰 경우 발생함.  \n",
    "  → 다중공선성이 높은 경우 어떤 독립 변수가 종속 변수에 얼마나 영향을 미치는지를 정확하게 구분하기 어렵기 때문에 회귀 모델은 어떤 변수의 영향을 반영해야 할지 불확실해지고, 그 결과 회귀분석의 정확도가 낮아짐.\n",
    "  \n",
    "#### 다중공선성 확인 방법\n",
    "\n",
    "1. **상관계수**: 상관계수는 변수간의 통계적 관계 즉, 상관관계의 정도를 수치로 나타낸 것.  \n",
    "   상관계수 값 $r$은 -1과 1 사이에 존재한다.\n",
    "   - $r$이 -1과 1에 가까울수록 두 변수의 상관성이 높다.\n",
    "   - $r$이 음수라면 음의 상관관계(한쪽이 커지면 한쪽은 작아짐), 양수라면 양의 상관관계(한쪽이 커지면 같이 커짐)를 뜻한다.\n",
    "   - 0은 선형 상관 관계 없음.\n",
    "   - 히트맵 혹은 `corr()` 함수를 통해 상관계수를 확인할 수 있다.\n",
    "   - `pairplot`과 같은 시각화로 산점도를 찍어 상관성을 직접 눈으로 확인할 수 있다.\n",
    "\n",
    "2. **VIF 지수 (분산 팽창 인수)**  \n",
    "   - 회귀 모델의 결정계수 R²를 사용하여 계산됨.\n",
    "   - VIF가 높으면 다중공선성이 존재한다고 판단.\n",
    "   - VIF = 1: 해당 독립 변수는 다른 변수와 상관관계가 전혀 없음을 의미합니다.\n",
    "   - VIF < 5: 일반적으로 다중공선성 문제가 없다고 간주됩니다.\n",
    "   - VIF > 5: 다중공선성의 징후가 있을 수 있으며, 주의를 요합니다.\n",
    "   - VIF > 10: 다중공선성이 심각하다고 간주되며, 이 변수를 포함한 회귀 모델은 불안정할 가능성이 큽니다. 이 경우 해당 변수를 제거하거나, 변수 간의 상관관계를 줄이기 위한 조치가 필요할 수 있습니다.\n",
    "\n",
    "#### 다중공선성 대처 방법\n",
    "\n",
    "1. **변수 제거 (변수 선택법)**: 말 그대로 독립변수로서 사용할 변수를 선택하는 방법이다.\n",
    "2. **변수 변환**: 변수들을 더하거나 빼서 새로운 변수 생성. 예) 남편의 수입과 아내의 수입이 서로 상관성이 높다면, 두 개를 더해 \"가족 수입\"이라는 하나의 변수로 투입.\n",
    "3. **규제 선형 모델 활용**: 릿지, 라쏘, 엘라스틱넷 등의 방법을 통해 모델의 복잡도를 줄이는 방법 사용.\n",
    "4. **PCA (주성분 분석)**: 데이터를 차원 축소하는 통계적 기법. PCA는 고차원 데이터에서 중요한 정보를 최대한 보존하면서 데이터의 복잡성을 줄이는 것을 목표로 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 규제선형모델\n",
    "\n",
    "- 모델이 학습 데이터에 과적합(overfitting) 되지 않도록 규제를 가하고자 등장한 모델.\n",
    "- 선형 회귀 모델에서는 특성에 곱해지는 계수(기울기)의 크기를 조정하는 것.\n",
    "- 특히 여러 독립 변수를 사용하여 예측하므로 모델이 복잡해지는 다중회귀에서 과적합(overfitting) 될 가능성 증가.\n",
    "- 기존 선형 모델에서는 **RSS(잔차 제곱합)**를 최소화하는 방식으로 비용 함수 설정 → 회귀 계수가 쉽게 커지게 되어 과적합 발생.\n",
    "- 비용 함수는 학습 데이터의 잔차 오류 값을 최소화하는 RSS 최소화 방법(최소제곱법)과, 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록 하는 방법이 서로 균형을 유지해야 함.\n",
    "\n",
    "### 규제선형모델의 종류\n",
    "\n",
    "1. **L2 규제 (릿지 회귀)**  \n",
    "   - 회귀 계수의 제곱에 대해 페널티를 부여하는 방식.  \n",
    "   - 예측 변수가 많을 때 고차원 데이터에서 모든 변수에 작은 가중치를 부여하고자 할 때 유용.\n",
    "   - 다중공선성이 존재할 때 변수 선택이 필요하지 않은 상황.\n",
    "\n",
    "2. **L1 규제 (라쏘 회귀)**  \n",
    "   - 회귀 계수의 절댓값에 대해 페널티를 부여하는 방식.  \n",
    "   - 예측 변수 수가 많고 그 중 일부만이 실제로 중요할 때 계수 중 일부를 정확히 0으로 만들어 변수 선택 효과를 부여.\n",
    "   - 모델의 해석을 간단하게 유지하고자 할 때 불필요한 변수를 제거하고 모델을 단순화.\n",
    "\n",
    "3. **L1 + L2 규제 (엘라스틱넷 회귀)**  \n",
    "   - 릿지의 계수 축소 기능과 라쏘의 변수 선택 기능을 모두 가짐.\n",
    "   - 예측 변수 수가 많고, 그 중 중요한 변수를 선택하면서 다중공선성을 관리할 때 유용.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 모델 평가 방법\n",
    "\n",
    "### 성능 평가 지표\n",
    "\n",
    "1. **평균 제곱 오차 (Mean Squared Error, MSE)**  \n",
    "   - 회귀 분석에서 모델의 예측값과 실제 관측값 사이의 오차 제곱의 평균을 의미.  \n",
    "   - MSE는 모델의 예측 성능을 평가하는 지표 중 하나입니다.\n",
    "\n",
    "2. **평균 절대 오차 (Mean Absolute Error, MAE)**  \n",
    "   - 모델의 예측값과 실제 관측값 사이의 절대값 오차의 평균을 의미.  \n",
    "   - MAE는 모델의 예측 성능을 평가하는 또 다른 지표입니다.\n",
    "\n",
    "### 변수 유의성 평가\n",
    "\n",
    "1. **T 검정**  \n",
    "   - **귀무가설 ($H_0$)**: 회귀 계수(a)가 0이다 → 독립변수가 종속변수에 영향을 주지 않는다.\n",
    "   - **대립가설 ($H_1$)**: 회귀 계수(a)가 0이 아니다 → 독립변수가 종속변수에 영향을 준다.\n",
    "   - **p-value < 0.05** → 유의 수준 5%에서 $H_0$ 기각 → **해당 변수는 유의미하다.**\n",
    "   - **p-value ≥ 0.05** → $H_0$ 기각 불가 → **해당 변수는 유의미하지 않다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0ebe1-a510-401f-a13d-04d3a99ea4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
