{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c45308-f27e-4bf0-b067-a418d296d2f6",
   "metadata": {},
   "source": [
    "# 1. 분류란?\n",
    "\n",
    "## (1) 머신러닝: 지도 학습과 비지도 학습\n",
    "\n",
    "### [ 머신러닝이란? ]\n",
    "- **머신러닝**: 인공지능의 한 분야로, 컴퓨터가 스스로 학습할 수 있도록 도와주는 알고리즘이나 기술을 개발하는 분야\n",
    "- 머신러닝에서 컴퓨터는 1) 알고리즘을 이용하여 데이터를 분석하고, 2) 분석 결과를 스스로 학습한 후, 이를 기반으로 3) 어떠한 판단이나 예측을 하게 됨\n",
    "\n",
    "### [ 지도 학습, 비지도 학습 ]\n",
    "- **머신러닝의 종류**:\n",
    "  - **지도 학습**: 문제와 정답을 모두 알려주고 공부시키는 방법\n",
    "  - **비지도 학습**: 답을 가르쳐주지 않고 공부시키는 방법\n",
    "  - **강화 학습**: 보상을 통해 상을 최대화/벌을 최소화하는 방향으로 공부시키는 방법\n",
    "\n",
    "- **지도 학습**: 정답(label)이 있는 데이터 사용, **예측값(prediction)**을 이미 만들어둔 정답과 같아지도록 기계를 학습시키는 것\n",
    "- **비지도 학습**: 정답(label)이 없는 데이터 사용, 데이터 속의 패턴 또는 각 데이터 간의 유사도를 기계가 학습하도록 하는 것\n",
    "- **강화 학습**: 시행착오를 반복하여 정답을 찾는 과정\n",
    "\n",
    "### 지도 학습과 비지도 학습 예시\n",
    "- 목표: 데이터를 기반으로 미래 예측\n",
    "- 예시: 기린과 고양이 구분\n",
    "  - **지도 학습**: 고양이/기린 사진에 각각 \"고양이\"/\"기린\" 레이블을 부여하고 학습\n",
    "  - **비지도 학습**: 레이블 없이 사진의 특성만으로 비슷한 사진들을 군집화\n",
    "\n",
    "### (참고) 군집화\n",
    "- **비지도학습 (군집화)**: 정답(label)이 없는 데이터를 통해 패턴 또는 유사도를 기계가 학습\n",
    "- 인간의 개입 없이 데이터 스스로 학습\n",
    "- 비지도학습의 종류: 군집화(Clustering), 밀도 추정(Density Estimation), 차원 축소(Dimensionality Reduction)\n",
    "- **군집화 활용**: 변칙감지(anomaly detection), 고객세분화\n",
    "\n",
    "## (2) 지도 학습: 회귀와 분류\n",
    "\n",
    "### [ 지도 학습 ]\n",
    "- **지도 학습**: 입력 값과 함께 결과 값(정답 레이블)을 같이 주고 학습을 시키는 방법\n",
    "- **학습의 방향**: **예측값(prediction)**과 정답(label)이 최대한 같아지도록 학습\n",
    "\n",
    "#### 구체적인 과정\n",
    "1. **labeled data**: 독립변수(feature)와 종속변수(target/label)가 모두 존재하는 데이터\n",
    "2. labeled data를 training set과 test set으로 분할\n",
    "3. training set으로 모델 학습: 독립변수를 통해 추정한 예측값이 종속변수와 같아지도록 모델 훈련\n",
    "\n",
    "#### (참고) 머신러닝에서 독립 변수와 종속 변수\n",
    "- **독립 변수**: 머신러닝에서 예측에 사용되는 입력 변수\n",
    "- **종속 변수**: 머신러닝에서 예측하고자 하는 대상 변수\n",
    "\n",
    "### [ 회귀와 분류 ]\n",
    "- **회귀(Regression)**: 주어진 데이터(X)를 기반으로 정답(Y)을 잘 맞추는 함수를 찾는 문제\n",
    "  - 예: 주택 가격(타겟) 예측\n",
    "    - 종속변수: 주택 가격\n",
    "    - 독립변수: 주택 평수, 역까지의 거리, 지역의 평균 소득\n",
    "\n",
    "- **분류(Classification)**: 기존 데이터의 레이블 패턴을 알고리즘으로 인지한 후 새로운 데이터의 레이블을 판별\n",
    "  - 예: 대출상환여부(0/1) 예측\n",
    "    - 종속변수: 대출상환여부(0/1)\n",
    "    - 독립변수: 고객의 특성, 대출의 특성\n",
    "  - 다른 예: 이메일 스팸 여부, 종양 악성 여부\n",
    "\n",
    "#### 회귀와 분류의 차이\n",
    "- **회귀**: 연속형 변수 예측 (예: 음식 배달 시간)\n",
    "- **분류**: 범주형 변수 예측 (예: 당뇨병 여부)\n",
    "\n",
    "#### [분류에 쓰이는 대표적인 머신러닝 알고리즘]\n",
    "1. **로지스틱 회귀(Logistic Regression)**: 독립변수와 종속변수의 선형 관계성에 기반해 분류\n",
    "2. **결정 트리(Decision Tree)**: 데이터 균일도에 따른 규칙 기반의 분류\n",
    "3. **서포트 벡터 머신(SVM)**: 개별 클래스 간의 최대 마진을 효과적으로 찾아 분류\n",
    "4. **최소 근접(Nearest Neighbor) 알고리즘**: 근접 거리 기준으로 분류\n",
    "\n",
    "## (3) 이진 분류와 다중 분류\n",
    "- **이진 분류(Binary Classification)**: 예측하고자 하는 변수가 어떤 기준에 대하여 참(True) 또는 거짓(False)의 값만을 가짐\n",
    "- **다중 분류(Multiclass Classification)**: 예측하고자 하는 변수가 가질 수 있는 값이 3개 이상임\n",
    "\n",
    "# 2. 분류 모델\n",
    "\n",
    "## (1) 로지스틱 회귀 (Logistic Regression)\n",
    "\n",
    "### 로지스틱 회귀란\n",
    "- 이진 분류 문제를 푸는 대표적인 알고리즘\n",
    "- 샘플이 특정 클래스에 속할 확률을 추정\n",
    "- 독립 변수의 선형 조합(선형 회귀)에 로지스틱 함수 적용해 출력값을 0~1 사이로 변환\n",
    "\n",
    "### 이진 분류를 다중 선형 회귀로 풀 때 문제점\n",
    "- 직선으로 이진 분류 문제를 풀 수 없음\n",
    "- 문제점:\n",
    "  1. S자 형태의 함수가 필요함 (직선은 분류 작업에 적합하지 않음)\n",
    "  2. 예측값이 0과 1 사이가 아닌 무한대 범위를 가질 수 있음\n",
    "\n",
    "### 시그모이드 함수 (Sigmoid function)\n",
    "- 시그모이드 함수 = 로지스틱 함수\n",
    "- 특징:\n",
    "  - 출력이 0과 1 사이의 값을 가짐\n",
    "  - S자 형태로 그려짐\n",
    "  - 입력값이 커지면 1에 수렴, 작아지면 0에 수렴\n",
    "- 수식: $H(x)=\\frac{1}{1+e^{-(wx+b)}}=sigmoid(wx+b)=\\sigma(wx+b)$\n",
    "\n",
    "### 시그모이드 함수의 가중치\n",
    "- 가중치 $w$: 그래프의 기울기 변화\n",
    "- 가중치 $b$: 그래프의 위치 변화\n",
    "- 인공지능은 주어진 데이터에 적합한 가중치 $w$와 $b$를 구하는 과정\n",
    "\n",
    "### 승산(odds)과 로지스틱 회귀\n",
    "- 승산 = \"사건 A가 일어날 확률\"/\"사건 A가 일어나지 않을 확률\"\n",
    "- 수식: $odds=\\frac{p(A)}{p(A^c)} = \\frac{p(A)}{1-p(A)}$\n",
    "- 로지스틱 회귀에서 승산 사용 이유: 해석을 더 직관적으로 하기 위함\n",
    "- logit 함수: $logit(p) = log(\\frac{p(A)}{1-p(A)})$\n",
    "\n",
    "## (2) 결정 나무 (Decision Tree)\n",
    "\n",
    "### 결정 나무란\n",
    "- 조건에 따라 데이터를 분류하며, 최종적으로 데이터가 순수한 label의 집합으로 구성될 때까지 분류를 반복하는 모델\n",
    "- 스무고개 놀이와 유사한 방식\n",
    "\n",
    "### 결정 나무 용어 정리\n",
    "- Root Node: 트리의 시작 노드\n",
    "- Edge: 노드와 노드를 연결하는 길목\n",
    "- Leaf Nodes: 트리의 가장 마지막 노드, 모델에서 label에 해당\n",
    "- Height(depth): 트리의 깊이\n",
    "- Level: 노드의 절대적 위계\n",
    "- Parent/Child: 상대적 높고 낮은 위계의 노드\n",
    "- Binary Tree: 최대 2개의 자식을 가진 트리\n",
    "\n",
    "### CART(Classification And Regression Tree) 알고리즘\n",
    "- 가장 대표적인 결정 나무 알고리즘\n",
    "- 데이터셋을 임계값 기준으로 두 child로 나누는 알고리즘\n",
    "- 불순도(지니 계수)가 낮아지는 방향으로 분류\n",
    "\n",
    "### 지니 계수(Gini Index)\n",
    "- 불순도를 나타내는 지표 (0~1 사이)\n",
    "- 수식: $Gini=1-\\sum_{i=1}^n{P_i}^2,\\space P_i\\text{는 class i의 비율}$\n",
    "- 최솟값 0: 데이터가 하나의 class에만 속할 때\n",
    "- 최댓값: 모든 class가 동일한 비율로 존재할 때\n",
    "\n",
    "### CART 알고리즘의 한계: Greedy Algorithm\n",
    "- 당장의 지니계수를 낮추는 판단만 하여 가장 효율적인 대안 제시 불가\n",
    "- 근시안적인 알고리즘으로 트리의 Height를 고려하지 않은 분류법 제시 가능\n",
    "\n",
    "### 실제 학습 시 고려사항\n",
    "1. 파라미터 설정:\n",
    "   - min_samples_split: 분할 위한 최소 샘플 수\n",
    "   - min_samples_leaf: 리프 노드의 최소 샘플 수\n",
    "   - max_leaf_nodes: 리프 노드의 최대 개수\n",
    "   - max_features: 분할에 사용할 특성의 최대 수\n",
    "\n",
    "2. 가지치기(Pruning):\n",
    "   - 불필요한 노드 제거하여 과적합 방지\n",
    "   - 일반화 성능 향상\n",
    "   - 모델 복잡도 감소\n",
    "\n",
    "## (3) 서포트 벡터 머신 (SVM)\n",
    "\n",
    "### 서포트 벡터 머신이란\n",
    "- 클래스를 분류할 수 있는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘\n",
    "- 명확히 분류 가능한 데이터에서 뛰어난 성능\n",
    "- 고차원 공간(다수의 feature)에서도 효과적\n",
    "\n",
    "### SVM의 구성\n",
    "- Support vector: 구분하는 선과 가장 가까운 포인트\n",
    "- Decision Boundary(결정 경계): 집단을 구분하는 선\n",
    "- Margin: 선과 각 점의 거리\n",
    "\n",
    "### 최적의 선을 찾는 방법\n",
    "- 결정 경계는 데이터로부터 가장 멀리 떨어져 있는 것이 좋음\n",
    "- SVM은 Margin(거리)이 가장 큰 경우를 선택하여 최적의 선 탐색\n",
    "\n",
    "## (4) KNN (K-Nearest Neighbor)\n",
    "\n",
    "### KNN이란\n",
    "- 유유상종 원리 활용: 비슷한 특성을 가진 데이터끼리 서로 가까이 있다는 점 이용\n",
    "- 데이터로부터 거리가 가까운 k개의 다른 데이터 레이블을 참조하여 분류하는 알고리즘\n",
    "\n",
    "### KNN 계산 순서\n",
    "1. 데이터 준비: 각 데이터는 특징 벡터와 레이블로 구성\n",
    "2. K 값 설정: 보통 홀수로 설정(동점 방지)\n",
    "3. 거리 계산: 새 데이터와 기존 모든 데이터 간 거리 계산(유클리드, 맨해튼 등)\n",
    "4. 가장 가까운 K개 이웃 선택\n",
    "5. 분류하기: K개 이웃 중 다수결로 클래스 결정\n",
    "\n",
    "### KNN의 장단점\n",
    "- 장점:\n",
    "  - 훈련이 필요하지 않음\n",
    "  - 정보 손실 없음\n",
    "- 단점:\n",
    "  - 쿼리 처리 시간이 오래 소요됨\n",
    "  - 이상치에 큰 영향을 받음(Not Robust)\n",
    "\n",
    "## 앙상블(Ensemble)\n",
    "\n",
    "### 앙상블이란\n",
    "- 여러 개의 개별 분류 모델들을 \"결합\"해 더 좋은 성능을 내는 기법\n",
    "- 약 분류기(Weak Classifier)를 결합하여 강 분류기(Strong Classifier) 구성\n",
    "- 여러 모델의 예측 결과를 종합하여 최종 예측\n",
    "\n",
    "### 앙상블의 종류\n",
    "1. 보팅(Voting): 다른 알고리즘의 모델을 병렬로 사용\n",
    "2. 배깅(Bagging): 동일 알고리즘의 모델을 병렬로 사용\n",
    "3. 부스팅(Boosting): 동일 알고리즘의 모델을 직렬(순차적)로 사용\n",
    "\n",
    "# 3️⃣ 분류 평가 지표\n",
    "\n",
    "## (1) 혼동 행렬\n",
    "- **개념**: 분류 모델의 예측 결과를 정확한/잘못된 예측으로 구분한 행렬\n",
    "- **구성 요소**:\n",
    "  - TP(True Positive): 실제 참, 참으로 예측\n",
    "  - TN(True Negative): 실제 거짓, 거짓으로 예측\n",
    "  - FP(False Positive): 실제 거짓, 참으로 예측(오류)\n",
    "  - FN(False Negative): 실제 참, 거짓으로 예측(오류)\n",
    "- **분류법**:\n",
    "  - T/F: 예측 성공/실패\n",
    "  - P/N: 참/거짓으로 예측\n",
    "\n",
    "- **평가 지표**:\n",
    "  - **정확도(Accuracy)**: 전체 예측 중 정확한 예측 비율 = (TP+TN)/(TP+TN+FP+FN)\n",
    "    - 단점: 데이터 불균형 시 신뢰도 낮음\n",
    "  - **정밀도(Precision)**: 참으로 예측한 것 중 실제 참인 비율 = TP/(TP+FP)\n",
    "  - **재현도(Recall)**: 실제 참인 것 중 참으로 예측한 비율 = TP/(TP+FN)\n",
    "\n",
    "- **상황별 중요 지표**:\n",
    "  - 암 환자 판단: Recall 중요(FN 감소 필요) → Threshold 낮춤\n",
    "  - 스팸 메일 분류: Precision 중요(FP 감소 필요) → Threshold 높임\n",
    "\n",
    "- **Precision-Recall 관계**:\n",
    "  - Threshold ↓ → Positive 예측 ↑ → Recall ↑, Precision ↓\n",
    "  - Threshold ↑ → Positive 예측 ↓ → Recall ↓, Precision ↑\n",
    "\n",
    "## (2) F1-Score\n",
    "- **개념**: 정밀도와 재현율의 조화 평균\n",
    "- **특징**: Precision과 Recall 간 균형 평가 지표\n",
    "\n",
    "## (3) ROC / AUC Curve\n",
    "- **ROC Curve**:\n",
    "  - 분류 성능을 보여주는 그래프\n",
    "  - TPR(True Positive Rate) vs FPR(False Positive Rate) 관계 표시\n",
    "  - TPR = Recall, FPR = 거짓인 것 중 참으로 잘못 예측한 비율\n",
    "\n",
    "- **AUC Curve**:\n",
    "  - ROC와 x축 사이의 면적(0~1 값)\n",
    "  - 1에 가까울수록 우수한 분류 성능 의미\n",
    "\n",
    "## (4) 다중 분류 평가 지표\n",
    "- **이진 분류**: 참/거짓 두 가지 값만 예측\n",
    "- **다중 분류**: 3개 이상 클래스로 분류\n",
    "\n",
    "- **평가 방법**:\n",
    "  - **Macro average**: 클래스별 평가 지표의 단순 평균\n",
    "  - **Weighted average**: 클래스별 샘플 수로 가중치 부여한 평균\n",
    "  - **Micro average**: 모든 클래스 예측 결과를 합산한 전체 성능 지표\n",
    "\n",
    "# 4️⃣ 하이퍼파라미터 최적화\n",
    "\n",
    "## (1) 하이퍼파라미터 최적화\n",
    "- **하이퍼파라미터**: 학습 시작 전 사용자가 직접 설정하는 변수\n",
    "- **최적화 과정**:\n",
    "  1. 탐색 범위 설정\n",
    "  2. 평가 지표 계산 함수 정의\n",
    "  3. 샘플링한 하이퍼파라미터로 검증 데이터 평가\n",
    "  4. 반복하며 범위 축소\n",
    "\n",
    "## (2) 하이퍼파라미터 최적화 방법\n",
    "- **Grid Search**:\n",
    "  - 정해진 범위 내 모든 하이퍼파라미터 조합 탐색\n",
    "  - 장점: 정확한 최적해 발견 가능\n",
    "  - 단점: 시간 소요 큼\n",
    "\n",
    "- **Random Search**:\n",
    "  - 정해진 범위 내 무작위 탐색\n",
    "  - 장점: Grid Search보다 빠름\n",
    "  - 단점: 정확도 낮음\n",
    "\n",
    "- **Bayesian Optimization**:\n",
    "  - 사전 정보 기반 확률적 추정 탐색\n",
    "  - 가우시안 프로세스 및 획득 함수 활용\n",
    "\n",
    "- **데이터 구성**:\n",
    "  - 훈련 데이터: 매개변수 학습\n",
    "  - 검증 데이터: 하이퍼파라미터 성능 평가\n",
    "  - 테스트 데이터: 범용 성능 평가\n",
    "\n",
    "- **자동화 프레임워크**:\n",
    "  - Optuna: 하이퍼파라미터 최적화 자동화 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a89169-caf4-439a-a665-83253e21979f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
