{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1643a7-386a-4627-a56a-b470f6c7f148",
   "metadata": {},
   "source": [
    "# 머신러닝과 모델링\n",
    "\n",
    "## 1. 머신러닝\n",
    "- **머신러닝**: 데이터에서 패턴을 찾아 새로운 데이터를 예측하거나 결정을 내리는 기술\n",
    "- **학습 방식**:\n",
    "  - **비지도학습**: 정답이 없는 데이터로 학습\n",
    "  - **지도학습**: 정답이 있는 데이터로 학습\n",
    "\n",
    "## 2. 모델링\n",
    "- 머신러닝 모델 생성 과정\n",
    "- **데이터 분할**:\n",
    "  - **훈련 데이터(train_set)**: 모델을 학습시키는 데이터\n",
    "  - **테스트 데이터(test_set)**: 학습된 모델을 검증하는 데이터 (unseen data)\n",
    "- 데이터를 분리하는 이유: 모델이 새로운 데이터에 대한 예측 성능을 공정하게 평가하기 위함\n",
    "\n",
    "# 회귀(Regression)와 모델링\n",
    "\n",
    "## 1. 회귀(Regression)란?\n",
    "- 데이터에서 패턴을 찾아 미래 값을 예측하는 것\n",
    "- 연속적인 숫자를 다룸 (예: 집값, 주식 가격 예측)\n",
    "- 목표변수가 연속적일 때는 회귀, 불연속적일 때는 분류\n",
    "\n",
    "## 2. 회귀의 종류\n",
    "- 선형회귀, 비선형 회귀, 릿지회귀, 라쏘회귀, 다항회귀 등\n",
    "\n",
    "## 3. 선형회귀\n",
    "- **선형회귀**: 데이터를 가장 잘 설명하는 회귀선(y=ax+b)을 찾아 예측하는 과정\n",
    "- **종속 변수(y)**: 예측하고자 하는 변수\n",
    "- **독립 변수(x)**: 예측을 위해 사용하는 변수\n",
    "\n",
    "### 선형 회귀의 종류\n",
    "- **단순 선형 회귀**: 하나의 독립 변수만 사용\n",
    "- **다중 선형 회귀**: 두 개 이상의 독립 변수 사용\n",
    "\n",
    "### 1. 단순선형회귀분석\n",
    "- 하나의 독립변수로 직선 y=aX+b를 찾는 과정\n",
    "- **최소제곱법(Least Squares Method)**: 잔차(실제값과 예측값의 차이)의 제곱 합이 최소가 되는 지점으로 최적의 회귀선을 구하는 방식\n",
    "- **경사하강법(Gradient Descent)**: 손실함수(비용함수)의 최솟값을 찾아가는 과정\n",
    "      1. 함수가 닫힌 상태인 경우\n",
    "      2. 함수가 너무 복잡해 미분 계수를 구하기 어려운 경우\n",
    "      3. Gradient Descent를 구현하는게 미분 계수를 구하는 것보다 더 쉬운 경우\n",
    "      4. 데이터 양이 너무 많은 경우 효율적으로 계산하기 위해 사용\n",
    "\n",
    "#### 경사하강법\n",
    "\n",
    "##### 기본 개념\n",
    "- 목적: 선형 회귀(y = ax + b)에서 최적의 a와 b를 찾는 방법\n",
    "- 원리: 손실함수(비용함수)의 최솟값을 찾아가는 과정\n",
    "\n",
    "##### 주요 용어 정리\n",
    "- **목적함수(Objective Function)**: 우리가 달성하고 싶은 목표를 수학적으로 표현\n",
    "- **손실함수(Loss Function)**: 개별 데이터에 대한 오차 측정 함수 (예: 제곱오차)\n",
    "- **비용함수(Cost Function)**: 전체 데이터셋에 대한 평균적인 손실\n",
    "\n",
    "##### 경사하강법 작동 원리\n",
    "1. 손실함수의 그래프에서 경사(기울기)를 계산\n",
    "2. 기울기가 가리키는 방향의 반대 방향으로 이동\n",
    "   - 기울기가 양수면 왼쪽(감소)으로 이동\n",
    "   - 기울기가 음수면 오른쪽(증가)으로 이동\n",
    "3. 최솟값에 도달하면 기울기가 0이 되어 더 이상 업데이트가 일어나지 않음\n",
    "\n",
    "##### 학습률(Learning Rate)\n",
    "- 의미: 각 반복에서 얼마나 큰 스텝으로 이동할지 결정하는 하이퍼파라미터\n",
    "- 특성:\n",
    "  - 학습률이 너무 작으면: 수렴까지 오랜 시간 소요\n",
    "  - 학습률이 너무 크면: 발산하여 최솟값에 도달하지 못함\n",
    "- 설정 방법: 경험적으로 0.01, 0.001 등의 값으로 시작하여 조정\n",
    "\n",
    "##### 지역 최소값(Local Minima) 문제\n",
    "- 비용함수가 여러 개의 최솟값을 가질 수 있음\n",
    "- 경사하강법은 작은 기울기에서 적게 이동하므로 지역 최소값에 갇힐 수 있음\n",
    "\n",
    "##### 해결책: 모멘텀(Momentum)\n",
    "- 개념: 이전 이동 방향을 고려하여 관성을 부여\n",
    "- 장점:\n",
    "  - 작은 기울기도 쉽게 넘어갈 수 있음\n",
    "  - 지역 최소값 탈출 가능\n",
    "  - 수렴 속도가 빨라짐\n",
    "\n",
    "### 2. 다중선형회귀분석\n",
    "- 독립 변수가 2개 이상인 경우의 선형회귀\n",
    "- **다중공선성**: 독립 변수들 간에 강한 상관관계가 있는 경우 발생하는 문제\n",
    "\n",
    "#### 다중공선성 확인 방법\n",
    "1. **상관계수**: 변수 간 상관관계의 정도를 수치로 표현 (-1~1)\n",
    "2. **VIF 지수**(분산 팽창 인수):\n",
    "   - VIF < 5: 다중공선성 문제 없음\n",
    "   - VIF > 5: 다중공선성 징후\n",
    "   - VIF > 10: 심각한 다중공선성\n",
    "\n",
    "#### 다중공선성 대처 방법\n",
    "1. **변수 제거**: 상관성이 높은 변수 중 일부 제거\n",
    "2. **변수 변환**: 변수들을 결합하여\n",
    "3. **규제 선형 모델**: 릿지, 라쏘, 엘라스틱넷 활용\n",
    "4. **PCA**(주성분분석): 데이터의 차원을 축소\n",
    "\n",
    "## 4. 규제선형모델\n",
    "\n",
    "### 선형모델의 과적합(Overfitting) 문제\n",
    "- 과적합된 모델은 학습 데이터에는 완벽히 맞지만, 테스트 데이터에서는 성능이 크게 떨어짐\n",
    "- 특히 다중회귀에서 여러 독립 변수를 사용할 때 과적합 가능성이 증가함\n",
    "\n",
    "### 규제선형모델의 개념\n",
    "- 모델이 학습 데이터에 과적합되지 않도록 규제를 가하는 모델\n",
    "- 선형 회귀에서 특성에 곱해지는 계수(기울기)의 크기를 조정\n",
    "- 비용 함수 = Min(학습데이터 잔차 오류 최소화 + 회귀계수 크기 제어)\n",
    "- alpha(튜닝 파라미터)로 규제 강도 조절\n",
    "  - alpha↑: 규제 강도가 세져서 계수 값 감소, 과소적합 유도\n",
    "  - alpha↓: 계수 감소 효과 줄어들어 과대적합 가능성 증가\n",
    "\n",
    "### 규제선형모델의 종류\n",
    "\n",
    "### 1. 릿지(Ridge) 회귀 (L2 규제)\n",
    "- 회귀계수(W)의 제곱에 페널티 부여\n",
    "- 큰 회귀계수일수록 더 강한 페널티 받음\n",
    "- 모든 계수의 크기는 작아지지만 완전히 0이 되지는 않음\n",
    "- 특징: 계수 크기 조절 기능, 모든 변수 유지하면서 크기 적절히 줄임\n",
    "\n",
    "### 2. 라쏘(Lasso) 회귀 (L1 규제)\n",
    "- 회귀계수(W)의 절댓값에 페널티 부여\n",
    "- 회귀계수 크기와 상관없이 같은 패널티 받음\n",
    "- 작은 계수는 0이 되어 완전히 제거될 수 있음\n",
    "- 특징: 변수 선택 기능, 비중이 낮은 변수는 제거되고 중요 변수만 남음\n",
    "\n",
    "### 3. 엘라스틱넷(Elastic Net) 회귀 (L1 + L2 규제)\n",
    "- L1 규제와 L2 규제를 결합한 모델\n",
    "- L1의 변수 선택 기능과 L2의 규제 기능을 동시에 활용\n",
    "- 불필요한 변수는 제거하고, 남은 변수들은 적절한 크기로 유지\n",
    "- 특징: 희소성(변수 제거)과 안정적 예측을 동시에 달성\n",
    "\n",
    "적절한 규제선형모델 선택\n",
    "릿지 회귀: 예측 변수가 많거나 다중공선성이 존재할 때\n",
    "라쏘 회귀: 일부 변수만 중요하거나 모델을 단순화하고 싶을 때\n",
    "엘라스틱넷: 변수 간 상관관계가 높으면서 중요 변수 선택이 필요할 때\n",
    "\n",
    "## 5. 모델 평가 방법\n",
    "\n",
    "### 1. 성능 평가 지표\n",
    "- MSE(평균 제곱 오차): 예측값과 실제값 차이의 제곱 평균, 큰 오차에 민감\n",
    "- MAE(평균 절대 오차): 예측값과 실제값 차이의 절댓값 평균, 이상치에 덜 민감\n",
    "\n",
    "### 2. 변수 유의성 평가 - t검정\n",
    "- 독립 변수의 회귀계수가 유의미한지 검정\n",
    "- 귀무가설(H₀): 회귀계수가 0이다 (변수가 영향을 주지 않음)\n",
    "- 대립가설(H₁): 회귀계수가 0이 아니다 (변수가 영향을 줌)\n",
    "- p-value < 0.05면 귀무가설 기각, 해당 변수는 유의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec631b39-68eb-48a7-b972-e5d2f0836a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
