{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddd1f8f",
   "metadata": {},
   "source": [
    "# 목표\n",
    "#### 1. 다중선형회귀의 기본가정을 알고 검정할 수 있다.\n",
    "#### 2. 비선형 모델과 선형 모델을 종합적으로 이해할 수 있다.\n",
    "#### 3. 모델 실습을 통해 회귀 모델링을 경험한다.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e53aa7",
   "metadata": {},
   "source": [
    "# Review: 회귀\n",
    "## 0-1. 회귀란?\n",
    "### 정의\n",
    "* 연속적인 데이터에서 패턴(함수 관계)을 찾아내는 통계적 방법\n",
    "### 종류\n",
    "* 선형회귀\n",
    "* 비선형회귀\n",
    "* 로지스틱회귀\n",
    "* 릿지회귀\n",
    "* 라쏘회귀\n",
    "* 다항회귀\n",
    "### 활용\n",
    "* 데이터 요약 및 예측\n",
    "* 시계열 모델링\n",
    "* 변수 간 인과관계 발견 등\n",
    "### 평가 지표\n",
    "* MSE\n",
    "* MAE\n",
    "* R-squared\n",
    "* Adjusted R-Squared\n",
    "* AIC\n",
    "* BC(SC)\n",
    "> 회귀는 데이터분석의 유용한 통계적 **도구**이자, ML/DL 이해의 **밑바탕**!!\n",
    "\n",
    "## 0-2. 단순/다중선형회귀분석\n",
    "### 단순선형회귀분석(Simple Linear Regression)\n",
    "* 하나의 독립변수만 존재\n",
    "### 다중선형회귀분석(Multiple Linear Regression)\n",
    "* 독립변수가 2개 이상\n",
    "\n",
    "## 0-3. 최적의 회귀선 찾기: 최소제곱법(Least Squares Method), 최소자승법(Ordinary Least Square)\n",
    "* 보통 **OLS**라고 함\n",
    "* `잔차` = 회귀선($\\hat{y}$)(예측값) - 관측된 데이터값($y$)\n",
    "* `잔차의 제곱의 합`을 최소화하는 회귀선을 찾는 방법!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c549a80",
   "metadata": {},
   "source": [
    "# 1. 선형회귀\n",
    "## 1-0. 전체적인 회귀 분석 프로세스\n",
    "### 1️⃣ 사전 검증\n",
    "* 데이터 탐색 및 전처리\n",
    "    * 결측치(NaN) 처리\n",
    "    * 이상치(Outlier) 처리\n",
    "* 회귀 분석의 기본 가정 6가지 검토\n",
    "### 2️⃣ 모델 생성 및 모델 fit\n",
    "* 회귀 모델 생성, 모델 설정, 모델 학습\n",
    "* 모델의 유의성 검정\n",
    "    * F-검정, T-검정\n",
    "### 3️⃣ 모델 성능 평가\n",
    "* R-squared\n",
    "* 회귀 분석 후 검증 가능한 가설 검토\n",
    "### 4️⃣ 모델 성능 개선\n",
    "* 필요 없는 변수 제거\n",
    "* 비선형 모델 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905abfcd",
   "metadata": {},
   "source": [
    "## 1-1. 다중선형회귀의 수식적 이해\n",
    "> $y_i = \\beta_0+\\beta_1X_{1i}+\\beta_2X_{2i}+...+\\beta_kX_{ki}+\\varepsilon_i $\n",
    "> * **변수 설명**\n",
    ">   * $y_i$: i번째 종속변수 (우리가 관심 있는 값)\n",
    ">   * $X_{1i}, X_{2i},..., X_{ki}$: $y_i$에 대한 여러 개의 설명변수(독립변수)\n",
    ">   * $\\beta_0$: 절편. 설명변수($X$)가 모두 0일 때의 y의 값\n",
    ">   * $\\beta_1, \\beta_2, ..., \\beta_k$: 설명변수에 의한 회귀 계수 (각 변수 X의 영향력)\n",
    ">   * $\\epsilon_i$: 오차항\n",
    "\n",
    "여기서 상수항($\\beta_0$)과 오차항($\\epsilon_i$)을 제외하면, 밑의 식처럼 쓸 수 있음.\n",
    "> $y=w_0x_0+w_1x_1+\\cdot\\cdot\\cdot+w_mx_m=\\sum_{i=0}^mw_ix_i=w^Tx$\n",
    "* 결국 **선형회귀식**은 \n",
    "    * 종속변수 y에 대한 설명변수(or feature)의 **가중평균**이라고 할 수 있음.\n",
    "    * 설명변수에 가중치를 곱해 더함으로써 y를 얼마나 잘 설명하는지를 나타냄.\n",
    "* 그러나 선형회귀를 모든 상황에 쓸 수 있는 것은 아님.\n",
    "    * 선형 회귀를 사용하기 위해서는 다음의 가정이 필요함! (분석 결과의 신뢰성, 분석 가능을 위한 가정)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b0d0ce",
   "metadata": {},
   "source": [
    "## 1-2. 다중선형회귀의 기본가정\n",
    "### 1️⃣ 가정1: 선형성(linearity)\n",
    ">> $E(y|X) = \\beta_1+\\beta_2X_2+...+\\beta_nX_n$\n",
    "> * 종속변수와 설명변수 간의 관계가 **선형적**이어야 한다. (위의 형태를 따라야 함)\n",
    "* 기본 가정 중 당연한 가정(아무래도.. 다중'선형'회귀니까..)!\n",
    "\n",
    "### 2️⃣ 가정2: 독립성(independence)\n",
    ">> $X_i, X_j \\ is \\ linearly\\ independent \\ for\\ all \\ i,j$\n",
    "> * 각각의 설명변수가 서로 **선형독립적**이어야 한다.\n",
    "* 선형독립적이지 않은 경우, **다중공선성(multi-collinearity)** 가 있다고 함.\n",
    "* **다중공선성**: 독립변수들이 서로 강한 상관관계를 가져서, 어떤 변수가 종속 변수에 가장 큰 영향을 주는지를 알 수 없는 상태\n",
    "    * 다중공선성이 있는지 알아보는 방법\n",
    "        * 상관계수 구하기\n",
    "        * VIF 구하기\n",
    "    * 다중공선성 대처 방법\n",
    "        * 변수 제거\n",
    "        * 규제 선형 모델\n",
    "        * PCA\n",
    "\n",
    "### 3️⃣ 가정3: 오차항의 평균은 0이다 (0에 가까워야 한다)\n",
    ">> $E(\\varepsilon_i|X_i) = 0$\n",
    "* **오차항($\\epsilon$)**: 실제 값과 예측 값 사이의 차이\n",
    "    * 다른 말로 white noise(백색 잡음)이라고도 함\n",
    "    * 가정3은 백색 잡음의 성질을 만족하기 위해 필요한 가정 중 하나임\n",
    "        * **백색 잡음(white noise)**: 특정 패턴 없이 랜덤하게 변하는 신호\n",
    "            * 랜덤한 신호 = 평균적으로 어느 방향에 치우치지 않음 = 평균이 0\n",
    "        \n",
    "### 4️⃣ 가정4: 등분산성(Homoscedasticity)\n",
    ">> $Var(\\varepsilon_i|X_i) = \\sigma^2$\n",
    "> * 오차항($\\epsilon$)의 분산($\\sigma^2$)이 일정해야 한다.\n",
    "* **이분산성(heteroscedasticity)**: 오차항의 분산이 일정하지 않은 경우\n",
    "    * 특히 **횡단면 자료**(어느 한 시점에 관찰된 여러 변수들의 관측치)에서 많이 나타남.\n",
    "    * 이 경우, 회귀 분석의 결과가 정확하지 않을 수 있음. (OLS 추정치의 분산이 inefficient)\n",
    "        * 등분산성 가정을 만족하지 않으면, <br>\n",
    "            모델의 예측이 **일부 구간**에서는 **매우 정확**하지만,\n",
    "            **일부 구간**에서는 오차가 매우 커질 수 있음.\n",
    "* 등분산성 알아보는 방법\n",
    "    1. 잔차의 도표화\n",
    "    2. 검정 (Brown-Forsythe, Breusch-Pagan)\n",
    "    \n",
    "### 5️⃣ 가정5: 오차항은 자기상관되어 있지 않다(non-autocorrelation)\n",
    ">> $Cov(\\varepsilon_i, \\varepsilon_j|X_i) =0$\n",
    "> 오차항의 **공분산(Cov)** 은 항상 0이어야 한다.\n",
    "* **공분산**: 두 변수 간의 **상관된 정보**를 나타내는 지표. <br>\n",
    "   즉, 두 변수가 **어떻게 함께 변하는지**를 측정하는 값.\n",
    "   * `Cov(X,Y)`에 대해,\n",
    "       * `Cov(X,Y)>0`: X가 증가하면 Y도 증가하는 관계(양의 상관관계)\n",
    "       * `Cov(X,Y)<0`: X가 증가하면 Y는 감소하는 관계(음의 상관관계)\n",
    "       * `Cov(X,Y)=0`: X와 Y는 독립적인 관계\n",
    "* **자기상관(Autocorrelation)**: 오차항의 공분산이 0이 아닌 경우.\n",
    "    * 한 변수의 **현재 값이 과거 값과 상관관계**를 가지는 현상.\n",
    "    * 특히 **시계열 자료**에서 많이 나타남.\n",
    "    * 모델이 데이터를 랜덤하게 예측하는 것이 아니라, <br>\n",
    "        **이전 데이터의 패턴을 학습하여 반복적인 오류**를 만들어 낼 가능성이 높음.\n",
    "    * 자기상관 알아보는 방법\n",
    "        1. 더빈-왓슨 검정(Durbin-Watson Test)\n",
    "        2. ACF, PACF 함수 찍어보기\n",
    "       \n",
    "### 6️⃣ 가정6(선택): 정규성(normality)\n",
    ">> $\\varepsilon_i \\sim N(0, \\sigma^2)$\n",
    "> 오차항이 정규분포를 따른다.\n",
    "* 가정6은 위배되어도 다중선형회귀분석의 결과에 큰 영향을 주지 않음.\n",
    "\n",
    "* **오차항이 정규분포를 따라야 하는 이유?**\n",
    "    * 애초에 가정이 정규분포임\n",
    "        * 회귀계수들의 신뢰구간, p-value 계산에 사용되는 공식\n",
    "    * 오차항이 정규성을 만족한다는 가정을 따르지 않으면, <br>\n",
    "        p-value가 잘못 계산될 수 있어서 <br>\n",
    "        결과 해석의 신뢰성이 떨어짐.\n",
    "\n",
    "* **정규성 알아보는 방법**\n",
    "    1. 샤피로-윌크 검정(Shapiro-Wilk-Test)\n",
    "    2. 자퀴-베라 검정(Jarque-Bera Test, DB)\n",
    "    3. Q-Q plot 찍어보기\n",
    "\n",
    "### 🤔 이러한 가정 6가지는 모두 회귀분석 전에 검증해야 하는 걸까??\n",
    "> ### No! 회귀분석 **후**에 검증 가능한 가정들도 있음.\n",
    "\n",
    "### 👀 회귀분석 전 vs. 회귀분석 후: 검증해야 할 가정 정리\n",
    "|기본 가정|회귀 분석 전에 검증 가능?|회귀 분석 후에 검증 가능?|검증 방법|\n",
    "|:---|:-----:|:-----:|:-----:|\n",
    "|선형성|✅|✅|scattor plot 찍어보기|\n",
    "|독립성 (다중공선성)|✅|✅|VIF 지수, 상관계수|\n",
    "|오차항 평균이 0이다|❌|✅|np.mean(residuals)|\n",
    "|등분산성|❌|✅|잔차의 도표화|\n",
    "|오차항 자기상관 없다|✅<br>(회귀 분석 전에는 간접적으로 검증 가능, 분석 후에 최종 검증)|✅|Durbin-Watson 검정(분석 후)|\n",
    "|정규성|❌|✅|Shapiro-Wilk 검증, Q-Q plot|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5cad0e",
   "metadata": {},
   "source": [
    "## 1-3. 회귀분석 평가방법\n",
    "> 회귀분석의 결과가 얼마나 유효한지 평가하고 싶다면? <br>\n",
    ">   * **회귀선**을 데이터와 함께 **시각화**하거나, <br>\n",
    ">   * **통계지표**를 이용하면 됨!\n",
    "### 🎨 시각화\n",
    "<figure class=\"half\">\n",
    "    <a href=\"link\"><img src=\"./img/img1.png\" width=\"49%\"></a>\n",
    "    <a href=\"link\"><img src=\"./img/img2.png\" width=\"49%\"></a>\n",
    "</figure><br>\n",
    "\n",
    "* 회귀선이 데이터 전반을 잘 요약하고 있음.\n",
    "* 하지만 둘 중 어느 회귀선이 해당 데이터를 더 잘 요약하는지 객관적으로 비교하기는 어려움.\n",
    "* 두 집단 차이의 정도를 객관적으로 비교하기 위해서 **검정**을 했듯이, <br>\n",
    "    이 경우에도 **통계지표**를 활용하는 것이 좋음. <br>\n",
    "\n",
    "### 📊 통계지표\n",
    "<img src=\"./img/img4.jpg\">\n",
    "\n",
    "#### 🤨 모델의 유의성 검정: Prob(F-statistic), T-statistic)\n",
    "##### ☑️ F-검정, T-검정 비교\n",
    "검정 종류|검정 대상|목적|해석 기준\n",
    ":---:|:---:|:---:|:-----:\n",
    "F-검정|전체 모델|회귀 모델이 유의미한지 검정|p < 0.05이면 회귀 모델이 유의미하다\n",
    "T-검정|개별 변수|특정 독립 변수가 유의미한지 검정|p < 0.05이면 해당 변수가 유의미하다\n",
    "##### ☑️ F-검정: F-Statistic, Prob(F-statistic)\n",
    "* **F-statistic**은 **회귀 모델이 유의미한지** 검정하는 지표<br>\n",
    "    (종속변수 y와 전체 독립변수 X 간의 관계가 통계적으로 유의미한지)\n",
    "* 귀무가설 $H_0$: 모든 회귀 계수가 0이다 (독립 변수들이 종속 변수들과 관계가 없다)\n",
    "* 대립가설 $H_1$: 적어도 하나의 회귀 계수는 0이 아니다 (독립 변수 중 하나 이상이 종속 변수에 영향을 준다)\n",
    "* **수치 해석**\n",
    "    * F-Statistic의 값이 클수록 모델이 통계적으로 유의미하다는 것을 의미함.\n",
    "    * Prob(F-statistic)은 F-statistic의 p-value로, <br>\n",
    "        Prob가 0.05보다 작다면, 귀무가설을 기각할 수 있음. <br>\n",
    "        (= 적어도 하나의 독립 변수가 종속 변수와 유의미한 관계를 갖고 있다)\n",
    "        (=회귀 모델이 유의미하다)\n",
    "##### ☑️ T-검정: T-statistic\n",
    "* **T-statistic**은 각 **독립 변수가 종속 변수 y에 유의미한 영향을 미치는지** 판단하는 지표 <br>\n",
    "* 귀무가설 $H_0$: 해당 변수의 회귀 계수가 0이다 (이 변수는 종속 변수에 영향을 미치지 않는다)\n",
    "* 대립가설 $H_1$: 적어도 하나의 회귀 계수는 0이 아니다 (이 변수는 종속 변수에 유의미한 영향을 준다)\n",
    "* p-value(P>|t|)를 기준으로 가설 검정을 수행함. (아래는 예시)\n",
    "    * **CRIM, ZN, INDUS, DIS, B, LSTAT**: <br> p-value가 모두 0.05보다 작으므로(각각 0.004, 0.000, 0.000, 0.000, 0.038, 0.000) 각 변수가 종속 변수(MEDV)에 유의미한 영향을 미친다.\n",
    "    * **AGE, RAD**: <br> p-value가 모두 0.05보다 크므로 (각 0.185, 0.122) 각 변수가 종속 변수(MEDV)에 유의미한 영향을 미치지 않는다.\n",
    "#### ⭐️ 모델의 성능 평가: R-squared, Adjusted R-squared, AIC, BIC\n",
    "##### ☑️ 결정 계수 (R-squared, $R^2$)\n",
    "* 회귀 분석에서 모델이 설명하는 데이터의 총 변동(평균과의 차이) 중에서 설명된 비율을 나타냄.\n",
    "    * 데이터의 평균을 예측값으로 사용할 때보다,<br>\n",
    "        모델을 예측값으로 사용할 때 얼마나 더 정확한 예측인지 나타냄.\n",
    "* 변수 설명\n",
    "    * $\\bar{Y}$: 평균\n",
    "    * $\\hat{Y}$: 예측 값\n",
    "    * $Y$: 실제 값\n",
    "* 위 그림에서는 노란색선으로 표시된 전체 데이터가 평균임\n",
    "* 초록색 선: SST\n",
    "* 보라색 선: SSR\n",
    "* 빨간색 선: SSE\n",
    "> $R^2 = \\dfrac{SSR}{SST} = 1 - \\dfrac{SSE}{SST}$\n",
    "* 모델이 데이터를 **얼마나 잘 설명하는지**를 측정하는 지표\n",
    "    * 0과 1 사이의 값으로 나타남\n",
    "* 1에 가까울수록 SSR과 SST의 값이 비슷해지고, SSE는 0에 가까워짐\n",
    "    * 모델이 데이터를 잘 설명함\n",
    "* 0이면, 데이터들의 평균을 구한 것과 모델로 예측한 것이 동일한 것\n",
    "    * 모델이 데이터를 전혀 설명하지 못함\n",
    "    \n",
    "##### ☑️ 조정된 결정 계수 (Adjusted R-squared)\n",
    "> R-square 값이 높으면 높을수록 데이터를 잘 설명한다. 그럼 R-square의 값이 높기만 하면 무조건 좋은 걸까?\n",
    "* $R^2$는 독립 변수의 개수가 증가하면 증가할수록 자연스럽게 증가함. <br>\n",
    "    ➡️ 데이터와 큰 관련성이 없는 변수를 추가해도 $R^2$은 높아짐.\n",
    "* **Adjusted $R^2$**: 변수의 개수 증가에 덜 민감하도록 조정한 지표\n",
    "    > $\\bar{R}^2=\\dfrac{SSR}{SST}\\dfrac{n-1}{n-k}=1-(\\dfrac{SSE/(n-k)}{SST/(n-1)})$\n",
    "* **변수 설명**\n",
    "    * $n$: 샘플 수(데이터 개수)\n",
    "    * $k$: 독립 변수의 개수\n",
    "* 변수의 개수($k$)가 증가하면 전체 값은 감소하게 됨. <br>\n",
    "    즉, 변수 개수에 따른 설명력 추가 부분을 상쇄하는 효과를 낼 수 있음. <br><br>\n",
    "* $\\dfrac{SSE}{n-k}$ VS. $\\dfrac{SST}{n-1}$\n",
    "    * 변수를 추가했을 때, **SSE**(회귀선에 의해 설명되지 않는 편차)**가 충분히 감소**하면, $\\dfrac{SSE}{n-k}$가 작아져서 $R^2$**이 증가**한다.\n",
    "    * 변수를 추가했을 때, **SSE**(회귀선에 의해 설명되지 않는 편차)**가 거의 그대로**라면(데이터와 관련성이 없는 변수를 추가하면), $\\dfrac{SSE}{n-k}$에서 SSE는 동일하고 n-k만 감소하므로 $\\dfrac{SSE/(n-k)}{SST/(n-1)}$에서 **분자 값이 커진다.** 따라서 **Adjusted $R^2$은 작아진다.** (1에서 더 큰 값을 빼니까)\n",
    "    * 이렇게 식에 $\\dfrac{n-1}{n-k}$**을 곱한 것**을 **변수의 개수(k)에 따른 penalty를 부여했다**고 표현함.\n",
    "\n",
    "##### ☑️ AIC, BIC(SC)\n",
    "* AIC와 BIC 두 지표 모두 정보기준(information criteria)이라고도 불림\n",
    "* 값이 **낮을수록** 좋다고 평가함\n",
    "* AIC = Akaike Information Criterion\n",
    "    * BIC에 비해 복잡성에 대한 패널티가 비교적 작음.\n",
    "* BIC = Bayesian Information Criterion, SC = Schwarz Criterion\n",
    "    * AIC보다 더 엄격한 기준\n",
    "    * 데이터의 양(n)에 따라 더 강한 패널티를 부과함.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39fecb8",
   "metadata": {},
   "source": [
    "# 2. 비선형회귀\n",
    "<img src=\"./img/img6.jpg\">\n",
    "\n",
    "> 위처럼 **데이터셋의 분포가 선형적이지 않은 경우**는 어떤 회귀를 사용해야 할까? <br>\n",
    "> ➡️ **선형적이지 않은 식을 작성**하면 됨!\n",
    "\n",
    "\n",
    "## 2-1. 다항식 회귀모델(Polynomial regression model)\n",
    "<img src = \"./img/img7.png\">\n",
    "\n",
    "* 기존의 선형 회귀: $Y=a+bX$\n",
    "* 다항식 회귀 모델: $Y=a+bX+cX^2+dX^3+...$ 의 형태로, <br>\n",
    "    독립 변수에 **거듭제곱 항**을 추가하여 데이터의 **곡선적(비선형적) 특성을 모델링**함.\n",
    "### ☑️ 사용 방법\n",
    "1. 기존의 선형 모델 $Y=a+bX$에 대해 $cX^2, dX^3$ 등을 포함하여 모델을 확장\n",
    "2. 확장한 상태에 선형 회귀를 적용\n",
    "### ☑️ 코드\n",
    "```python\n",
    "# 1️⃣단계. 기존의 변수들을 다항식으로 만드는 코드\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "\n",
    "# 2️⃣단계. 그 다음에 statsmodel이나 sklearn의 선형회귀 클래스를 똑같이 이용함\n",
    "# 2-1. statsmodels의 sm을 활용하는 방법\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_poly = sm.add_constant(X_poly)\n",
    "model = sm.OLS(Y, X_poly).fit()\n",
    "Y_pred = model.predict(X_poly)\n",
    "\n",
    "# 2-2. sklearn의 LinearRegression 활용하는 방법\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, Y)\n",
    "Y_pred = model.predict(X_poly)\n",
    "\n",
    "```\n",
    "#### (참고) `statsmodel의 SM.OLS`와 `sklearn의 LinearRegression`의 비교\n",
    "<img src=\"./img/img13.png\">\n",
    "<img src=\"./img/img14.png\">\n",
    "    \n",
    "\n",
    "### ☑️ 다항식 회귀모델의 장단점\n",
    "* **장점**: 선형 모델에 비해 회귀선을 잘 fit하도록 그릴 수 있음.\n",
    "* **단점**: 너무 많은 feature를 이용하면 결과가 overfitting될 수 있음. <br>\n",
    "    (overfitting을 방지하기 위해 릿지 규제, 라쏘 규제를 이용할 수 있음.)\n",
    "    \n",
    "### ☑️ 다항식 회귀모델이 적합한 상황\n",
    "    * 관계가 직선적이지 않고 **곡선을 그리는 데이터 패턴**에서 유용함.\n",
    "    * **성장률, 감소율** 등이 시간에 따라 변화하는 속도가 다른 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab2b20",
   "metadata": {},
   "source": [
    "## 2-2. 지수 회귀모델(Exponential model)\n",
    "<img src=\"./img/img8.png\">\n",
    "\n",
    "* 지수 회귀모델은 **종속 변수가 지수적으로 변화하는 관계를 모델링 할 때** 사용할 수 있음.\n",
    "\n",
    "### ☑️ 사용 방법\n",
    "1. 지수함수 형태의 데이터는 $Y=ae^{bX}$의 수식을 따름.\n",
    "2. 이때, 종속변수 Y에 로그함수를 적용하기. <br>\n",
    "    $Y=ae^{bX}$의 양변에 로그를 취하면 <br>\n",
    "    $\\ln Y = \\ln a + bX$가 됨. <br>\n",
    "    여기서 $\\ln Y$를 $Y'$으로, $\\ln a$를 $a'$으로 본다면 $Y'=a'+bX$가 됨.\n",
    "3. ➡️ 지수 모델은 **종속 변수 Y에 로그**를 취한 후, **선형 회귀를 적용**하면 됨!\n",
    "    <br> 이렇게 계산된 $Y'$은 $\\ln Y$이므로 기존의 데이터($Y$)와 형태가 달라짐.\n",
    "4. ➡️ 따라서 **원래의 데이터 형태로 복구하기 위해 예측 값에 지수함수($e^x$)를 적용**하여 로그 변환하기 전 $Y$로 변형\n",
    "\n",
    "### ☑️ 코드\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 지수함수 형태의 임의의 데이터 생성 \n",
    "np.random.seed(42)\n",
    "X = np.linspace(1, 10, 20).reshape(-1, 1)\n",
    "Y = 2 * np.exp(0.8 * X).flatten() + np.random.normal(0, 10, X.shape[0]) \n",
    "\n",
    "# 종속변수 Y에 자연 로그 변환\n",
    "log_Y = np.log(Y) # log_Y: 로그 변환된 Y\n",
    "\n",
    "# 회귀 모델 학습 (log(Y) = a + bX)\n",
    "X_const = sm.add_constant(X)  \n",
    "model = sm.OLS(log_Y, X_const).fit()\n",
    "\n",
    "# 예측값 변환 (exp를 적용해 원래 스케일로 변환)\n",
    "log_Y_pred = model.predict(X_const)\n",
    "Y_pred = np.exp(log_Y_pred) # exp(지수 함수) 적용해서 로그 변환 되기 전의 Y로\n",
    "\n",
    "# 데이터 시각화\n",
    "plt.scatter(X, Y, label=\"Actual Data\", color=\"blue\")\n",
    "plt.plot(X, Y_pred, label=\"Exponential Fit\", color=\"red\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.title(\"Exponential Regression\")\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"./img/img9.png\">\n",
    "\n",
    "* 파란 점: 실제 데이터\n",
    "* 빨간 곡선: 모델이 예측한 회귀선\n",
    "\n",
    "### ☑️ 지수 회귀모델이 적합한 상황\n",
    "* 값이 시간에 따라 **지수적으로 증가하거나 감소**하는 경우 적함\n",
    "* 기술 발전, 투자 수익률 증가 등\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d314eaf",
   "metadata": {},
   "source": [
    "## 2-3. 로그 회귀모델(Log regression model)\n",
    "<img src=\"./img/img10.png\">\n",
    "\n",
    "* 로그 회귀모델은 종속 변수와 독립 변수 간의 관계가 로그 함수를 통해 더 잘 표현될 때 사용할 수 있음.\n",
    "\n",
    "### ☑️ 사용 방법\n",
    "* 로그함수 형태의 데이터는 $Y=a+b\\log (X)$의 수식을 따름. <br>\n",
    "    이는 $Y=a+bX$의 선형식에서 $X$의 자리에 $\\log (X)$가 들어간 것임.\n",
    "1. 독립 변수 $X$에 로그 함수를 적용\n",
    "2. 선형 회귀를 적용\n",
    "* ➡️ 이렇게 예측된 값은 $Y$이므로 **예측값에 별도의 처리를 하지 않고 그대로** 사용할 수 있음. <br>\n",
    "    (지수 회귀모델에서는 예측된 값이 $Y'$이었기 때문에 $Y'$에 지수함수 연산을 수행했었음)\n",
    "\n",
    "### ☑️ 코드\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 로그함수 형태의 임의의 데이터 생성 \n",
    "np.random.seed(42)\n",
    "X = np.linspace(1, 10, 20).reshape(-1, 1)\n",
    "Y = 5 + 3 * np.log(X).flatten() + np.random.normal(0, 0.5, X.shape[0]) \n",
    "\n",
    "# 독립 변수 X에 자연 로그 변환\n",
    "log_X = np.log(X)\n",
    "\n",
    "# 회귀 모델 학습 (Y = a + b * log(X))\n",
    "log_X_const = sm.add_constant(log_X) \n",
    "model = sm.OLS(Y, log_X_const).fit()\n",
    "\n",
    "# 예측값 계산\n",
    "Y_pred = model.predict(log_X_const)\n",
    "\n",
    "# 데이터 시각화\n",
    "plt.scatter(X, Y, label=\"Actual Data\", color=\"blue\")\n",
    "plt.plot(X, Y_pred, label=\"Logarithmic Fit\", color=\"red\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Logarithmic Regression\")\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"./img/img11.png\">\n",
    "\n",
    "* 파란 점: 실제 데이터\n",
    "* 빨간 곡선: 모델이 예측한 회귀선\n",
    "\n",
    "### ☑️ 로그 회귀 모델이 적합한 상황\n",
    "* 데이터가 **초기에 빠르게 증가**하고 **점차 증가율이 줄어드는 패턴**을 보일 때 효과적\n",
    "* 인구 성장, 감염병 확산 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ebce8",
   "metadata": {},
   "source": [
    "## 2-4. 스플라인 회귀(Spline Regression)\n",
    "<img src=\"./img/img12.png\">\n",
    "\n",
    "\n",
    "* Spline이란, 매듭점 사이를 충분히 매끄럽게 연결해주는 함수임.\n",
    "* 스플라인 회귀는 데이터를 구간별로 나누고, 각 구간에서 서로 다른 선형 또는 비선형 함수를 적용하여 예측하는 방식임.\n",
    "\n",
    "### ☑️ 사용 방법\n",
    "* 전체 데이터 범위를 여러 구간으로 나누고, 각 구간에 대해 회귀 모델(선형 또는 다항 등)을 적용\n",
    "* 구간 경계에서의 **연속성을 유지**하는 것이 중요함\n",
    "> $Y = \\begin{cases} a_1 + b_1X & \\text{if } X < c_1 \\\\a_2 + b_2X & \\text{if } c_1 \\leq X < c_2 \\\\\\cdots \\\\a_n + b_nX & \\text{if } X \\geq c_{n-1}\\end{cases}$\n",
    "\n",
    "### ☑️ 코드\n",
    "```python\n",
    "# 스플라인 회귀 모델에 대한 간략한 버전의 코드입니다. \n",
    "# 전체 구간에 대해 한 종류의 회귀 모델만 사용 가능합니다 (선형 또는 다항)\n",
    "# 첫 번째 구간에는 선형 모델을 사용, 두 번째 구간에는 다항 모델을 사용하는 것처럼\n",
    "# 구간별로 다른 모델을 사용하기 위해서는 반복문을 활용한 코드를 사용해야 합니다. \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from patsy import dmatrix\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 랜덤 데이터 생성\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100)\n",
    "Y = np.sin(X) + np.random.normal(0, 0.2, size=X.shape)\n",
    "\n",
    "# X의 데이터에 스플라인 함수를 적용하여 새로운 데이터 X_spline을 만들고, \n",
    "# 이 새로운 데이터로 선형 회귀를 진행하는 방식\n",
    "\n",
    "X_spline = dmatrix(\"bs(X, df=5, degree=3, include_intercept=False)\", \n",
    "{\"X\": X}, return_type='dataframe')\n",
    "# X: 데이터\n",
    "# df: 나눌 구간의 수+1 (df=5라면 4개의구간으로 구분합니다)\n",
    "# degree: 사용할 회귀 모델의 차수 (degree=1이면 선형회귀, degree=2면 2차 다항 회귀)\n",
    "# include_intercept=False: 절편 제거(필요하면 True로)\n",
    "# return_type='dataframe': dataframe 형식으로 반환합니다.\n",
    "\n",
    "model_spline = sm.OLS(Y, X_spline).fit()\n",
    "Y_pred_spline = model_spline.predict(X_spline)\n",
    "\n",
    "# 시각화\n",
    "plt.scatter(X, Y, label=\"Actual Data\", color=\"blue\", alpha=0.5)\n",
    "plt.plot(X, Y_pred_spline, label=\"Cubic Spline Regression\", color=\"red\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.title(\"Cubic Spline Regression Fit\")\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"./img/img15.png\">\n",
    "\n",
    "### ☑️ 스플라인 회귀 분석이 적합한 상황\n",
    "* 데이터 패턴이 **여러 구간에서 서로 다르게** 나타날 때 유용\n",
    "* 계절에 따라 판매량이 변화하는 소매 데이터 분석 등"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
