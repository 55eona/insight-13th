{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c80b7a86",
   "metadata": {},
   "source": [
    "# 사전 과제\n",
    "* 선형회귀 (단순, 다중선형회귀)\n",
    "* 경사하강법\n",
    "* 다중공선성\n",
    "* 규제선형모델\n",
    "* 모델평가방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f80e28",
   "metadata": {},
   "source": [
    "# 1. 머신러닝과 모델링\n",
    "## 1-1. 머신러닝\n",
    "* **인공지능(AI, Artificial Intelligence)**\n",
    "    * **머신러닝(ML, Meachine Learning)**\n",
    "        * 새로운 데이터를 예측하거나, 결정을 내릴 수 있도록 하는 기술. \n",
    "        * 데이터에서 **패턴을 찾아내고**, 이 패턴을 바탕으로 **새로운 데이터에 대한 예측**을 가능하게 함.\n",
    "        * **지도학습**(정답지 있는 데이터로 학습)\n",
    "            * 회귀(regression)\n",
    "            * 분류(classification)\n",
    "        * **비지도학습**(정답지 없는 데이터로 학습하는 방식)\n",
    "            * 군집화/클러스터링(clustering)\n",
    "        * **강화학습**(행동에 대한 보상을 통해 최적 방법을 학습하는 방식)\n",
    "    * **딥러닝(DL, Deep Learning)**\n",
    "        * 머신러닝과 달리 인간이 feature selection을 하지 않고, 기계가 하는 방식 (머신러닝에 속해 있는 범주로 보긴 함)\n",
    "        * 신경망(Neural Network) 모델을 통해, 인간이 직접 파악하기 어려운 수많은 feature을 기계가 수행해줌으로써 비정형 데이터 처리에 유리함\n",
    "## 1-2. 모델링\n",
    "<img src=\"./img/img2.webp\" width=\"500px\"><br>\n",
    "\n",
    "* **훈련 데이터(train set)**: 모델을 학습(훈련)시킬 때 쓰는 데이터\n",
    "* **테스트 데이터(test set)**: 학습된 모델을 검증할 때 쓰는 데이터\n",
    "<br>\n",
    "\n",
    "* ❓ **전체 data를 훈련 데이터와 테스트 데이터로 쪼개는 이유?**\n",
    "    * 학습에 쓰인 데이터(이미 모델이 아는 데이터)가 아니라, 학습에 쓰이지 않은 데이터(모델이 모르는 데이터)를 통해 테스트함으로써<br>새로운 데이터에 대해 얼마나 예측을 잘 수행하는지 **공정하게 평가**하기 위해 데이터를 분리해야 함.\n",
    "    * **모델링의 핵심**: 데이터로부터 **패턴을 찾아**내어, **새로운 데이터를 예측**하고자 함\n",
    "    * 만약 훈련 데이터와 테스트 데이터를 분리하지 않고, 이미 훈련 데이터로 쓰인 데이터를 테스트 데이터로도 쓴다면, <br>이미 학습한 내용에 대한 결과만을 뱉어내는 것일 수 있기에 실제 미래 예측 성능을 제대로 평가할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445539c0",
   "metadata": {},
   "source": [
    "# 2. 회귀(Regression)와 모델링\n",
    "## 2-1. 회귀(Regression)란?\n",
    "* **회귀**\n",
    "    * 데이터에서 **패턴**을 찾아내어 **미래 값을 예측**하는 것\n",
    "    * 통계, 머신러닝의 주요 개념임\n",
    "    * **연속적인 숫자**를 다룸\n",
    "        * ex) 집값 예측, 주식 가격 예측 등\n",
    "        * ❓ 목표 변수가 불연속적일 땐(범주형) **분류**를 이용함! 즉, **목표 변수의 연속성/불연속성**에 따라 **회귀/분류**로 나누어짐!\n",
    "## 2-2. 회귀의 종류\n",
    "* 회귀 기법에는 다양한 종류가 있음!\n",
    "    * 선형 회귀\n",
    "    * 비선형 회귀\n",
    "    * 릿지 회귀\n",
    "    * 라쏘 회귀\n",
    "    * 다항 회귀\n",
    "    * ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d5ff1",
   "metadata": {},
   "source": [
    "## 2-3. 선형 회귀 ⭐️\n",
    "* **선형 회귀**는 **데이터를 가장 잘 설명하는 회귀선(ex. 직선 y=ax+b)을 찾아 예측하는 과정!**   \n",
    "    <img src=\"./img/img4.webp\" height=\"300px\" width=\"300px\">\n",
    "    \n",
    "    * 기존에 관측된 데이터 포인트를 기준으로, **그 데이터를 가장 잘 설명하는 선**을 찾아 **미래를 예측**함\n",
    "    * 이러한 회귀선은 **현재 데이터를 요약**하는 동시에, 아직 관측되지 않은 **미래 값을 예측하는 도구**가 됨\n",
    "<br>\n",
    "    \n",
    "* **변수**\n",
    "    * **독립 변수(x)**: 예측을 위해 사용하는 변수\n",
    "    * **종속 변수(y)**: 예측하고자 하는 변수\n",
    "    * ex) 시간에 따른 주가를 예측하고 싶다면?\n",
    "        * 독립변수(x): 시간, 종속변수(y): 주가\n",
    "<br><br>\n",
    "        \n",
    "* **선형 회귀의 종류**\n",
    "    * 1️⃣ **단순 선형 회귀**: 데이터를 직선 형태(하나의 독립 변수)로 표현하는 경우\n",
    "    * 2️⃣ **다중 선형 회귀**: 데이터를 두 개 이상의 독립변수를 사용하여 표현하는 경우\n",
    "    \n",
    "    \n",
    "### 2-3-1️⃣. 단순선형회귀분석\n",
    "<img src=\"./img/img5.webp\" height=\"300px\" width=\"300px\">\n",
    "\n",
    "* 🔍 **단순**의 의미\n",
    "    * **하나의 독립변수**만 존재하는 선형 회귀\n",
    "    * 데이터를 가장 잘 설명하는 직선 $y=aX+b$을 찾는 과정에 해당\n",
    "    * $y=ax+b$에서 \n",
    "        * $a$: **회귀계수(기울기)** 를 의미\n",
    "        * $b$: **y절편**을 의미\n",
    "    * 적절한 **회귀계수($a$)와 절편($b$)** 를 찾는 과정\n",
    "<br><br>\n",
    "\n",
    "* 🔍 어떻게 적절한 **회귀계수($a$)와 절편($b$)** 를 찾을 수 있을까?\n",
    "    * **잔차(residual)** 를 정의해나가기\n",
    "        * 잔차와 오차의 차이?\n",
    "            * 잔차(residual): 모델이 설명하지 못한 실제 - 예측값 간의 차이\n",
    "            * 오차(error): 모델이 설명하지 못한 실제 - 실제 평균값 간의 차이 \n",
    "        * 📌 **최소제곱법(Least Squares Method)**\n",
    "            * 최소자승법(Ordinary Least Square)이라고도 부름\n",
    "            * 최적의 회귀선을 찾는 방식\n",
    "            <img src=\"./img/img10.webp\" height=\"250px\" width=\"300px\">\n",
    "            * **잔차의 제곱의 합(Sum of Squared Residuals, RSS)이 최소**가 되는 지점으로 **최적의 회귀선**을 구하는 방식\n",
    "            * **잔차**: 실제과 예측값의 차이\n",
    "            * **핵심: 회귀선과 실제 데이터와의 차이를 최소화**하기 위한 시도임. \n",
    "            * *최소제곱법을 통해 구한 함수가 복잡*해서 최소값을 구하기 어려운 경우?<br>\n",
    "                * => 주로 **경사하강법** 사용 (아래는 경사하강법을 사용하는 4가지 경우)\n",
    "                    1. 함수가 닫힌 상태인 경우\n",
    "                    2. 함수가 너무 복잡해 미분계수를 구하기 어려운 경우\n",
    "                    3. 경사하강법을 구현하는 게 미분계수를 구하는 것보다 더 쉬운 경우\n",
    "                    4. 데이터 양이 너무 많은 경우 (효율적 계산을 위해)\n",
    "               \n",
    "#### ☑️ 경사하강법(Gradient Descent) ⭐️\n",
    "* 경사하강법의 목적\n",
    "    * 최소제곱법과 마찬가지로, \n",
    "        * $y=aX+b$에서 $a$와 $b$를 잘 찾기 위함\n",
    "        * 잔차제곱을 줄이려는 목적은 동일함\n",
    "* 계산 과정이 복잡할 때, 최소로 가는 과정에서 경사하강법을 사용함\n",
    "\n",
    "##### (0) 목적함수, 비용함수, 손실함수\n",
    "* **`목적함수(Objective Function)`**\n",
    "    * ***우리가 달성하고 싶은 목표***\n",
    "        * 최적화(Optimization)를 위해 오차를 최소화하는 것이 목적인 함수\n",
    "        * 즉, 최적화하려는 대상을 수학적으로 표현한 함수\n",
    "            * ex) 시험점수를 최대화\n",
    "            * ex) 실수를 최소화\n",
    "            * -> case별로 어떤 방향으로 최적화하는지 달라짐\n",
    "    * 머신러닝에서) 모델의 예측이 얼마나 잘 맞는지 평가하는 기준\n",
    "<br><br>\n",
    "* **`손실함수(Loss Function)`**\n",
    "    * ***개별 데이터 샘플에 대한 오차를 측정하는 함수***\n",
    "        * 한 개의 데이터 포인트에 대해 모델이 얼마나 틀렸는지를 평가\n",
    "        * 예측값 $\\hat{y}$와 실제값 $y$의 차이를 계산\n",
    "            * ex)\n",
    "                * 회귀 분석: 평균제곱오차(MSE), 평균절대오차(MAE) <br>\n",
    "                  $(MSE\\,for\\,one\\,sample) = (y-\\hat{y})^2$\n",
    "                * 분류 문제: 교차 엔트로피(Cross-Entropy Loss)\n",
    "<br><br>\n",
    "* **`비용함수(Cost Function)`**\n",
    "    * ***전체 데이터셋에서 평균적인 손실을 측정하는 함수***\n",
    "        * 손실함수를 모든 데이터 포인트에 대해 계산한 수 **평균 or 합산**한 값.\n",
    "        * 머신러닝에서는 대부분 모델을 훈련할 때 **비용함수를 최적화(최소화)하는 것**이 목표임.\n",
    "            * ex)\n",
    "                * 평균제곱오차(MSE)를 비용함수로 사용할 경우 <br>\n",
    "                $(MSE) = \\dfrac{1}{N}\\sum_{i=1}^N(y_i-\\hat{y_i})^2$\n",
    "                    * $N$은 전체 샘플 수\n",
    "         * 모델을 학습할 때 **대부분은 비용함수를 최소화하는 것이 목표!**\n",
    "         <br> 이때는 **비용함수 = 목적함수**로 봐도 무방함\n",
    "         <br> (`비용함수 최소화` = `목적함수가 원하는 최적화의 방향`)\n",
    "<br><br>\n",
    "* 머신러닝 과정에서는 **모델을 평가하고 최적화하는 함수**를 찾는 게 핵심임!\n",
    "* 이런 의미에서 엄밀하게는 **손실함수와 비용함수**는 **개별, 합계**로서 차이가 있으나 **혼용**하는 경우가 많음\n",
    "\n",
    "\n",
    "##### (1) 손실함수\n",
    "* 손실함수와 경사하강법 톺아보기\n",
    "    <img src=\"./img/img11.webp\" height=\"250px\" width=\"350px\"><br>\n",
    "    * $t_i$: $i$번째 데이터포인트 실제값\n",
    "    * $y_i$: $i$번째 예측값\n",
    "    * 즉, 잔차($t_i-y_i$)를 제곱한 뒤 합하는 형태 (그러고 평균 내기)\n",
    "    * 손실함수 수식 (y식 대입: $y=Wx+b$)\n",
    "    <img src=\"./img/img12.webp\" width=\"350px\"><br>\n",
    "\n",
    "        * 목표: E를 최소화하는 W(weight)와 b(bias)를 찾아내는 것\n",
    "* 아래로 볼록한 곡선의 그래프에서, 임의의 W값에서 E값을 살펴볼 때 (그래프 위에 한 점에서)\n",
    "    * 미분계수가 양수라면 (기울기가 양수라면)\n",
    "        * W값이 현재 점보다는 왼쪽으로 가야 E가 최소가 되는 지점으로 도달할 수 있음\n",
    "        * 즉 `현재 값 - 미분값(이때 미분값은 양수)`를 하면 현재값보다 왼쪽으로 점이 이동함\n",
    "    * 미분계수가 음수라면 (기울기가 음수라면)\n",
    "        * W값이 현재 점보다는 오른쪽으로 가야 E가 최소가 되는 지점으로 도달할 수 있음\n",
    "        * 즉 `현재 값 - 미분값(이때 미분값은 음수)`를 하면 현재값보다 오른쪽으로 점이 이동함\n",
    "    * 이러다가 최솟값에 도달하면 **미분값이 0**이 되어서 `현재 값 - 0`으로 더이상 변화하지 않고 멈추게 됨\n",
    "* 수식을 정리하면\n",
    "    <img src=\"./img/img18.webp\" width=\"500px\"><br>\n",
    "\n",
    "##### (2) 학습률 ($\\alpha$: learning rate)\n",
    "* 이동하는 방향은 ㅇㅋ\n",
    "* 이동하는 크기는?? -> 학습률(Learning Rate) (Step Size 라고도 함)\n",
    "<img src=\"./img/img21.webp\" width=\"500px\"><br>\n",
    "* 학습률의 크기는 어때야 할까?\n",
    "    * 너무 크면: 모델이 수렴하지 못하고 발산함 (부호가 자꾸 바뀜)\n",
    "    * 너무 작으면: 모델이 수렴할 때까지 너무 오랜 시간이 걸림\n",
    "* 학습률의 크기는 개발자가 직접 정하는 hyperparameter\n",
    "    * 실험을 통해 조정할 수 있음\n",
    "    * Learning Rate Scheduler, Grid Search, Random Search를 통해 자동으로 학습률을 찾을 수도 있음\n",
    "\n",
    "\n",
    "##### (3) Local Minima 문제\n",
    "<img src = \"./img/img22.webp\" width=\"500px\"><br>\n",
    "* 우리가 원하는 Global Minima가 아니라 Local Minima에 매몰되어서, 최소 지점을 제대로 찾지 못하는 문제가 발생할 수 있음\n",
    "\n",
    "* **해결법: 모멘텀(Momentum)**\n",
    "    * 기존 경사하강법은 이전 기울기는 다 잊어버림;; (현재 기울기만을 기준으로 움직임) 그래서 local minima에 빠져버리는 문제가 생김\n",
    "    * 모멘텀 -> 관성을 가진 공처럼, 이전의 기울기와 이동해오던 방향을 참고해서 작은 언덕도 넘을 수 있게 함\n",
    "    * 가속도 개념이랑 비슷한 듯\n",
    "    \n",
    "    \n",
    "\n",
    "### 2-3-2️⃣. 다중선형회귀분석\n",
    "* 🔍 **다중**의 의미\n",
    "    * $y=W_1x_1+W_2x_2+...+W_nx_n+b$\n",
    "    <br>\n",
    "    <img src = \"./img/img24.webp\" width=\"300px\"><br>\n",
    "    \n",
    "    * **독립 변수가 2개 이상**인 경우의 선형 회귀\n",
    "    * 독립 변수 x_i가 2개 이상인 경우 3차원 공간에 표현됨. **회귀선은 평면**이 됨.\n",
    "    * 단순선형회귀분석과 마찬가지로 **최소제곱법**사용 가능\n",
    "        * 파악이 어려운 경우 **경사하강법** 사용해야 하는 것도 마찬가지임\n",
    "    * 변수가 2개보다 많은 경우는 비슷한 느낌으로 확장됨\n",
    "#### ☑️ 다중공선성(Multicollinearity) ⭐️\n",
    "* 🔍 **다중공선성**\n",
    "    * 회귀 분석에서 **독립 변수들 간의 상관관계가 큰 경우** 발생함\n",
    "    * 다중공선성이 큰 경우\n",
    "        * 어떤 독립 변수가 종속변수에 얼마나 영향을 미치는지 정확하게 구분이 어려움\n",
    "        * 회귀 모델이 어떤 변수의 영향을 반영해야 할지 불확실해짐\n",
    "        * 회귀 분석의 정확도가 낮아짐\n",
    "* 🔍 **다중공선성 확인 방법**\n",
    "    1. 상관계수\n",
    "        * 상관계수의 절댓값이 1에 가까울수록 두 변수의 상관성이 높음.\n",
    "        * 상관계수가 0인 경우 선형 상관관계가 없음.\n",
    "        * `heatmap`, `corr()` 등을 통해 상관계수를 확인할 수 있음.\n",
    "        * `pairplot`을 통해 산점도 시각화를 하여 상관성을 확인할 수 있음.\n",
    "    2. VIF지수 (분산 팽창 인수)\n",
    "        * 회귀 모델의 결정계수 $R^2$를 사용하여 계산됨\n",
    "        * VIF가 높으면 다중공선성이 존재한다고 판단 (아래 기준은 절대적인 것은 아니며 일반적인 가이드라인임)\n",
    "            * VIF = 1: 해당 독립변수는 다른 변수와 상관관계가 전혀 없음을 의미\n",
    "            * VIF < 5: 일반적으로 다중공선성 문제가 없다고 간주됨\n",
    "            * VIF > 5: 다중공선성의 징후가 있을 수 있으며, 주의해야 함\n",
    "            * VIF > 10: 다중공선성이 심각하다고 간주됨. 이 변수를 포함한 회귀 모델은 불안정할 가능성이 큼. 이 경우 해당 변수를 제거하거나, 변수 간의 상관관계를 줄이기 위한 조치가 필요할 수 있음. \n",
    "* 🔍 **다중공선성 대처 방법**\n",
    "    1. 변수 제거 (변수 선택법)\n",
    "        * 독립변수로서 사용할 변수를 선택하는 방법\n",
    "    2. 변수 변환\n",
    "        * 변수들을 더하거나 빼서 새로운 변수를 생성\n",
    "            * 독립변수를 더하거나 빼더라도 문제가 없는 경우\n",
    "    3. 규제 선형 모델 활용\n",
    "        * 릿지, 라쏘, 엘라스틱넷 등의 방법을 통해 모델의 복잡도를 줄이는 방법 사용 (아래에 나옴)\n",
    "    4. PCA (주성분분석)\n",
    "        * 데이터의 차원을 축소하는 데 사용되는 통계적 기법\n",
    "        * 고차원 데이터에서 중요한 정보를 최대한 보존하면서 데이터의 복잡성을 줄이는 것을 목표로 함\n",
    "            * ex) dataset에 별로 영향을 주지 않는 x2차원을 없애기\n",
    "        * 상관 있는 변수들을 묶어서 **더 적은 수의 대표 축으로 바꾸는 것**이 PCA의 핵심 아이디어\n",
    "        * PCA 과정\n",
    "            <img src=\"./img/img26.webp\" width=\"450px\"><br>\n",
    "            \n",
    "            1. 데이터 전처리(정규화)\n",
    "                2. 평균을 0으로 조정 -> 데이터 중심을 원점으로 이동\n",
    "                3. 표준화 (스케일링) -> 분산을 1로 맞춰 단위 영향 제거\n",
    "            2. 주성분 찾기\n",
    "                4. 공분산 행렬 계산 & 고유벡터, 고유값 추출 <br>\n",
    "                    고유벡터 -> 데이터가 가장 많이 퍼진 방향(주성분 축)\n",
    "            3. 차원 축소\n",
    "                5. 주성분 축에 데이터 투영 -> 중요한 정보만 유지\n",
    "            4. 복원 가능 (선택적)\n",
    "                6. 원래 좌표계로 변환 가능 (표준화 해제)\n",
    "                    \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c8d85",
   "metadata": {},
   "source": [
    "## 2-4. 규제선형모델 ⭐️\n",
    "<img src=\"./img/img27.webp\" width=\"500px\">\n",
    "<br>\n",
    "\n",
    "1. underfitting\n",
    "2. good fitting -> 굿굿\n",
    "3. overfitting\n",
    "    * 과적합 -> 학습 데이터에는 완벽히 맞지만, 새로운 데이터에서는 성능이 크게 떨어짐\n",
    "\n",
    "🔍 **규제 선형 모델**\n",
    "* 모델이 과적합되지 않도록 규제를 가하고자 등장한 모델<br>\n",
    "    -> 선형 회귀 모델: 특성에 곱해지는 계수(기울기)의 크기를 조정하는 것\n",
    "* 특히 여러 독립 변수를 사용하여 예측하여 모델이 복잡해지는 **다중회귀**에서 **과적합될 가능성**이 증가함\n",
    "* 기존 성형 모델에서는 RSS(잔차 제곱합)를 최소화(최소제곱법)하는 방식으로 비용함수를 설정했음<br>\n",
    "    -> 회귀 계수가 쉽게 커지게 되어(특정 독립 변수에 과하게 의존하게 되어) 과적합이 발생함\n",
    "\n",
    "* 비용함수는\n",
    "    1. 학습 데이터의 잔차 오류값을 최소화하는 RSS 최소화 방법(최소제곱법)과\n",
    "    2. 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록 하는 방법 <br>\n",
    "    이 서로 균형을 유지해야 함\n",
    "* -> 오류는 최대한 줄이면서도, 특정 독립 변수에 집착하지 않도록 균형을 맞춰 과적합 방지!\n",
    "\n",
    "🔍 **규제 선형 모델의 종류**\n",
    "1. `릿지 회귀`: L2 규제: W(회귀계수)의 제곱에 대해 패널티를 부여하는 방식\n",
    "2. `라쏘 회귀`: L1 규제: W(회귀계수)의 절댓값에 대해 패널티를 부여하는 방식\n",
    "3. `엘라스틱넷 회귀`: L2 규제 + L1 규제 결합\n",
    "    \n",
    "### 2-4-1️⃣. 릿지 회귀 (L2 규제)\n",
    "* **L2 규제**: 기둥 크기에 따라 다른 힘으로 누르는 망치 🔨  (강강약약 ㄷㄷㅋㅋ)\n",
    "    * 큰 기둥은 세게 누름\n",
    "    * 작은 기둥은 살살 누름\n",
    "    * 비율이 균등하지 않을 수 있지만 **전체적으로 작아짐**\n",
    "    * 회귀계수($w_j$)의 제곱을 패널티로 주기 때문에, 큰 회귀계수일수록 훨씬 강한 패널티를 받음\n",
    "    * 모든 기둥(모든 계수)의 크기는 작아지지만, 완전히 사라지지는 않음(회귀 계수가 0이 되지는 않음) -> 변수 제거 X\n",
    "* **계수의 크기 조절 기능**\n",
    "    * 모든 변수를 유지하면서도, 크기를 적절히 줄여 과적합을 방지!\n",
    "* **언제 사용?**\n",
    "    * 예측 변수가 많을 때 <br>\n",
    "        -> 고차원 데이터에서 모든 변수에 작은 가중치를 부여하고 할 때 유용\n",
    "    * 다중공선성 존재할 때 <br>\n",
    "        -> 변수 선택이 필요하지 않은 상황\n",
    "    \n",
    "### 2-4-2️⃣. 라쏘 회귀 (L1 규제)\n",
    "* **L1 규제**: 모든 기둥을 같은 힘으로 누르는 망치 🔨\n",
    "    * 작은 기둥들은 힘을 버티지 못하고 완전히 사라질 수도 있음 😵\n",
    "    * 회귀계수($w_j$)의 절댓값을 패널티로 주기 때문에, 회귀계수의 크기와 상관없이 같은 패널티를 받음\n",
    "    * 작은 기둥(작은 계수)는 힘을 이기지 못해 0이 되기도 함 -> 일부 기둥이 완전히 사라짐(변수 선택 기능)\n",
    "* **변수 선택 기능**\n",
    "    * 비중이 낮은 변수(작은 계수)는 아예 사라지고, 중요한 변수만 남음!\n",
    "* **언제 사용?**\n",
    "    * 예측 변수의 수가 많고, 그 중 일부만이 실제로 중요할 때<br>\n",
    "        -> 계수 중 일부를 정확히 0으로 만들어 변수 선택 효과 부여\n",
    "    * 모델의 해석을 간단하게 유지하고자 할 때<br>\n",
    "        -> 불필요한 변수를 제거하고 모델을 단순화\n",
    "        \n",
    "    \n",
    "### 2-4-3️⃣. 엘라스틱넷 (L1 + L2)\n",
    "* **엘라스틱넷(Elastic Net)**: L1, L2 망치를 동시에 사용 ㄷㄷ\n",
    "    * L1 망치: 작은 기둥 아예 부숴서 불필요한 변수 제거\n",
    "    * L2 망치: 남은 기둥을 균형 잡히게 눌러서 크기를 조절\n",
    "* **두 망치 함께 사용하면?**\n",
    "    * 불필요한 변수 제거\n",
    "    * 남은 변수들은 적절한 크기로 유지\n",
    "    * L1의 변수 선택 기능(일부 계수를 0으로 만듦)과 L2의 규제 기능(큰 계수를 줄임)을 동시에 활용하는 모델!\n",
    "    * L1 -> 상관관계가 높은 변수 중 일부만 선택하는 경향이 있음\n",
    "        * -> L2 함께 적용시 상관관계가 높은 변수를 함께 고려하면서도 최적의 규제 가능!\n",
    "    * -> **안정적인 예측(모든 변수가 균형 있게 줄어듦)** 과 **희소성(변수 제거)** 을 동시에 달성 가능!\n",
    "* **언제 사용?**\n",
    "    * 예측 변수의 수가 많고, 그중 중요한 변수를 선택하면서도 다중공선성을 관리할 때<br>\n",
    "        -> 변수 간 상관관계가 높은 경우 릿지(L2)만 사용할 때보다 더 나은 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c39b7",
   "metadata": {},
   "source": [
    "## 2-5. 모델 평가방법 ⭐️\n",
    "* 내가 만든 회귀 모델이 좋은 성능을 내고 있는지 어떻게 평가하지?\n",
    "* 내가 만든 회귀 모델이 올바르게 만든 회귀 모델인지 어떻게 평가하지?\n",
    "\n",
    "### 2-5-1️⃣. 성능 평가 지표\n",
    "#### ☑️ 평균 제곱 오차 (MSE, Mean Square Error)\n",
    "* 회귀 분석에서 **모델의 예측값**과 **실제 관측값** 사이의 **오차 제곱의 평균**을 의미\n",
    "* 모델의 **예측 성능을 평가하는 지표** 중 하나\n",
    "* $MSE=\\dfrac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y_i})^2$\n",
    "#### ☑️ 평균 절대 오차 (MAE, Mean Absolute Error)\n",
    "* 회귀 분석에서 **모델의 예측값**과 **실제 관측값** 사이의 **절대값 오차의 평균**을 의미\n",
    "* 모델의 **예측 성능을 평가하는 지표** 중 하나\n",
    "* $MAE=\\dfrac{1}{n}\\sum_{i=1}^n |y_i-\\hat{y_i}|$\n",
    "#### ❓ MSE와 MAE의 차이?\n",
    "* MSE\n",
    "    * 큰 오차에 대해 민감하게 반응함(제곱)\n",
    "    * 이상치에 민감하게 반응함\n",
    "* MAE \n",
    "    * 모든 오차를 동일하게 취급함\n",
    "    * 이상치가 존재하는 경우에도 전반적인 예측 성능을 유지할 수 있음\n",
    "* 모델의 특성, 데이터의 특성, 평가 목적에 따라 적절히 선택하면 됨\n",
    "\n",
    "### 2-5-2️⃣. 변수 유의성 평가\n",
    "#### ☑️ t 검정\n",
    "**1. 개념**\n",
    "* t 검정은 **독립 변수의 회귀계수가 유의미한지 검정**하는 데 사용됨.\n",
    "    * **귀무가설($H_0$)**: 회귀계수가 0이다 -> 독립변수가 종속변수에 영향을 주지 않는다.\n",
    "    * **대립가설($H_1$)**: 회귀계수가 0이 아니다 -> 독립변수가 종속변수에 영향을 준다.<br><br>\n",
    "    \n",
    "**2. 검정 과정** <br><br>\n",
    "**2-(1) t값 계산** <br>\n",
    "$t=\\dfrac{\\hat{\\beta}-0}{SE(\\hat{\\beta})}$ <br>\n",
    "* $\\hat{\\beta}$: 추정된 회귀계수\n",
    "* $SE(\\hat{\\beta})$: 표준오차\n",
    "* t-분포를 따름.\n",
    "\n",
    "**2-(2) p-value 확인 및 판단** <br>\n",
    "* `p-value < 0.05`: 유의 수준 5%에서 $H_0$ 기각 -> 해당 변수는 유의미하다.\n",
    "* `p-value >= 0.05`: $H_0$ 기각 불가 -> 해당 변수는 유의미하지 않다.\n",
    "\n",
    "**3. 해석** <br>\n",
    "* 회귀계수가 0이면 해당 독립변수는 의미 없음\n",
    "* p-value가 작을수록 해당 변수는 종속변수에 더 큰 영향을 준다고 볼 수 있음.\n",
    "* t-값이 크면 귀무가설을 기각할 가능성이 높아짐."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
