{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b76c624-b1b6-442d-81a6-6db2c0302f16",
   "metadata": {},
   "source": [
    "## 머신러닝\n",
    "머신러닝이란 새로운 데이터를 예측하거나 결정을 내릴 수 있도록 하는 기술이다.\n",
    "목적: 데이터에서 패턴을 찾아내고, 패턴을 바탕으로 새로운 데이터에 대한 예측을 가능하게 하는 것\n",
    "\n",
    "비지도학습: 정답이 없는 데이터로 학습하는 방식\n",
    "지도학습: 정답이 있는 데이터로 학습하는 방식\n",
    "\n",
    "## 모델링\n",
    "머신러닝 모델을 생성하는 과정\n",
    "전체데이터 -> 훈련데이터 -> 모델                                                                                               -> 최종 모델\n",
    "           -> 테스트 데이터 (데이터의 분리) -> 성능평가(훈련 데이터로부터 만든 모델을 테스트 데이터로 반복 성능 평가) -> 모델   -> 최종 모델\n",
    "\n",
    "훈련 데이터: 모델을 학습시킬 때 쓰이는 데이터\n",
    "테스트 데이터: 학습된 모델을 \"검증\"하기 위해 사용하는 데이터 (unseen data)\n",
    "\n",
    "** 전체 데이터를 훈련 데이터와 테스트 데이터로 쪼개는 이유?\n",
    "훈련 데이터를 통해 학습시킨 모델이 unseen data에 대해 얼마나 예측을 잘 수행하는지 \"공정하게\" 평가하기 위하여 데이터를 분리함\n",
    "즉, 데이터로부터 패턴을 찾고, 그 패턴에서 \"새로운 데이터에 대한 예측\"을 하는 것이 모델링의 목적이기 때문에, 데이터를 분리하여 새로운 데이터를 통해 검증하는 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e9c94-151f-4529-9faa-fed5ec0b9e03",
   "metadata": {},
   "source": [
    "# 회귀와 모델링\n",
    "\n",
    "회귀: 데이터에서 패턴을 찾아내어 미래의 값을 예측하는 것\n",
    "- 주로 연속적인 숫자를 다룸\n",
    "- 연속적이지 않은 숫자를 다룰 때는 \"분류\"를 이용한다.\n",
    "\n",
    "회귀의 종류\n",
    "- 선형회귀, 비선형 회귀, 릿지회귀, 라쏘회귀, 다항회귀 등.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889bdfea-8a25-4b22-aef1-c52099be4de7",
   "metadata": {},
   "source": [
    "## 선형회귀\n",
    "\n",
    "선형회귀: 데이터를 가장 잘 설명하는 회귀선 (y=ax+b)을 찾아 예측하는 과정\n",
    "예측하고자 하는 것을 종속 변수(y), 예측을 위해 사용하는 변수를 독립 변수(x)라고 한다.\n",
    "(ex. 연령에 따른 몸무게를 예측할 때 -> 종속 변수(y)는 몸무게, 독립 변수(x)는 연령)\n",
    "\n",
    "** 선형이 꼭 직선을 의미하는 것은 아님!\n",
    "\"모델이 파라미터(계수)에 의해 선형\"이라는 의미이다.\n",
    "따라서, 직선도 선형이라고 볼 수 있지만, y=c+bx+ax^2도 곡선이지만 계수에 대해서 선형이다.\n",
    "-> x^2를 다른 변수로 변환한다면 선형 회귀로 다룰 수 있음\n",
    "\n",
    "선형 회귀를 사용할 수 있는 조건?\n",
    "1. 선형성: 종속 변수와 독립 변수들의 관계가 선형적이어야 한다.\n",
    "2. 독립성: 각각의 관측치는 서로 독립적이어야 한다. (즉, 하나의 관측치가 다른 관측치에 영향을 주면 안됨)\n",
    "3. 등분산성: 모든 독립 변수들의 수준에서 오차 항의 분산이 일정해야 한다. (음..아직 모르겠음)\n",
    "4. 정규성: 오차 항이 정규 분포를 따라야 한다.\n",
    "\n",
    "선형회귀의 종류\n",
    "1) 단순 선형 회귀: 데이터를 직선 형태 (하나의 독립 변수)로 표현하는 경우\n",
    "2) 다중 선형 회귀: 데이터를 두 개 이상의 독립 변수를 사용하여 표현하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4d6d2-93b4-45e5-aa85-008cde1094fb",
   "metadata": {},
   "source": [
    "## 1. 단순선형회귀분석\n",
    "\n",
    "y = ax+b\n",
    "하나의 독립변수만 존재하는 선형 회귀이며, y = ax+b를 찾는 과정이다.\n",
    "여기서 a는 회귀계수, b는 y절편을 의미한다.\n",
    "여기서 적절한 회귀계수와 절편을 찾아야 한다.\n",
    "\n",
    "이때, 우리가 임의로 그린 예측직선에 대해서 오차를 정의해나가면 가장 일치하는 y=ax+b 선을 찾을 수 있을 것이다.\n",
    "이를 위한 방법이 '최소제곱법'이다.\n",
    "\n",
    "# 최소제곱법\n",
    "최소제곱법: 최적의 회귀선을 찾는 방식\n",
    "y=ax+b와 각 관측값을 비교해서 직선의 y값과 관측값의 y값을 뺀 값의 제곱이 \"최소가 되게 만드는 것\"\n",
    "잔차: 관측값의 y값 - 직선의 y값\n",
    "\n",
    "그러나, 최소제곱법을 통해 구한 함수가 복잡하여 최소값을 구하기 어려우면 '경사하강법'을 사용한다.\n",
    "구체적으로,\n",
    "1) 함수가 닫힌 상태인 경우\n",
    "2) 함수가 너무 복잡해 미분 계수를 구하기 어려운 경우\n",
    "3) gradient descent를 구현하는게 미분 계수를 구하는 것보다 더 쉬운 경우\n",
    "4) 데이터 양이 너무 많은 경우 효율적으로 계산하기 위해\n",
    "'경사하강법'을 사용한다.\n",
    "목적함수, 비용함수, 손실함수\n",
    "1. 목적함수 - \"우리가 달성하고 싶은 목표\"\n",
    "- 최적화를 위해 오차를 최소화하는 것이 목적인 함수이다.\n",
    "- 머신러닝에서는 모델의 예측이 얼마나 잘 맞는지 평가하는 \"기준\"이 필요한데 (테스트 데이터를 평가할 떄?), 이때 기준이 목적함수\n",
    "\n",
    "2. 손실함수 - \"개별 데이터 샘플에 대한 오차를 측정하는 함수\"\n",
    "- 한 개의 데이터 포인트에 대해 모델이 얼마나 틀렸는지를 평가\n",
    "- 예측값 y와 실제값 y의 차이를 계산\n",
    "- (y-y')^2를 통해 회귀분석을 진행한다\n",
    "y-y' = y-(ax+b)^2가 되고, 이거에 대한 그래프는 이차함수 그래프로 나타난다.\n",
    "이때, 이차함수 그래프의 최솟값으로 움직여야 하는데, 특정 관측값의 y값은 위의 이차함수 그래프에 대한 접선의 기울기이기도 하다.\n",
    "만약 접선의 기울기가 음수라면 '현재값-음수'이므로 값이 커지게 되고, 접선의 기울기가 양수라면 '현재값-음수'가 값이 작아지게 된다.\n",
    "그러다 최솟값에 도달하면 '현재값-0'이 되어서 더이상 변화하지 않고 멈추게 되고, 결론적으로 최솟값에 도달하게 하는 것이 목적이다.\n",
    "\n",
    "또한, 학습률(learning rate - 알파로 표기)은 얼마나 왼쪽 혹은 오른쪽으로 이동할지를 결정해준다.\n",
    "- 만약 학습률이 너무 작다면 모델이 수렴할 때까지 오랜 시간이 걸린다.\n",
    "- 그러나 학습률이 너무 크다면 수렴하지 못하고 발산하게 된다.\n",
    "- 학습률은 개발자가 직접 설정하는 하이퍼파라미터로 초기에는 작은 값부터 (0.01, 0.001, 0.0001같은 값) 시도해보고, 학습이 너무 느리면 학습률을 키우고, 학습이 발산하면 학습률을 줄이며 실험한다.\n",
    "\n",
    "3. 비용함수 - \"전체 데이터셋에서 평균적인 손실을 측정하는 함수\"\n",
    "- 손실함수를 모든 데이터 포인트에 대해 계산한 후 평균 or 합산한 값\n",
    "- 머신러닝에서 비용함수를 최적화(최소화)하는 것이 목표!\n",
    "- 모델을 학습할 때, 대부분의 머신러닝의 목표!\n",
    "- 모든 비용함수가 하나의 최솟값을 가지는 매끈한 곡선 형태는 아님. 즉, global minima와 중간중간 local minima가 있을 수 있다. 이때, 작은 기울기에서는 (학습률) 업데이트가 조금씩 이루어지기 때문에 lcoal minima에서 나오기가 어렵다.\n",
    "- MSE, RMSE와 같은 비용함수는 함수가 볼록함수라는 것이 증명되었지만, 아닌 경우에는 주의해야 한다.\n",
    "\n",
    "* 그러나, 보상함수를 최대화하기 위해서 강화학습을 하기도 한다. (비용을 최소화하는 것이 아니라, 보상을 높이는 것)\n",
    "* 두 목적이 충돌하는 경우도 있어 최대화/최소화가 혼재하는 생성 모델도 있음.\n",
    "\n",
    "## 경사하강법\n",
    "\n",
    "최소제곱법과 마찬가지로 y=ax+b에서 a와 b를 찾고자 함\n",
    "경사하강법은 이전의 기울기를 고려하지 않고, 현재의 기울기만 기준으로 움직인다.\n",
    "따라서, 이전의 기울기와 이동하던 방향을 고려하도록 '관성'을 부여하면 더 빠르게 찾을 수 있다.\n",
    "즉, 관성을 부여하면 이전의 기울기의 관성에 의해 작은 기울기를 더 쉽게 넘어갈 수 있고, local minima를 빠르게 탈출할 수도 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75154130-4728-4a2d-bc16-bd8271a9b89d",
   "metadata": {},
   "source": [
    "## 2. 다중선형회귀분석\n",
    "\n",
    "다중선형회귀분석: 독립 변수가 2개 이상인 경우의 선형회귀이고, 즉 N가지의 독립변수가 있음을 의미\n",
    "독립 변수가 2개인 경우 3차원의 공간에 표현되고, 회귀선을 '평면'이 된다.\n",
    "마찬가지로 최소제곱법을 사용할 수 있고, 이때는 관측치와 평면의 차이가 잔차가 된다.\n",
    "\n",
    "1. 다중공선성\n",
    "다중공선성: 회귀 분석에서 독립변수들끼리 강한 상관관계를 가질 때 발생하는 문제이고, 독립변수들이 너무 비슷한 정보를 가지고 있으면 회귀 모델이 불안정해질 수 있다.\n",
    "다중공서성이 높은 경우, 어떤 독립 변수가 종속 변수에 얼마나 영향을 미치는지 파악하기 어렵기 때문에, 회귀 모델은 어떤 독립 변수의 영향을 반영해야 할지 불확실해지고, 결국 정확도가 낮아지게 되는 것!\n",
    "\n",
    "* 다중공선성을 확인하는 방법\n",
    "1) 상관계수\n",
    "- -1과 1 사이에 존재하는 상관계수의 값을 파악하면 된다.\n",
    "- -1과 1에 가까울수록 각각 음/양의 상관성이 높다는 것을 의미한다.\n",
    "- 0은 상관관계가 없다는 것을 의미한다.\n",
    "- 히트맵, 또는 corr()함수를 통해 상관계수를 확인할 수 있다.\n",
    "- pairplot과 같은 시각화를 통해 (산점도) 파악할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45ad4cb-51cb-464f-8dcd-5267b62ec316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap('원하는 변수', '원하는 변수', annot=True)\n",
    "df.corr()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3a416d-a740-4c0a-95f5-f2412f26f027",
   "metadata": {},
   "source": [
    "2) VIF지수\n",
    "- 분산팽창인수, 회귀 모델의 결정계수 R square를 사용하여 계산됨\n",
    "- VIF가 1인 경우, 해당 독립 변수는 다른 변수와 상관관계가 없음을 의미한다.\n",
    "- VIF<5인 경우, 일반적으로 다중공선성 문제가 없다고 간주한다.\n",
    "- VIF>5인 경우, 다중공선성의 징후가 있을 수 있다.\n",
    "- VIF>10인 경우, 다중공선성이 심각하다고 간주되며, 회귀모델이 불안정할 가능성이 크다. 따라서, 해당 변수를 제거하거나,\n",
    "해당 변수 간의 상관관계를 줄이기 위한 조치가 필요하다.\n",
    "\n",
    "** 다중공선성 대처 방법\n",
    "1) 변수를 제거: 독립변수로서 사용할 변수를 선택\n",
    "2) 변수 변환: 변수들을 더하거나 빼서 새로운 변수를 생성. 이때, 독립변수를 더하거나 빼도 문제가 없는 경우에서 가능\n",
    "3) 규제 선형 모델 활용: 릿지, 라쏘, 엘라스틱넷 등의 방법을 통해 모델의 복잡도를 줄이는 방법 사용\n",
    "4) PCA(주성분분석): 데이터의 차원을 축소하는 데 사용되는 통계적 기법으로 고차원 데이터에서 중요한 정보는 보존하되,\n",
    "데이터의 복잡성을 줄이는 것이 목표이다.\n",
    "\n",
    "* PCA의 과정\n",
    "1) 데이터 전처리 (정규화)\n",
    "- 평균을 0으로 조정하고, 분산을 1로 맞춘다.\n",
    "2) 주성분 찾기\n",
    "- 공분산의 행렬을 계산하고, 고유벡터와 고유값을 추출한다.\n",
    "- 고유벡터: 데이터가 가장 많이 퍼진 방향 (주성분의 축)\n",
    "3) 차원 축소\n",
    "- 주성분 축에 데이터를 투영하여 중요한 정보만 유지한다.\n",
    "4) 복원 가능 (원래 좌표계로 변환이 가능하다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa195ee3-7853-4fcf-b394-625d03636670",
   "metadata": {},
   "source": [
    "## 규제선형모델\n",
    "\n",
    "규제선형모델: 모델이 학습 데이터에 과적합(overfitting)되지 않도록 규제를 가하고자 등장한 모델\n",
    "- 선형 회귀 모델에서는 특성에 곱해지는 계수(기울기)의 크기를 조정하는 것\n",
    "- 모델이 복잡해지는 다중회귀에서는 과적합이 발생할 가능성이 높다.\n",
    "- 기존 선형 모델에서는 잔차의 제곱의 합을 최소화하는 방식으로 비용 함수를 설정하는데, 이때, 회귀 계수가 쉽게 커지게 되므로 과적합의 발생 가능성이 높다.\n",
    "- 회귀계수가 커진다는 것은 특정 독립 변수에 과하게 의존하고 이동한다는 것을 의미함!\n",
    "-> 따라서, 비용 함수에서는 RSS 최소화 방법 (잔차의 제곱의 합을 최소화 하는 방법)과 회귀 계수 값이 커지지 않게 하는 방법이 서로 균형을 이루어야 한다.\n",
    "\n",
    "* 규제선형모델의 종류\n",
    "1) L2규제: 회귀계수의 제곱에 대해 페널티를 부여하는 방식, 릿지회귀\n",
    "- 회귀계수의 제곱에 대해 페널티를 부여하기 때문에, 큰 회귀계수일수록 더 큰 페널티를 받게 된다.\n",
    "- 모든 계수의 크기는 작아지지만, 완전히 사라지지는 않음 (즉, 회귀계수가 0이 되지는 않음, 변수 제거가 아님)\n",
    "- 계수의 크기를 조절 가능함 (모든 변수를 유지하면서 과적합을 방지!)\n",
    "- 예측 변수가 많을 때, 다중공선성이 존재할 때 사용한다!\n",
    "- 수치적으로 라쏘회귀보다 더 안정적\n",
    "\n",
    "2) L1규제: 회귀계수의 절댓값에 대해 페널티를 부여하는 방식, 라쏘회귀\n",
    "- 절댓값에 대해 페널티를 부여하기 때문에 회귀계수의 크기와 상관없이 같은 페널티를 받는다.\n",
    "- 작은 계수는 힘을 이기지 못해 0이 되기도 한다. (즉, 변수가 사라질 수 있고, 이 말은 변수를 선택할 수 있다는 것을 의미한다.) -> 중요한 변수만 남게 됨!\n",
    "- 예측 변수 수가 많은데, 일부만 필요하고, 모델의 해석을 간단하게 유지하려고 할 때 사용한다!\n",
    "\n",
    "3) L2규제+L1규제의 결합: 엘라스틱넷 회귀\n",
    "- 불필요한 변수는 제거하고, 남은 변수들을 적절한 크기로 유지할 수 있다.\n",
    "- L1규제를 활용하여 원하는 변수만 선택하고, L2규제를 활용하여 큰 계수를 줄여 과적합을 방지하는 것!\n",
    "- 따라서 희소성(변수 제거)와 안정적 예측을 동시에 달성할 수 있다.\n",
    "- 엘라스틱넷 회귀는 상관관계가 높은 피처의 처리에 강하다. (상관관계가 높은 피처들을 비슷한 계수 크기로 유지하면서도 불필요한 피처를 제거하기 때문!)\n",
    "- 알파값과 p를 조절하여 L1와 L2 정규화의 비중을 조절할 수 있다.\n",
    "- 예측 변수의 수가 많고, 그 중 중요한 변수를 선택하면서도 다중공선성을 관리할 때 사용한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e787a3-465c-4199-bdfc-42512b207e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#릿지회귀 (L2 규제) 코드\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge()\n",
    "ridge.fit(train_scaled, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d7a36-d584-461e-bc51-cf704813934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#라쏘 회귀 (L1 규제) 코드\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n",
    "lasso.fit(train_scaled, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46314c84-4af7-4d92-b07e-2f452f59d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#엘라스틱넷 회귀 (L2규제+L1규제) 코드\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alph=0.1, l1_ratio=0.5, max_iter=1000) #L1과 L2의 비중을 0.5씩 가져감\n",
    "elastic_net.fit(train_scaled, train_target)\n",
    "# 이때, L1_ratio=a/a+b인데, 즉, L1_ratio=0이라는 뜻은 a=0이므로 L2규제를 의미, L1_ratio=1은 L1규제와 동일, 0.5일때 RMSE가 가장 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717cc93-3278-49b8-a9af-81b452877615",
   "metadata": {},
   "source": [
    "## 모델의 평가방법\n",
    "\n",
    "성능의 평가 지표\n",
    "1) 평균 제곱 오차 (MSE): 오차 제곱의 평균을 의미\n",
    "2) 평균 절대 오차 (MAE): 절대값 오차의 평균을 의미\n",
    "-> MSE는 큰 오차에 대해 민갑하게 반응하지만, MAE는 모든 오차를 동일하게 취급한다.\n",
    "\n",
    "변수 유의성 평가\n",
    "1) t 검정: 독립 변수이 회귀계수가 유의미한지 검정하는데 사용된다.\n",
    "- 귀무가설: 회귀계수가 0이다. -> 독립변수가 종속변수에 영향을 주지 않는다.\n",
    "- 대립가설: 회귀계수가 0이 아니다. -> 독립변수가 종속변수에 영향을 준다.\n",
    "- p-value가 0.05보다 작으면 귀무가설을 기각하고, 대립가설을 채택한다.\n",
    "- t값이 크면 귀무가설을 기각할 가능성이 높아진다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
