{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb32297",
   "metadata": {},
   "source": [
    "# 회귀기초\n",
    "#### 선형회귀(단순, 다중) 경사하강법 다중공선성 규제선형모델 모델평가방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89497705",
   "metadata": {},
   "source": [
    "## 머신러닝\n",
    "머신러닝 : 패턴 from 데이터 -> 새로운 데이터 예측\n",
    "* 비지도학습: 정답이 없는 데이터로 학습\n",
    "* 지도학습: 정답이 있는 데이터로 학습\n",
    "\n",
    "## 모델링\n",
    "전체데이터 -> 훈련 데이터 / 테스트 데이터 분리 -> 훈련 데이터로부터 만든 모델을 테스트 데이터로 반복 성능 평가 -> 최종 모델 생성\n",
    "train_set: 모델 학습시 / test_set: 학습된 모델을 검증할때 (unseen data)\n",
    "train data를 통해 학습시킨 모델이 test data(unseen data)에 대해 얼마나 예측을 잘 수행하는지 평가하기 위해 data 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5f4f3",
   "metadata": {},
   "source": [
    "## 회귀 - 지도학습\n",
    "\n",
    "### 회귀(Regresssion)\n",
    "- 데이터에서 패턴을 찾아내어 미래 값을 예측하는 것\n",
    "- 연속적인 숫자를 다룸 \n",
    "*** 목표 변수가 불연속적이라면(범주형) \"분류\" 이용\n",
    "\n",
    "* 회귀 종류: 선형 / 비선형/ 릿지 / 라쏘 / 다항...\n",
    "\n",
    "### 선형 회귀\n",
    "- 데이터를 가장 잘 설명하는 회귀선을 찾아 예측하는 과정\n",
    "ex) 시간에 따른 주가 예측 -> 회귀선: y = ax + b \n",
    "종속변수 y : 예측하고자 하는 것 - 주가\n",
    "독립변수 x : 예측을 위해 사용하는 변수 - 시간\n",
    "\n",
    "(1) 단순선형회귀분석: 하나의 독립변수 사용해 데이터를 직선 형태로 표현\n",
    "Y = aX + B를 찾는 과정\n",
    "적절한 회귀계수 a(기울기), y 절편(b)를 찾는 과정\n",
    "- 데이터의 경향성이 잘 반영되도록. \n",
    "=> 잔차(residual): 모델이 설명 못한 \"실제 - 예측값\" 차이\n",
    "예측값을 조절하며 잔차를 줄여나가는 과정을 통해 회귀선을 찾는다.\n",
    "** 오차: \"실제 - 실제 평균값\"\n",
    "\n",
    "* 최소제곱법(Least Squares Method): 최적의 회귀선 찾는 방식 ~ 최소자승법...\n",
    "잔차의 제곱의 합(Sum of Squared Residuals, RSS)이 최소가 되는 지점으로 최적의 회귀선을 구한다.\n",
    "잔차 제곱 -> 부호 상쇄. -> 경사하강법 계산의 미분에 유리하도록!\n",
    "\n",
    "* 경사하강법(Gradient Descent): 최소제곱법을 통해 구한 함수가 복잡하여 최소값을 구하기 어려울 때 보통 사용\n",
    "1. 함수가 닫힌 상태\n",
    "2. 함수가 너무 복잡해 미분계수 구하기 어렵\n",
    "3. Gradient Descent를 구현하는게 미분 계수를 구하는 것보다 쉬운 경우\n",
    "4. 데이터 양이 너무 많은 경우 효율적으로 계산하기 위해\n",
    "\n",
    "** 목적함수: 최적화를 위해 오차를 최소화하는 것이 목적인 함수 \n",
    "=> 머신러닝에서 모델의 예측이 얼마나 잘 맞는가 \"평가하는 기준\"\n",
    "    - 최적화하려는 대상을 수학적으로 표현 ex) 시험점수 최대화, 실수 최소화...\n",
    "    \n",
    "** 손실함수: 개별 데이터 샘플에 대한 오차 측정.\n",
    "    - 예측값과 실제값 차이 계산\n",
    "    ex) 회귀: MSE(평균 제곱 오차) / MAE(평균절대오차)... 분류: Cross-Entropy Loss\n",
    "    E(W,b): 데이터 포인트들의 \"실제값 - 예측값\"의 제곱의 합의 평균\n",
    "    => 실제 값(y = Wx + b), 예측값 ti => E를 최소화하는 W / b 찾기\n",
    "    1) W(기울기)에 대한 식으로 구성 - 임의의 W값에서 E를 살펴봄\n",
    "    2) 임의의 W값에서 미분계수가 양수라면 W는 왼쪽으로 가야 됨\n",
    "    -> W값을 반복적으로 업데이트 => 미분계수 & \"학습률\" 이용\n",
    "    ** 학습률(α: learning rate) : 얼마나 이동? \n",
    "    3) W 값 업데이트 -> 미분값이 0이 되어 최소인 지점에서 업데이트 멈춤\n",
    "    -> 최솟값 도달 시 미분값 0. \"현재값 - 0\"이 되며 업데이트 멈춤\n",
    "    \n",
    "    ~ Gradient Descent: '학습한다' = 오차의 최솟값을 찾아나간다.\n",
    "\n",
    "** 비용함수: 손실함수를 모든 데이터 포인트에서 계산 후 평균 or 합산\n",
    "    - 머신러닝에서 모델 훈련 시 비용함수 최적화(최소화)가 목표 (비용함수 = 목적함수...)\n",
    "    - 강화학습 / 생성모델(GAN, VAEs)에서는 예외... \n",
    "\n",
    "(2) 학습률(α): W값 업데이트 시 \"α\" 얼마나? ~ 미분값 얼마나 조절해 +-?\n",
    "    - 너무 작다면, 너무 오랜 시간\n",
    "    - 너무 크다면, 수렴하지 못하고 발산 ~ parameter의 부호가 계속 바뀌기 때문\n",
    "    \n",
    "(3) Local Minima 문제\n",
    "-  작은 기울기에서 조금, 큰 기울기에서 많이 이동하는 문제 \n",
    "-  모든 비용 함수가 하나의 최솟값을 갖지 않기 때문에 Global / Local Minima 존재\n",
    "-  Global minima가 아닌 Local Minima(범위 내 최소값)에 빠져 최적값 도달 X \n",
    "\n",
    "(4) 해결법 - 모멘텀\n",
    "- 현재 기울기만 기준으로 움직임 -> 이전의 기울기, 이동해오던 방향을 고려해 관성 부여...\n",
    "- 기울기에 관성을 부여하여 작은 기울기는 쉽게 넘어갈 수 있게 -> 지역 최소값을 탈출할 수 있게 함. \n",
    "- 기울기가 매우 작은 구간을 빨리 빠져 나올 수 있음\n",
    "- 식: 현재 단계 이동량 계산 시. 이전단계 이동량과 비용함수 기울기에 모멘텀 계수부여\n",
    "\n",
    "### 다중선형회귀분석\n",
    "- 독립변수가 2개 이상인 경우의 선형 회귀 -> N가지 독립변수 \n",
    "- 독립변수가 2개면 3차원 공간에 표현 -> 회귀선은 평면\n",
    "- 2개면 최소제곱법 사용 가능. 잔차: 관측치 - 평면\n",
    "- 힘들면 경사하강법 사용\n",
    "- 2개 이상이면 비슷한 느낌으로 확장\n",
    "\n",
    "(1) 다중공선성(Multicollinearity)\n",
    "- 독립 변수들 간에 상관관계가 큰 경우 발생.\n",
    "=> 다중공선성이 높으면 어떤 독립변수가 종속변수에 얼마나 영향을 미치는지 정확하게 구분하기 어렵다. => 어떤 변수의 영향을 반영할지 불확실. 회귀모델 정확도 낮아짐\n",
    "\n",
    "* 확인법\n",
    "    - 상관계수: -1 ~ 1. 양 끝 값: 상관성이 높다. ~ heatmap(). corr()\n",
    "    - VIF 지수: R Square를 사용하여 계산, VIF가 높으면 다중공선성이 존재한다고 판단\n",
    "    - VIF = 1: 해당 독립변수는 다른 변수와 상관관계 X 의미\n",
    "    \n",
    "* 대처 방법\n",
    "    1. 변수 제거\n",
    "    2. 변수 변환: 더하거나 빼서 새로운 변수 생성 ~ 문제 없는 경우\n",
    "    3. 규제 선형 모델 활용 ex) 릿지, 라쏘, 엘라스틱넷 등... 모델의 복잡도를 줄임\n",
    "    4. PCA(주성분분석): 데이터의 차원을 축소하는데 사용하는 통계적 기법\n",
    "        - 고차원 데이터에서 중요한 정보를 최대한 보존하며 데이터의 복잡성을 줄이는 것을 목표로 함 -> 여러 변수를 공통된 하나의 feature로 \n",
    "        \n",
    "### 규제선형모델\n",
    "** overfitting 모델은 학습 데이터 셋에는 완벽히 맞지만, 새로운 데이터에서는 성능이 크게 떨어짐\n",
    "\n",
    "- 과적합되지 않도록 \"규제\"를 가하고자 등장\n",
    "    -> 선형 회귀 모델에서는 특성에 곱해지는 기울기 크기 조정\n",
    "- 여러 독립 변수 사용하여 예측하는 복잡한 모델의 다중회귀에서는 과적합 가능성 증가 \n",
    "- RSS(잔차 제곱합) 최소화(최소제곱법) -> 회귀 계수가 쉽게 커져 과적합 발생\n",
    "** 다중회귀식의 회귀 계수(W_n): n번째 특징에서 W만큼 가중치 -> 회귀 계수가 커진다 = 특정 독립변수 특징에 과하게 의존한다...\n",
    "\n",
    "=> 비용함수: 최소제곱법과 과적합 방지를 위해 회귀계수 값이 커지지 않도록 하는 방법이 균형을 유지해야 함. ~> 오류 줄이며 특정변수 집착 막아야 함.\n",
    "=> 규제: 비용함수에 alpha 값으로 페널티 부여해, 회귀 계수 크기를 감소시켜 과적합 개선\n",
    "\n",
    "종류: L2(릿지) / L1(라쏘) / L2 + L1(엘라스틱넷)\n",
    "\n",
    "* L2(릿지 회귀): 회귀계수의 제곱을 페널티로 줌.\n",
    "=> 큰 기둥은 세게, 작은 기둥은 살살, 전체적으로 작아짐\n",
    "=> 모든 변수 유지, 크기를 적절히 줄일 수 있음\n",
    "\n",
    "* L1(라쏘 회귀): 회귀계수의 절댓값을 페널티로 줌\n",
    "=> 모든 기둥을 같은 힘으로 누름. 작은 기둥이 완전히 사라질 수 있음\n",
    "=> 비중이 낮은 변수 사라지고, 중요한 변수만 선택 사능\n",
    "\n",
    "* L1+L2(엘라스틱넷): 회귀계수의 제곱 & 절댓값에 페널티\n",
    "=> 불필요한 변수 제거 & 남은 변수들은 적절한 크기를 유지 가능\n",
    "\n",
    "*** 적절한 alpha 값 찾기: 0.001 ~ 100까지 10배씩 늘려가며 릿지 회귀 모델 훈련해 fitting 정도 확인 ***\n",
    "\n",
    "### 모델 평가방법\n",
    "- 회귀 모델이 좋은 성능? 올바른 회귀모델인지 평가\n",
    "\n",
    "* 성능 평가 지표\n",
    "(1) 평균 제곱 오차(Mean square error, MSE)\n",
    "    - 모델의 예측값과 실제 관측값 사이의 오차 제곱의 평균\n",
    "\n",
    "(2) 평균 절대 오차(Mean Absolute Error, MAE)\n",
    "    - 모델의 예측값과 실제 관측값 사이의 절대값 오차의 평균\n",
    "    - 모델의 예측 성능을 평가하는 지표\n",
    "    \n",
    "* 변수 유의성 평가\n",
    "(1) t검정: 독립변수의 회귀계수가 유의미한지 검정\n",
    "귀무: 회귀계수가 0이다 -> 독립변수가 종속변수에 영향을 주지 않는다\n",
    "대립: 회귀계수가 0이 아니다.\n",
    "\n",
    "    - 검정 과정\n",
    "    beta_hat: \"추정된 회귀계수\"\n",
    "    SE: 표준오차\n",
    "    \n",
    "=> P-value가 작을수록 해당 변수는 종속변수에 더 큰 영향을 준다고 볼 수 있음\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21a3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
