{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ae26a7-7e73-46c1-8aeb-faadb9f6baba",
   "metadata": {},
   "source": [
    "1. 지도 학습과 비지도 학습의 차이에 대해 설명해주세요.\n",
    "2. 회귀와 분류의 차이에 대해 설명해주세요.\n",
    "3. 분류 모델의 네 가지 종류와, 각 모델이 무엇인지 간단하게 정리해주세요.\n",
    "4. 분류 평가 지표에는 무엇이 있는지 작성해주세요.\n",
    "5. 하이퍼파라미터 최적화가 무엇인지 쓰세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a866110-23b3-4106-88f8-8ecb1a5de2f8",
   "metadata": {},
   "source": [
    "머신러닝\n",
    "- 데이터 분석 by 알고리즘\n",
    "- 분석 결과 스스로 학습하여\n",
    "- 어떤 판단이나 예측 수행\n",
    "\n",
    "### 지도 학습 / 비지도 학습\n",
    "- 지도학습: 정답(label)이 있는 데이터를 사용해 예측값을 이미 만들어 둔 정답과 같아지도록 학습시키는 것\n",
    "    - 회귀, 분류\n",
    "    - ex) 기린 vs 고양이 -> 기린 구분 문제: 기린-고양이 사진을 주고 각 사진이 기린인지 고양이인지 알려주고 학습시킴\n",
    "\n",
    "- 비지도 학습: 정답(label)이 없는 데이터를 사용해 데이터 속의 패턴, 데이터 간 유사도를 학습하도록 하는 것\n",
    "    - 군집화, 밀도 추정, 차원 축소\n",
    "    - ex) 기린 vs 고양이 -> 기린 구분 문제: 기린-고양이 사진 주고 사진에서 특성을 파악하여 비슷한 사진끼리 분류\n",
    "    \n",
    "** 강화학습: 시행착오를 반복하여 정답을 찾는 과정\n",
    "    - 보상을 통해 상을 최대화 / 벌을 최소화하는 방향으로 공부시키는 방법\n",
    "    - 알파고\n",
    "    \n",
    "* 비지도 학습: 정답(label) X feature만 O, 주어진 데이터를 통해 그 속의 패턴 or 데이터 간 유사도를 학습하도록 하는 것\n",
    "군집화: 데이터의 특성을 파악하여 비슷한 사진끼리 분류\n",
    "정답이 주어져있지 않은 데이터의 특성을 학습하여 스스로 패턴 파악\n",
    "    * 군집화: 비슷한 데이터를 묶어 큰 단위로 만듦\n",
    "    * 밀도 추정: 데이터 분포 예측\n",
    "    * 차원 축소: 데이터 차원을 간추림\n",
    "\n",
    "\n",
    "* 지도 학습: 입력값과 정답(label), 결과값을 같이 주고 학습시킴\n",
    "labeled data: 독립변수(feature) + 종속변수(target/label) 모두 존재\n",
    "독립변수를 통해 추정한 prediction이 주어진 종속변수와 가까워지도록 모델을 훈련\n",
    "-> 예측이 의도한 정답이 되도록 지도... 경가하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab579c2-bc29-4fe2-9200-3aa8df669a20",
   "metadata": {},
   "source": [
    "### 회귀와 분류\n",
    "1. 회귀(Regression)\n",
    "- 주어진 데이터를 기반으로 정답을 잘 맞추는(fit)하는 함수를 찾는 문제\n",
    "- 연속형 변수를 예측하기 위해 사용 ex) 종속변수 : 주택 가격 / 독립 변수 : 평 수, 역까지 거리, 지역 평균 소득\n",
    "\n",
    "2. 분류(Classification)\n",
    "- 기존 데이터가 어떤 레이블에 속하는지 알고리즘으로 인지한 뒤 새롭게 관측된 데이터에 대한 레이블을 판별하는 문제\n",
    "- 범주형 변수를 예측하기 위해 사용 ex) 종속변수 : 대출상환여부(0/1) / 독립변수: 고객의 특성, 대출의 특성\n",
    "- 이진분류: 종속변수가 참 or 거짓 / 다중 분류: 종속변수가 가질 수 있는 값이 3개 이상\n",
    "  - 분류에 쓰이는 대표적인 머신러닝 알고리즘\n",
    "      1. 로지스틱 회귀\n",
    "      2. 결정트리\n",
    "      3. 서포트 벡터 머신\n",
    "      4. 최소 근접"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f82e1-31e6-46b7-a05f-746a6a9a9e3e",
   "metadata": {},
   "source": [
    "### 분류 모댈\n",
    "1. 로지스틱 회귀\n",
    "  - 이진 분류 문제 ex) 스팸 메일 / 질병 유무\n",
    "  - 독립 변수의 선형조합(선형 회귀)에 로지스틱 함수 적용 -> 출력값을 0 또는 1로 변환\n",
    "  - 다중 선형 회귀 in 분류\n",
    "     - 곡선이 더 유리\n",
    "     - 곡선(시그모이드 함수): 입력값이 작거나 클 때 천천히 변화, 중간값에서 급격히 변화 ~> 현실적인 확률 값 제공\n",
    "     - 예측값은 0 ~ 1 사이의 값 <=> 직선(선형 회귀) 시 값이 음의 무한대 ~ 양의 무한대\n",
    "\n",
    "* 시그모이드 함수(Sigmoid function) = 로지스틱 함수\n",
    "  - 출력 0 ~ 1 값. S자 형태\n",
    "  - 입력값이 커지면 1 수렴, 작아지면 0 수렴\n",
    "  - 출력 값에서 특정값을 기준으로 이상 1, 이하 0으로 정하면 이진 분류 문제 해결 가능\n",
    "\n",
    "* 시그모디으 함수의 가중치\n",
    "    - 인공지능: 적절한 가중치 w와 b를 구하여 분류\n",
    "    - w: 그래프 기울기 변화 / b: 그래프의 위치 변화\n",
    "    ** odds(승산): '사건 A가 일어날 확률' / '사건 A가 일어나지 않을 확률' ~ 어떤 확률이 발생할 확률이 발생하지 않을 확률보다 몇배 더 높을까\n",
    "      - logit 함수 = odds의 logit 변환(odds에 로그 취함)\n",
    "      - 시그모이드 함수 적용 => 모델이 예측한 확률(p(A))\n",
    "      - {p(A) = 시그모이드} 항등식 양변에 logit transform 적용\n",
    "        => logit(p) = wx + b의 선형관계로 변환됨\n",
    "      - 로지스틱 회귀에서의 odds를 사용(logit transformation)하여 시그모이드 함수를 적용한 비선형성 모델의 결과를 선형적으로 해석할 수 있다.\n",
    "\n",
    "2. 결정 트리(Decision Tree)\n",
    "    - 조건에 따라 데이터를 분류하며 최종적으로 데이터가 순수한 label의 집합으로 구성될 때까지 분류를 반복하는 모델\n",
    "    - CART 알고리즘: 데이터셋 임계값 기준\n",
    "       - 임계값 ~ 불순도 낮아지는 방향으로 가지를 형성해 나\n",
    "       - 불순도 ~ 지니 계수 사용\n",
    "       - Greedy Algorithm\n",
    "    - 클래스를 정확하게 구분할 분류기준을 찾는 것이 중요.\n",
    "    - 노드가 너무 많아지면 과적합될 확률 높음 -> 하부 트리 제거하여 일반화 성능을 높임\n",
    "\n",
    "3. 서포트 벡터 머신 (SVM)\n",
    "    - 클래스를 분류할 수 있는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘, 고차원 공간에서 효과적으로 사용 가능\n",
    "    - 특성들에 대해 집단을 구분하는 선 : Decision Boundary\n",
    "    - 구분하는 선과 가장 가까운 포인트: Support Vector\n",
    "    - 선과 각 점의 거리: Margin\n",
    "    - 결정경계(Decision boundary): 데이터로부터 가장 멀리. Margin이 가장 큰 경우가 최적의 선\n",
    "       \n",
    "4. KNN (K-Nearest Neighbor)\n",
    "    - 비슷한 특성을 가진 데이터끼리 서로 가까이 있다는 점 이용\n",
    "    -> 거리가 가까운 K개(k개의 이웃)의 다른 데이터 레이블을 참조하여 분류 \n",
    "    - 데이터 준비(특징 벡터 & 레이블) -> k 값 설정(홀수로, 동점 방지) -> 거리 계산(예측하려는 데이터와의 기존 데이터 간 거리) -> 가장 가까운 k개 선택 -> 분류... ~> 예측 결과: K개의 이웃 중 가장 많이 등장하는 클래스\n",
    "    => 모델 X. 어떤 학습이 필요하지 않음. 데이터만을 이용해서 새로운 데이터가 오면 주변 데이터를 이용해 분류\n",
    "    => 시간 오래 소요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d15c7-1614-4c6b-afbf-0ab155d4f5e3",
   "metadata": {},
   "source": [
    "### 앙상블\n",
    "여러개의 개별 분류모델을 결합하여 예측 결과를 종합해 최종 예측(병렬 or 직렬로)\n",
    "- Voting: 다른 알고리즘의 모델을 병렬로 사용\n",
    "- Bagging: 동일 알고리즘의 모델을 병렬로 사용\n",
    "- Boosting: 동일 알고리즘의 모델을 직렬로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378a43a-8008-4ab5-89ce-2179e171184b",
   "metadata": {},
   "source": [
    "### 분류 평가 지표\n",
    "1) 혼동 행렬: 분류 모델 예측 결과 -> 정확 / 잘못으로 구분 ~ 4가지 경우의 수\n",
    "    - 평가지표: 정확도(모든 가능한 예측 중 참인 비율) / 정밀도(참이라고 예측한 경우 실제 참 비율) / 재현도(실제로 참인 경우 중 참으로 예측)\n",
    "    - 암환자: 암환자인데 암환자 X 예측 치명적 => 실제로 참인데 참으로 예측하는 비율을 높여야 함 ~> 재현율을 높여야 함.\n",
    "    - => Threshold를 낮추어 모델이 참으로 예측하는 비율을 높여야 한다.\n",
    "    - 스팸메일: 스팸 X를 스팸으로 분류 치명적 -> 스팸인데 실제 스팸일 확률 높여야 ~ 정밀도 높여야. -> Threshold 높여서 참일 확률 낮춰야 함\n",
    "    - 정밀도와 재현도는 TRADE OFF 관계 -> 그래프 상 두 value가 만나는 지점을 Threshold하면 예측 오류 최소화\n",
    "      \n",
    "2) F1-Score: 정밀도와 재현율의 조화평균(균형 정도를 보여줌)\n",
    "\n",
    "3) ROC / AUC Curve\n",
    "    - ROC: 얼마나 분류가 잘되었는가를 보여줌 \n",
    "    - AUC: ROC와 x축 사이의 면적 ~ 1에 가까울수록 분류 성능 좋다. \n",
    "\n",
    "** 다중 분류 평가지표\n",
    "- Macro Average: 클래스별로 구한 평가지표 평균하여 성능 평가 ~ 모든 label이 유사한 중요도\n",
    "- Weighted Average: 클래스별 평가지표를 클래스의 샘플수로 가중 평균하여 성능 평가 ~ 샘플이 많은 label에 중요도\n",
    "- Micro Average:  모든 클래스의 예측 결과를 더해 동등한 가중치 부여하여 평가 ~ label 상관 X 전체적인 성능 평가 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b584f6c-ab17-4582-90c3-bd3691401a87",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 최적화\n",
    "하이퍼파라미터: 학습시작 전 사용자가 직접 설정하는 변수\n",
    "하이퍼파라미터 최적화: 적절한 하이퍼파라미터를 찾아 모델 성능을 향상\n",
    "\n",
    "\n",
    "* 하이퍼파라미터 최적화 과정\n",
    "    1. 최적 값을 찾고 싶은 하이퍼파라미터의 범위를 설정\n",
    "    2. 평가 지표 계산 함수(성능 평가 함수) 정의\n",
    "       - 탐색하려는 Hyperparameter를 인수로 받아 평가지표 값을 계산해주는 함수\n",
    "    3. 1단계에서 샘플링한 Hyperparameter 값을 사용하여 검증 데이터로 정확도 평가\n",
    "    4. 위 단계를 특정 횟수 반복, 정확도 결과 -> 하이퍼파라미터의 범위 좁힘\n",
    "\n",
    "* 방법\n",
    "    - Grid Search: 범위내 모두 순회\n",
    "    - Random Search: 범위 내 무작위 탐색\n",
    "    - Bayesian Optimization: 사전 정보 바탕으로 확률적으로 추정\n",
    "\n",
    "* 검증: Valid data 사용. (Train / Valid / Test 로 데이터 set 나눔)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c12117-e325-4d22-a4ed-b753b8307a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
